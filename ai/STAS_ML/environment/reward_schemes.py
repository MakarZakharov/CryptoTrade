"""
–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ñ —Å—Ö–µ–º–∏ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥ –¥–ª—è —Ç–æ—Ä–≥–æ–≤–æ–≥–æ —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞.
–ü—ñ–¥—Ç—Ä–∏–º—É—î —Ä—ñ–∑–Ω—ñ –º–µ—Ç–æ–¥–∏ —Ä–æ–∑—Ä–∞—Ö—É–Ω–∫—É –≤–∏–Ω–∞–≥–æ—Ä–æ–¥ —Ç–∞ —ó—Ö –∫–æ–º–±—ñ–Ω—É–≤–∞–Ω–Ω—è.
"""

import numpy as np
import pandas as pd
from typing import List, Dict, Optional, Callable
from abc import ABC, abstractmethod


class BaseRewardScheme(ABC):
    """–ë–∞–∑–æ–≤–∏–π –∫–ª–∞—Å –¥–ª—è —Å—Ö–µ–º –≤–∏–Ω–∞–≥–æ—Ä–æ–¥."""
    
    def __init__(self, weight: float = 1.0):
        self.weight = weight
    
    @abstractmethod
    def calculate(self, env_state: Dict) -> float:
        """–†–æ–∑—Ä–∞—Ö—É–≤–∞—Ç–∏ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥—É –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Å—Ç–∞–Ω—É —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞."""
        pass
    
    def reset(self):
        """–°–∫–∏–Ω—É—Ç–∏ —Å—Ç–∞–Ω —Å—Ö–µ–º–∏ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥."""
        pass


class ProfitReward(BaseRewardScheme):
    """–í–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –ø—Ä–∏–±—É—Ç–∫—É –ø–æ—Ä—Ç—Ñ–µ–ª—è."""
    
    def __init__(self, weight: float = 1.0, normalize: bool = True):
        super().__init__(weight)
        self.normalize = normalize
        self.last_portfolio_value = None
    
    def calculate(self, env_state: Dict) -> float:
        portfolio_value = env_state['portfolio_value']
        
        if self.last_portfolio_value is None:
            self.last_portfolio_value = portfolio_value
            return 0.0
        
        # –í—ñ–¥–Ω–æ—Å–Ω–∞ –∑–º—ñ–Ω–∞ –ø–æ—Ä—Ç—Ñ–µ–ª—è
        profit_change = (portfolio_value - self.last_portfolio_value) / self.last_portfolio_value
        self.last_portfolio_value = portfolio_value
        
        if self.normalize:
            # –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –ø–æ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—ñ
            reward = np.tanh(profit_change * 100) * self.weight
        else:
            reward = profit_change * self.weight
        
        return reward
    
    def reset(self):
        self.last_portfolio_value = None


class DrawdownPenalty(BaseRewardScheme):
    """–®—Ç—Ä–∞—Ñ –∑–∞ –ø—Ä–æ—Å–∞–¥–∫—É."""
    
    def __init__(self, weight: float = -0.5, max_drawdown_threshold: float = 0.1):
        super().__init__(weight)
        self.max_drawdown_threshold = max_drawdown_threshold
    
    def calculate(self, env_state: Dict) -> float:
        max_drawdown = env_state.get('max_drawdown', 0.0)
        
        if max_drawdown > self.max_drawdown_threshold:
            # –ï–∫—Å–ø–æ–Ω–µ–Ω—Ü—ñ–π–Ω–∏–π —à—Ç—Ä–∞—Ñ –∑–∞ –ø–µ—Ä–µ–≤–∏—â–µ–Ω–Ω—è –ø–æ—Ä–æ–≥—É –ø—Ä–æ—Å–∞–¥–∫–∏
            penalty = np.exp((max_drawdown - self.max_drawdown_threshold) * 10) - 1
            return -penalty * abs(self.weight)
        
        return 0.0


class DynamicRewardScaler:
    """
    –î–∏–Ω–∞–º—ñ—á–Ω–µ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è –≤–∏–Ω–∞–≥–æ—Ä–æ–¥ –Ω–∞ –æ—Å–Ω–æ–≤—ñ —ñ—Å—Ç–æ—Ä–∏—á–Ω–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤.
    –ó–∞–ø–æ–±—ñ–≥–∞—î –∑–∞–≤–µ–ª–∏–∫–∏–º –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞–º —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–∏–≤–Ω—É –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—é.
    """
    
    def __init__(self, history_window: int = 100, target_range: tuple = (-2.0, 2.0)):
        self.history_window = history_window
        self.target_range = target_range
        self.reward_history = []
        self.running_mean = 0.0
        self.running_std = 1.0
        self.alpha = 0.1  # –ö–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç –∑–≥–ª–∞–¥–∂—É–≤–∞–Ω–Ω—è –¥–ª—è –µ–∫—Å–ø–æ–Ω–µ–Ω—Ü—ñ–π–Ω–æ–≥–æ —Å–µ—Ä–µ–¥–Ω—å–æ–≥–æ
    
    def scale_reward(self, raw_reward: float) -> float:
        """–ú–∞—Å—à—Ç–∞–±—É—î –≤–∏–Ω–∞–≥–æ—Ä–æ–¥—É –Ω–∞ –æ—Å–Ω–æ–≤—ñ —ñ—Å—Ç–æ—Ä–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö."""
        # –î–æ–¥–∞—î–º–æ –ø–æ—Ç–æ—á–Ω—É –≤–∏–Ω–∞–≥–æ—Ä–æ–¥—É –¥–æ —ñ—Å—Ç–æ—Ä—ñ—ó
        self.reward_history.append(raw_reward)
        
        # –û–±–º–µ–∂—É—î–º–æ —Ä–æ–∑–º—ñ—Ä —ñ—Å—Ç–æ—Ä—ñ—ó
        if len(self.reward_history) > self.history_window:
            self.reward_history = self.reward_history[-self.history_window:]
        
        # –û–Ω–æ–≤–ª—é—î–º–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –µ–∫—Å–ø–æ–Ω–µ–Ω—Ü—ñ–π–Ω–∏–º –∑–≥–ª–∞–¥–∂—É–≤–∞–Ω–Ω—è–º
        if len(self.reward_history) > 1:
            current_mean = np.mean(self.reward_history[-10:])  # –û—Å—Ç–∞–Ω–Ω—ñ 10 –∑–Ω–∞—á–µ–Ω—å
            current_std = np.std(self.reward_history[-10:]) + 1e-8  # –ó–∞–ø–æ–±—ñ–≥–∞—î–º–æ –¥—ñ–ª–µ–Ω–Ω—é –Ω–∞ 0
            
            # –ï–∫—Å–ø–æ–Ω–µ–Ω—Ü—ñ–π–Ω–µ –∑–≥–ª–∞–¥–∂—É–≤–∞–Ω–Ω—è
            self.running_mean = (1 - self.alpha) * self.running_mean + self.alpha * current_mean
            self.running_std = (1 - self.alpha) * self.running_std + self.alpha * current_std
        
        # Z-score –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –∑ –∞–¥–∞–ø—Ç–∏–≤–Ω–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        if self.running_std > 0:
            normalized_reward = (raw_reward - self.running_mean) / self.running_std
        else:
            normalized_reward = 0.0
        
        # –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è –¥–æ —Ü—ñ–ª—å–æ–≤–æ–≥–æ –¥—ñ–∞–ø–∞–∑–æ–Ω—É –∑ tanh –¥–ª—è –º'—è–∫–æ–≥–æ –æ–±–º–µ–∂–µ–Ω–Ω—è
        target_min, target_max = self.target_range
        target_center = (target_max + target_min) / 2
        target_scale = (target_max - target_min) / 4  # tanh(¬±2) ‚âà ¬±0.96
        
        scaled_reward = target_center + target_scale * np.tanh(normalized_reward)
        
        # –õ–æ–≥—É–≤–∞–Ω–Ω—è –¥–ª—è –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ (—Ç—ñ–ª—å–∫–∏ –ø—Ä–∏ –≤–µ–ª–∏–∫–∏—Ö –≤—ñ–¥—Ö–∏–ª–µ–Ω–Ω—è—Ö)
        if abs(raw_reward) > 10:
            print(f"üîß REWARD SCALING: {raw_reward:.2f} -> {scaled_reward:.2f} (Œº={self.running_mean:.2f}, œÉ={self.running_std:.2f})")
        
        return scaled_reward
    
    def reset(self):
        """–°–∫–∏–¥–∞–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ (—á–∞—Å—Ç–∫–æ–≤–æ –∑–±–µ—Ä—ñ–≥–∞—î–º–æ —ñ—Å—Ç–æ—Ä—ñ—é –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ)."""
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –ø–æ–ª–æ–≤–∏–Ω—É —ñ—Å—Ç–æ—Ä—ñ—ó –¥–ª—è –∫—Ä–∞—â–æ—ó —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ
        if len(self.reward_history) > 20:
            self.reward_history = self.reward_history[-20:]
        # –ù–µ —Å–∫–∏–¥–∞—î–º–æ running_mean —ñ running_std –ø–æ–≤–Ω—ñ—Å—Ç—é –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ


class CompositeRewardScheme:
    """–ö–æ–º–±—ñ–Ω–æ–≤–∞–Ω–∞ —Å—Ö–µ–º–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥ –∑ –¥–∏–Ω–∞–º—ñ—á–Ω–∏–º –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è–º."""
    
    def __init__(self, schemes: List[BaseRewardScheme], enable_dynamic_scaling: bool = True):
        self.schemes = schemes
        self.reward_history = []
        self.enable_dynamic_scaling = enable_dynamic_scaling
        
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î–º–æ –¥–∏–Ω–∞–º—ñ—á–Ω–∏–π –º–∞—Å—à—Ç–∞–±—É–≤–∞—á
        if self.enable_dynamic_scaling:
            self.scaler = DynamicRewardScaler(
                history_window=50,  # –ú–µ–Ω—à–µ –≤—ñ–∫–Ω–æ –¥–ª—è —à–≤–∏–¥—à–æ—ó –∞–¥–∞–ø—Ç–∞—Ü—ñ—ó
                target_range=(-3.0, 3.0)  # –†–æ–∑—É–º–Ω–∏–π –¥—ñ–∞–ø–∞–∑–æ–Ω –¥–ª—è PPO
            )
        
        # –°–ò–°–¢–ï–ú–ê –ü–†–û–ì–†–ï–°–ò–í–ù–ò–• –ù–ê–ì–û–†–û–î –í–ò–ú–ö–ù–ï–ù–ê —á–µ—Ä–µ–∑ –Ω–µ–∫–æ—Ä–µ–∫—Ç–Ω—ñ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
        self.last_performance = {
            'return': 0.0,
            'drawdown': 0.0,
            'trades': 0,
            'win_rate': 0.0
        }
        self.improvement_bonus_count = 0
        
        # –°–ò–°–¢–ï–ú–ê –ï–°–ö–ê–õ–ê–¶–Ü–á –®–¢–†–ê–§–Ü–í –ó–ê –ü–û–°–õ–Ü–î–û–í–ù–£ –ü–û–ì–ê–ù–£ –ü–†–û–î–£–ö–¢–ò–í–ù–Ü–°–¢–¨
        self.consecutive_poor_episodes = 0
        self.poor_performance_threshold = -0.15  # -15% —è–∫ –ø–æ–≥–∞–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç (–∑–±—ñ–ª—å—à–µ–Ω–æ –∑ -2%)
    
    def calculate(self, env_state: Dict) -> float:
        """–†–æ–∑—Ä–∞—Ö—É–≤–∞—Ç–∏ –∑–∞–≥–∞–ª—å–Ω—É –≤–∏–Ω–∞–≥–æ—Ä–æ–¥—É –∑ –¥–∏–Ω–∞–º—ñ—á–Ω–∏–º –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è–º —Ç–∞ –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –≤—ñ–¥'—î–º–Ω–æ—ó –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ."""
        total_reward = 0.0
        component_rewards = {}
        
        for scheme in self.schemes:
            component_reward = scheme.calculate(env_state)
            # –ú'—è–∫–µ –æ–±–º–µ–∂–µ–Ω–Ω—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤ –ø–µ—Ä–µ–¥ –ø—ñ–¥—Å—É–º–æ–≤—É–≤–∞–Ω–Ω—è–º
            component_reward = np.clip(component_reward, -15.0, 15.0)
            component_rewards[scheme.__class__.__name__] = component_reward
            total_reward += component_reward
        
        # –ö–†–ò–¢–ò–ß–ù–û –í–ê–ñ–õ–ò–í–û: –ö–æ–Ω—Ç—Ä–æ–ª—å –≤—ñ–¥'—î–º–Ω–æ—ó –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
        total_return = env_state.get('total_return', 0.0)
        total_trades = env_state.get('total_trades', 0)
        
        # –õ—ñ—á–∏–ª—å–Ω–∏–∫ –¥–ª—è summary –ª–æ–≥—É–≤–∞–Ω–Ω—è
        if not hasattr(self, 'negative_override_count'):
            self.negative_override_count = 0
            self.last_summary_step = 0
        
        # –í–ò–ú–ö–ù–ï–ù–û: –ù–µ–≥–∞—Ç–∏–≤–Ω–∏–π override –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ –Ω–∞–≤—á–∞–Ω–Ω—è
        # –ó–∞–º—ñ—Å—Ç—å –∂–æ—Ä—Å—Ç–∫–∏—Ö —à—Ç—Ä–∞—Ñ—ñ–≤ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –º'—è–∫—ñ —Å–∏–≥–Ω–∞–ª–∏ —á–µ—Ä–µ–∑ –æ—Å–Ω–æ–≤–Ω—ñ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏
        # if total_return < -0.20 and total_trades > 10:  # –¢—ñ–ª—å–∫–∏ –ø—Ä–∏ –∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ—ñ—á–Ω–∏—Ö –≤—Ç—Ä–∞—Ç–∞—Ö >20%
        #     negative_override = total_return * 2.0  # –ú'—è–∫–∏–π —à—Ç—Ä–∞—Ñ
        #     negative_override = max(negative_override, -1.0)  # –ú–∞–∫—Å–∏–º—É–º -1.0
        #     
        #     # –ó–∞—Å—Ç–æ—Å–æ–≤—É—î–º–æ override —Ç—ñ–ª—å–∫–∏ —è–∫—â–æ –ø–æ—á–∞—Ç–∫–æ–≤–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –±—É–ª–∞ –ø–æ–∑–∏—Ç–∏–≤–Ω–æ—é
        #     if total_reward > negative_override:
        #         total_reward = negative_override
        #         self.negative_override_count += 1
        #         
        #         # –¢–Ü–õ–¨–ö–ò summary –ª–æ–≥—É–≤–∞–Ω–Ω—è –∫–æ–∂–Ω—ñ 50 overrides
        #         current_step = env_state.get('step', 0)
        #         if (self.negative_override_count % 50 == 0 or 
        #             current_step - self.last_summary_step > 1000):
        #             print(f"üìä NEGATIVE PERFORMANCE SUMMARY:")
        #             print(f"   Overrides applied: {self.negative_override_count}")
        #             print(f"   Current return: {total_return:.2%}, trades: {total_trades}")
        #             print(f"   Latest override: {negative_override:.2f}")
        #             self.last_summary_step = current_step
        
        # –î–û–î–ê–¢–ö–û–í–Ü –ê–ì–†–ï–°–ò–í–ù–Ü –®–¢–†–ê–§–ò –∑–∞ –ø–æ–≥–∞–Ω—É –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å
        current_drawdown = env_state.get('max_drawdown', 0.0)
        win_rate = env_state.get('win_rate', 0.0)
        
        # –í–ò–ú–ö–ù–ï–ù–û: –í—Å—ñ –¥–æ–¥–∞—Ç–∫–æ–≤—ñ —à—Ç—Ä–∞—Ñ–∏ –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ –Ω–∞–≤—á–∞–Ω–Ω—è
        # –û—Å–Ω–æ–≤–Ω—ñ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏ —Å—Ö–µ–º–∏ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥ –≤–∂–µ –≤–∫–ª—é—á–∞—é—Ç—å –∫–æ–Ω—Ç—Ä–æ–ª—å —Ä–∏–∑–∏–∫—ñ–≤
        # –î–æ–¥–∞—Ç–∫–æ–≤—ñ —à—Ç—Ä–∞—Ñ–∏ —Å—Ç–≤–æ—Ä—é—é—Ç—å –Ω–µ—Å—Ç–∞–±—ñ–ª—å–Ω—ñ—Å—Ç—å —Ç–∞ –∑–∞–≤–∞–∂–∞—é—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—é
        
        # # –ú'—è–∫–∏–π —à—Ç—Ä–∞—Ñ –∑–∞ –∫—Ä–∏—Ç–∏—á–Ω—É –ø—Ä–æ—Å–∞–¥–∫—É (—Ç—ñ–ª—å–∫–∏ –ø—Ä–∏ >30%)
        # if current_drawdown > 0.30:  
        #     drawdown_penalty = -current_drawdown * 5.0  # –ú'—è–∫–∏–π —à—Ç—Ä–∞—Ñ
        #     total_reward += drawdown_penalty
        
        # # –ú'—è–∫–∏–π —à—Ç—Ä–∞—Ñ –∑–∞ –∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ—ñ—á–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
        # if total_return < -0.50 and total_trades > 20 and win_rate < 0.2:  
        #     winrate_penalty = -(0.2 - win_rate) * 5.0  # –î—É–∂–µ –º'—è–∫–∏–π —à—Ç—Ä–∞—Ñ
        #     total_reward += winrate_penalty
        
        # üéØ –°–ò–°–¢–ï–ú–ê –ü–†–û–ì–†–ï–°–ò–í–ù–ò–• –ù–ê–ì–û–†–û–î –ü–û–í–ù–Ü–°–¢–Æ –í–ò–ú–ö–ù–ï–ù–ê
        # improvement_bonus = self._calculate_improvement_bonus(env_state)
        # total_reward += improvement_bonus
        
        # –í–ò–ú–ö–ù–ï–ù–û: –°–∏—Å—Ç–µ–º–∞ –µ—Å–∫–∞–ª–∞—Ü—ñ—ó —à—Ç—Ä–∞—Ñ—ñ–≤ (—Å—Ç–≤–æ—Ä—é—î –Ω–µ—Å—Ç–∞–±—ñ–ª—å–Ω—ñ—Å—Ç—å)
        # escalation_penalty = self._calculate_escalation_penalty(env_state)
        # total_reward += escalation_penalty
        
        # –í–ò–ú–ö–ù–ï–ù–û: –î–∏–Ω–∞–º—ñ—á–Ω–µ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è (—Å—Ç–≤–æ—Ä—é—î –Ω–µ—Å—Ç–∞–±—ñ–ª—å–Ω—ñ—Å—Ç—å)
        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –ø—Ä–æ—Å—Ç–µ –º'—è–∫–µ –æ–±–º–µ–∂–µ–Ω–Ω—è –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ
        scaled_reward = np.tanh(total_reward / 5.0) * 2.0  # –ú'—è–∫–µ –æ–±–º–µ–∂–µ–Ω–Ω—è –¥–æ ¬±2.0
        
        # –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–π —à—É–º –¥–ª—è –≤–∞—Ä—ñ–∞—Ç–∏–≤–Ω–æ—Å—Ç—ñ
        noise = np.random.normal(0, 0.005)
        final_reward = scaled_reward + noise
        
        self.reward_history.append(final_reward)
        return final_reward
    
    def _calculate_improvement_bonus(self, env_state: Dict) -> float:
        """–í–ò–ü–†–ê–í–õ–ï–ù–ê –ª–æ–≥—ñ–∫–∞ –±–æ–Ω—É—Å—ñ–≤ - –í–ò–ú–ö–ù–ï–ù–û –º—ñ–∂–µ–ø—ñ–∑–æ–¥–Ω—ñ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è."""
        # –ü–û–í–ù–Ü–°–¢–Æ –í–ò–ú–ö–ù–ï–ù–û —Å–∏—Å—Ç–µ–º—É –ø–æ–∫—Ä–∞—â–µ–Ω—å —á–µ—Ä–µ–∑ –Ω–µ–∫–æ—Ä–µ–∫—Ç–Ω—ñ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
        # –°–∏—Å—Ç–µ–º–∞ –ø–æ—Ä—ñ–≤–Ω—é–≤–∞–ª–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ —Ä—ñ–∑–Ω–∏—Ö –µ–ø—ñ–∑–æ–¥—ñ–≤, —Å—Ç–≤–æ—Ä—é—é—á–∏ –ª–æ–∂–Ω—ñ "–ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è"
        # –ù–∞–ø—Ä–∏–∫–ª–∞–¥: -50% –≤ –æ–¥–Ω–æ–º—É –µ–ø—ñ–∑–æ–¥—ñ ‚Üí +20% –≤ –Ω–∞—Å—Ç—É–ø–Ω–æ–º—É = "–ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è" –Ω–∞ 70%
        # –¶–µ –ø—Ä–∏–∑–≤–æ–¥–∏–ª–æ –¥–æ —Ä–æ–∑–±—ñ–∂–Ω–æ—Å—Ç–µ–π –º—ñ–∂ –ø–æ–∫–∞–∑–Ω–∏–∫–∞–º–∏ total_return —Ç–∞ PROFIT IMPROVEMENT
        
        return 0.0  # –í–ò–ú–ö–ù–ï–ù–û –≤—Å—ñ –±–æ–Ω—É—Å–∏ –∑–∞ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è
    
    def _calculate_escalation_penalty(self, env_state: Dict) -> float:
        """–†–æ–∑—Ä–∞—Ö–æ–≤—É—î –µ—Å–∫–∞–ª–∞—Ü—ñ–π–Ω–∏–π —à—Ç—Ä–∞—Ñ –∑–∞ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω—É –ø–æ–≥–∞–Ω—É –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å."""
        current_return = env_state.get('total_return', 0.0)
        current_step = env_state.get('step', 0)
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ –ø–æ—Ç–æ—á–Ω–∞ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å –ø–æ–≥–∞–Ω–∞
        if current_return < self.poor_performance_threshold:
            self.consecutive_poor_episodes += 1
        else:
            # –°–∫–∏–¥–∞—î–º–æ –ª—ñ—á–∏–ª—å–Ω–∏–∫ —è–∫—â–æ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å –ø–æ–∫—Ä–∞—â–∏–ª–∞—Å—è
            self.consecutive_poor_episodes = 0
            return 0.0
        
        # –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ –µ—Å–∫–∞–ª–∞—Ü—ñ–π–Ω–∏–π —à—Ç—Ä–∞—Ñ
        escalation_penalty = 0.0
        
        if self.consecutive_poor_episodes >= 10:  # 10+ –ø–æ–≥–∞–Ω–∏—Ö –µ–ø—ñ–∑–æ–¥—ñ–≤ –ø—ñ–¥—Ä—è–¥
            # –ñ–û–†–°–¢–ö–ò–ô –®–¢–†–ê–§ –∑–∞ —Ç—Ä–∏–≤–∞–ª—É –ø–æ–≥–∞–Ω—É –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å
            escalation_multiplier = min(self.consecutive_poor_episodes / 10.0, 5.0)  # –î–æ 5x –º–Ω–æ–∂–Ω–∏–∫–∞
            base_penalty = abs(current_return) * 30.0  # –ë–∞–∑–æ–≤–∏–π —à—Ç—Ä–∞—Ñ
            escalation_penalty = -base_penalty * escalation_multiplier
            escalation_penalty = max(escalation_penalty, -50.0)  # –ú–∞–∫—Å–∏–º—É–º -50.0 —à—Ç—Ä–∞—Ñ—É
            
            # –õ–æ–≥—É–≤–∞–Ω–Ω—è –∫–æ–∂–Ω—ñ 50 –ø–æ–≥–∞–Ω–∏—Ö –µ–ø—ñ–∑–æ–¥—ñ–≤
            if self.consecutive_poor_episodes % 50 == 0:
                print(f"üî• ESCALATION PENALTY: {self.consecutive_poor_episodes} consecutive poor episodes")
                print(f"   Return: {current_return:.2%}, Penalty: {escalation_penalty:.2f}")
        
        elif self.consecutive_poor_episodes >= 5:  # 5-9 –ø–æ–≥–∞–Ω–∏—Ö –µ–ø—ñ–∑–æ–¥—ñ–≤
            # –ü–æ–º—ñ—Ä–Ω–∏–π –¥–æ–¥–∞—Ç–∫–æ–≤–∏–π —à—Ç—Ä–∞—Ñ
            escalation_penalty = -abs(current_return) * 10.0
            escalation_penalty = max(escalation_penalty, -10.0)
        
        return escalation_penalty
    
    def reset(self):
        """–°–∫–∏–¥–∞–Ω–Ω—è —ñ—Å—Ç–æ—Ä—ñ—ó –≤–∏–Ω–∞–≥–æ—Ä–æ–¥."""
        self.reward_history = []
        if self.enable_dynamic_scaling:
            self.scaler.reset()
        for scheme in self.schemes:
            scheme.reset()
        
        # –í–ò–ü–†–ê–í–õ–ï–ù–û: –°–∫–∏–¥–∞—î–º–æ —Å–∏—Å—Ç–µ–º—É –ø—Ä–æ–≥—Ä–µ—Å–∏–≤–Ω–∏—Ö –Ω–∞–≥–æ—Ä–æ–¥ –¥–æ –†–ï–ê–õ–¨–ù–ò–• –ø–æ—á–∞—Ç–∫–æ–≤–∏—Ö –∑–Ω–∞—á–µ–Ω—å
        self.last_performance = {
            'return': 0.0,  # –í–ò–ü–†–ê–í–õ–ï–ù–û: –ü–æ—á–∏–Ω–∞—î–º–æ –∑ –Ω—É–ª—å–æ–≤–æ—ó –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—ñ (–Ω–µ -100%)
            'drawdown': 0.0,  # –í–ò–ü–†–ê–í–õ–ï–ù–û: –ü–æ—á–∏–Ω–∞—î–º–æ –±–µ–∑ –ø—Ä–æ—Å–∞–¥–∫–∏
            'trades': 0,
            'win_rate': 0.0
        }
        self.improvement_bonus_count = 0
        
        # –°–∫–∏–¥–∞—î–º–æ —Å–∏—Å—Ç–µ–º—É –µ—Å–∫–∞–ª–∞—Ü—ñ—ó —à—Ç—Ä–∞—Ñ—ñ–≤
        self.consecutive_poor_episodes = 0


class SharpeRatioReward(BaseRewardScheme):
    """–í–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∞ –®–∞—Ä–ø–∞."""
    
    def __init__(self, weight: float = 0.3, window: int = 50, risk_free_rate: float = 0.02):
        super().__init__(weight)
        self.window = window
        self.risk_free_rate = risk_free_rate / 252  # –î–µ–Ω–Ω–∞ –±–µ–∑—Ä–∏–∑–∏–∫–æ–≤–∞ —Å—Ç–∞–≤–∫–∞
        self.returns_history = []
    
    def calculate(self, env_state: Dict) -> float:
        portfolio_history = env_state.get('portfolio_history', [])
        
        if len(portfolio_history) < 2:
            return 0.0
        
        # –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ –¥–µ–Ω–Ω—ñ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—ñ
        returns = np.diff(portfolio_history) / portfolio_history[:-1]
        self.returns_history.extend(returns[-1:])  # –î–æ–¥–∞—î–º–æ —Ç—ñ–ª—å–∫–∏ –æ—Å—Ç–∞–Ω–Ω—é –¥–æ—Ö–æ–¥–Ω—ñ—Å—Ç—å
        
        # –û–±–º–µ–∂—É—î–º–æ —Ä–æ–∑–º—ñ—Ä —ñ—Å—Ç–æ—Ä—ñ—ó
        if len(self.returns_history) > self.window:
            self.returns_history = self.returns_history[-self.window:]
        
        if len(self.returns_history) < 10:  # –ú—ñ–Ω—ñ–º—É–º –¥–ª—è —Ä–æ–∑—Ä–∞—Ö—É–Ω–∫—É
            return 0.0
        
        # –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç –®–∞—Ä–ø–∞
        excess_returns = np.array(self.returns_history) - self.risk_free_rate
        if np.std(excess_returns) > 0:
            sharpe = np.mean(excess_returns) / np.std(excess_returns)
            return np.tanh(sharpe) * self.weight
        
        return 0.0
    
    def reset(self):
        self.returns_history = []


class TradeQualityReward(BaseRewardScheme):
    """–í–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑–∞ —è–∫—ñ—Å—Ç—å —É–≥–æ–¥."""
    
    def __init__(self, weight: float = 0.2, min_trades: int = 5):
        super().__init__(weight)
        self.min_trades = min_trades
    
    def calculate(self, env_state: Dict) -> float:
        total_trades = env_state.get('total_trades', 0)
        win_rate = env_state.get('win_rate', 0.0)
        
        if total_trades < self.min_trades:
            return 0.0
        
        # –ë–æ–Ω—É—Å –∑–∞ –≤–∏—Å–æ–∫—É –¥–æ–ª—é –ø—Ä–∏–±—É—Ç–∫–æ–≤–∏—Ö —É–≥–æ–¥
        if win_rate > 0.6:
            return (win_rate - 0.5) * 2 * self.weight
        elif win_rate < 0.4:
            return -(0.5 - win_rate) * 2 * self.weight
        
        return 0.0


class VolatilityPenalty(BaseRewardScheme):
    """–®—Ç—Ä–∞—Ñ –∑–∞ –≤–∏—Å–æ–∫—É –≤–æ–ª–∞—Ç–∏–ª—å–Ω—ñ—Å—Ç—å –ø–æ—Ä—Ç—Ñ–µ–ª—è."""
    
    def __init__(self, weight: float = -0.1, window: int = 20):
        super().__init__(weight)
        self.window = window
    
    def calculate(self, env_state: Dict) -> float:
        portfolio_history = env_state.get('portfolio_history', [])
        
        if len(portfolio_history) < self.window:
            return 0.0
        
        # –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ –≤–æ–ª–∞—Ç—ñ–ª—å–Ω—ñ—Å—Ç—å –æ—Å—Ç–∞–Ω–Ω—ñ—Ö –∑–Ω–∞—á–µ–Ω—å
        recent_values = portfolio_history[-self.window:]
        returns = np.diff(recent_values) / recent_values[:-1]
        volatility = np.std(returns)
        
        # –®—Ç—Ä–∞—Ñ –∑–∞ –≤–∏—Å–æ–∫—É –≤–æ–ª–∞—Ç—ñ–ª—å–Ω—ñ—Å—Ç—å
        if volatility > 0.05:  # 5% –¥–µ–Ω–Ω–∞ –≤–æ–ª–∞—Ç—ñ–ª—å–Ω—ñ—Å—Ç—å
            return -volatility * 10 * abs(self.weight)
        
        return 0.0


class ConsistencyReward(BaseRewardScheme):
    """–í–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑–∞ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ñ—Å—Ç—å –ø—Ä–∏–±—É—Ç–∫—É."""
    
    def __init__(self, weight: float = 0.15, window: int = 30):
        super().__init__(weight)
        self.window = window
    
    def calculate(self, env_state: Dict) -> float:
        portfolio_history = env_state.get('portfolio_history', [])
        
        if len(portfolio_history) < self.window:
            return 0.0
        
        # –ê–Ω–∞–ª—ñ–∑—É—î–º–æ –æ—Å—Ç–∞–Ω–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è
        recent_values = portfolio_history[-self.window:]
        returns = np.diff(recent_values) / recent_values[:-1]
        
        # –î–æ–ª—è –ø–æ–∑–∏—Ç–∏–≤–Ω–∏—Ö –¥–Ω—ñ–≤
        positive_days_ratio = np.sum(returns > 0) / len(returns)
        
        # –ë–æ–Ω—É—Å –∑–∞ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ñ—Å—Ç—å
        if positive_days_ratio > 0.6:
            return (positive_days_ratio - 0.5) * 2 * self.weight
        
        return 0.0


class PerformanceDeclineReward(BaseRewardScheme):
    """
    –î–∏–Ω–∞–º—ñ—á–Ω–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ —â–æ –∑–º–µ–Ω—à—É—î—Ç—å—Å—è –ø—Ä–∏ –ø–æ–≥—ñ—Ä—à–µ–Ω–Ω—ñ –∫–ª—é—á–æ–≤–∏—Ö –º–µ—Ç—Ä–∏–∫:
    - –ö—ñ–ª—å–∫—ñ—Å—Ç—å —É–≥–æ–¥
    - Win rate (–≤—ñ–¥—Å–æ—Ç–æ–∫ –ø—Ä–∏–±—É—Ç–∫–æ–≤–∏—Ö —É–≥–æ–¥)
    - –ü—Ä–æ—Å–∞–¥–∫–∞
    """
    
    def __init__(self, weight: float = -2.0, history_window: int = 10, decline_threshold: float = 0.15):
        super().__init__(weight)
        self.history_window = history_window
        self.decline_threshold = decline_threshold  # 15% –ø–æ–≥—ñ—Ä—à–µ–Ω–Ω—è
        
        # –Ü—Å—Ç–æ—Ä—ñ—è –º–µ—Ç—Ä–∏–∫
        self.trades_history = []
        self.win_rate_history = []
        self.drawdown_history = []
        
        self.last_trades = 0
        self.last_win_rate = 0.0
        self.last_max_drawdown = 0.0
    
    def calculate(self, env_state: Dict) -> float:
        current_trades = env_state.get('total_trades', 0)
        current_win_rate = env_state.get('win_rate', 0.0)
        current_max_drawdown = env_state.get('max_drawdown', 0.0)
        
        # –û–Ω–æ–≤–ª—é—î–º–æ —ñ—Å—Ç–æ—Ä—ñ—é –º–µ—Ç—Ä–∏–∫
        self.trades_history.append(current_trades)
        self.win_rate_history.append(current_win_rate)
        self.drawdown_history.append(current_max_drawdown)
        
        # –û–±–º–µ–∂—É—î–º–æ —Ä–æ–∑–º—ñ—Ä —ñ—Å—Ç–æ—Ä—ñ—ó
        if len(self.trades_history) > self.history_window:
            self.trades_history = self.trades_history[-self.history_window:]
            self.win_rate_history = self.win_rate_history[-self.history_window:]
            self.drawdown_history = self.drawdown_history[-self.history_window:]
        
        # –ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ —ñ—Å—Ç–æ—Ä—ñ—ó –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É
        if len(self.trades_history) < 3:
            return 0.0
        
        total_penalty = 0.0
        
        # 1. –ê–ù–ê–õ–Ü–ó –ö–Ü–õ–¨–ö–û–°–¢–Ü –£–ì–û–î (–ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∑–º–µ–Ω—à–µ–Ω–Ω—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ)
        if len(self.trades_history) >= 6:
            recent_trades_data = self.trades_history[-3:]
            older_trades_data = self.trades_history[-6:-3]
            
            if len(recent_trades_data) >= 3 and len(older_trades_data) >= 3:
                recent_trades = np.mean(recent_trades_data)
                older_trades = np.mean(older_trades_data)
                
                if older_trades > 0:
                    trades_change = (recent_trades - older_trades) / older_trades
                    if trades_change < -self.decline_threshold:  # –ó–º–µ–Ω—à–µ–Ω–Ω—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ –Ω–∞ 15%+
                        penalty = abs(trades_change) * 2.0  # –®—Ç—Ä–∞—Ñ –ø—Ä–æ–ø–æ—Ä—Ü—ñ–π–Ω–∏–π –∑–º–µ–Ω—à–µ–Ω–Ω—é
                        total_penalty += penalty
                        print(f"üîª TRADE ACTIVITY DECLINE: {trades_change:.1%} -> Penalty: {penalty:.2f}")
        
        # 2. –ê–ù–ê–õ–Ü–ó WIN RATE (–ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∑–º–µ–Ω—à–µ–Ω–Ω—è –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ)
        if len(self.win_rate_history) >= 6:
            recent_win_rate_data = [x for x in self.win_rate_history[-3:] if x > 0]
            older_win_rate_data = [x for x in self.win_rate_history[-6:-3] if x > 0]
            
            # –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø: –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —â–æ —Å–ø–∏—Å–∫–∏ –Ω–µ –ø—É—Å—Ç—ñ –ø–µ—Ä–µ–¥ np.mean
            if len(recent_win_rate_data) > 0 and len(older_win_rate_data) > 0:
                recent_win_rate = np.mean(recent_win_rate_data)
                older_win_rate = np.mean(older_win_rate_data)
                
                if older_win_rate > 0.1 and recent_win_rate > 0:  # –¢—ñ–ª—å–∫–∏ —è–∫—â–æ —î –∑–Ω–∞—á—É—â—ñ –¥–∞–Ω—ñ
                    win_rate_change = (recent_win_rate - older_win_rate) / older_win_rate
                    if win_rate_change < -self.decline_threshold:  # –ó–º–µ–Ω—à–µ–Ω–Ω—è –≤–∏–Ω—Ä–µ–π—Ç—É –Ω–∞ 15%+
                        penalty = abs(win_rate_change) * 3.0  # –ë—ñ–ª—å—à–∏–π —à—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≥—ñ—Ä—à–µ–Ω–Ω—è —è–∫–æ—Å—Ç—ñ
                        total_penalty += penalty
                        print(f"üîª WIN RATE DECLINE: {win_rate_change:.1%} -> Penalty: {penalty:.2f}")
        
        # 3. –ê–ù–ê–õ–Ü–ó –ü–†–û–°–ê–î–ö–ò (–ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∑–±—ñ–ª—å—à–µ–Ω–Ω—è —Ä–∏–∑–∏–∫—É)
        if len(self.drawdown_history) >= 6:
            recent_drawdown_data = self.drawdown_history[-3:]
            older_drawdown_data = self.drawdown_history[-6:-3]
            
            if len(recent_drawdown_data) >= 3 and len(older_drawdown_data) >= 3:
                recent_drawdown = np.mean(recent_drawdown_data)
                older_drawdown = np.mean(older_drawdown_data)
                
                if older_drawdown > 0.001:  # –¢—ñ–ª—å–∫–∏ —è–∫—â–æ –±—É–ª–∞ –ø—Ä–æ—Å–∞–¥–∫–∞
                    drawdown_change = (recent_drawdown - older_drawdown) / older_drawdown
                    if drawdown_change > self.decline_threshold:  # –ó–±—ñ–ª—å—à–µ–Ω–Ω—è –ø—Ä–æ—Å–∞–¥–∫–∏ –Ω–∞ 15%+
                        penalty = drawdown_change * 2.5  # –®—Ç—Ä–∞—Ñ –∑–∞ –∑–±—ñ–ª—å—à–µ–Ω–Ω—è —Ä–∏–∑–∏–∫—É
                        total_penalty += penalty
                        print(f"üîª DRAWDOWN INCREASE: {drawdown_change:.1%} -> Penalty: {penalty:.2f}")
        
        # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ —à—Ç—Ä–∞—Ñ (–Ω–µ–≥–∞—Ç–∏–≤–Ω—É –≤–∏–Ω–∞–≥–æ—Ä–æ–¥—É)
        final_penalty = -total_penalty * abs(self.weight) if total_penalty > 0 else 0.0
        
        # –õ–æ–≥—É–≤–∞–Ω–Ω—è –∑–Ω–∞—á–Ω–∏—Ö —à—Ç—Ä–∞—Ñ—ñ–≤
        if final_penalty < -1.0:
            print(f"üìâ PERFORMANCE DECLINE PENALTY: {final_penalty:.2f}")
        
        return final_penalty
    
    def reset(self):
        self.trades_history = []
        self.win_rate_history = []
        self.drawdown_history = []
        self.last_trades = 0
        self.last_win_rate = 0.0
        self.last_max_drawdown = 0.0


def create_default_reward_scheme() -> CompositeRewardScheme:
    """–°—Ç–≤–æ—Ä–∏—Ç–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É —Å—Ö–µ–º—É –≤–∏–Ω–∞–≥–æ—Ä–æ–¥."""
    schemes = [
        ProfitReward(weight=1.0),
        DrawdownPenalty(weight=-0.5),
        SharpeRatioReward(weight=0.3),
        TradeQualityReward(weight=0.2),
        VolatilityPenalty(weight=-0.1),
        ConsistencyReward(weight=0.15)
    ]
    return CompositeRewardScheme(schemes)


def create_conservative_reward_scheme() -> CompositeRewardScheme:
    """–°—Ç–≤–æ—Ä–∏—Ç–∏ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—É —Å—Ö–µ–º—É –≤–∏–Ω–∞–≥–æ—Ä–æ–¥ (–∞–∫—Ü–µ–Ω—Ç –Ω–∞ —Å—Ç–∞–±—ñ–ª—å–Ω—ñ—Å—Ç—å)."""
    schemes = [
        ProfitReward(weight=0.7),
        DrawdownPenalty(weight=-1.0),
        SharpeRatioReward(weight=0.5),
        VolatilityPenalty(weight=-0.3),
        ConsistencyReward(weight=0.4)
    ]
    return CompositeRewardScheme(schemes)


def create_aggressive_reward_scheme() -> CompositeRewardScheme:
    """–°—Ç–≤–æ—Ä–∏—Ç–∏ –∞–≥—Ä–µ—Å–∏–≤–Ω—É —Å—Ö–µ–º—É –≤–∏–Ω–∞–≥–æ—Ä–æ–¥ (–∞–∫—Ü–µ–Ω—Ç –Ω–∞ –ø—Ä–∏–±—É—Ç–æ–∫)."""
    schemes = [
        ProfitReward(weight=1.5),
        DrawdownPenalty(weight=-0.2),
        TradeQualityReward(weight=0.3),
        SharpeRatioReward(weight=0.2)
    ]
    return CompositeRewardScheme(schemes)


class StaticReward(BaseRewardScheme):
    """–ü—Ä–æ—Å—Ç–∞ —Å—Ç–∞—Ç–∏—á–Ω–∞ —Å—Ö–µ–º–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥ –∑ —á—ñ—Ç–∫–∏–º–∏ –ø–æ–∑–∏—Ç–∏–≤–Ω–∏–º–∏/–Ω–µ–≥–∞—Ç–∏–≤–Ω–∏–º–∏ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞–º–∏."""
    
    def __init__(self, weight: float = 1.0, static_initial_balance: float = None):
        super().__init__(weight)
        self.step_count = 0
        self.last_portfolio_value = None
        self.static_initial_balance = static_initial_balance  # –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø: —Å—Ç–∞—Ç–∏—á–Ω–∏–π –ø–æ—á–∞—Ç–∫–æ–≤–∏–π –±–∞–ª–∞–Ω—Å
        self.initial_balance = None
        
    def calculate(self, env_state: Dict) -> float:
        portfolio_value = env_state['portfolio_value']
        current_step = env_state.get('step', 0)
        
        # –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø: –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –°–¢–ê–¢–ò–ß–ù–ò–ô –ø–æ—á–∞—Ç–∫–æ–≤–∏–π –±–∞–ª–∞–Ω—Å
        if self.initial_balance is None:
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Å—Ç–∞—Ç–∏—á–Ω–∏–π –±–∞–ª–∞–Ω—Å –∞–±–æ –±–µ—Ä–µ–º–æ –∑ env_state
            if self.static_initial_balance is not None:
                self.initial_balance = self.static_initial_balance
            else:
                # –û—Ç—Ä–∏–º—É—î–º–æ –ø–æ—á–∞—Ç–∫–æ–≤–∏–π –±–∞–ª–∞–Ω—Å –∑ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞
                self.initial_balance = env_state.get('initial_balance', 10000.0)
            self.last_portfolio_value = portfolio_value
            
        self.step_count += 1
        
        # –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø: –ë–µ–∑–ø–µ—á–Ω–∏–π —Ä–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –∑–º—ñ–Ω–∏ –≤–∞—Ä—Ç–æ—Å—Ç—ñ –ø–æ—Ä—Ç—Ñ–µ–ª—è
        step_change = portfolio_value - self.last_portfolio_value if self.last_portfolio_value is not None else 0
        step_change_percent = step_change / self.last_portfolio_value if self.last_portfolio_value is not None and self.last_portfolio_value > 0 else 0
        
        # –†–æ–∑—Ä–∞—Ö—É—î–º–æ –∑–∞–≥–∞–ª—å–Ω—É –ø—Ä–∏–±—É—Ç–∫–æ–≤—ñ—Å—Ç—å
        total_return_percent = (portfolio_value - self.initial_balance) / self.initial_balance
        
        # –î–£–ñ–ï –ü–†–û–°–¢–ê —Å—Ç–∞—Ç–∏—á–Ω–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑ –≥–∞—Ä–∞–Ω—Ç–æ–≤–∞–Ω–æ—é –≤–∞—Ä—ñ–∞—Ü—ñ—î—é
        base_reward = 0.0
        
        # 1. –û—Å–Ω–æ–≤–Ω–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ –∑–º—ñ–Ω–∏ –ø–æ—Ä—Ç—Ñ–µ–ª—è
        if abs(step_change_percent) > 0.005:  # –ó–Ω–∞—á–Ω–∞ –∑–º—ñ–Ω–∞ > 0.5%
            if step_change_percent > 0:
                base_reward = 3.0  # –í–µ–ª–∏–∫–∏–π –ø—Ä–∏–±—É—Ç–æ–∫
            else:
                base_reward = -3.0  # –í–µ–ª–∏–∫–∏–π –∑–±–∏—Ç–æ–∫
        elif abs(step_change_percent) > 0.001:  # –ü–æ–º—ñ—Ä–Ω–∞ –∑–º—ñ–Ω–∞ > 0.1%
            if step_change_percent > 0:
                base_reward = 1.0  # –ü–æ–º—ñ—Ä–Ω–∏–π –ø—Ä–∏–±—É—Ç–æ–∫
            else:
                base_reward = -1.0  # –ü–æ–º—ñ—Ä–Ω–∏–π –∑–±–∏—Ç–æ–∫
        elif abs(step_change_percent) > 0.0001:  # –ú–∞–ª–∞ –∑–º—ñ–Ω–∞ > 0.01%
            if step_change_percent > 0:
                base_reward = 0.5  # –ú–∞–ª–∏–π –ø—Ä–∏–±—É—Ç–æ–∫
            else:
                base_reward = -0.5  # –ú–∞–ª–∏–π –∑–±–∏—Ç–æ–∫
        else:
            base_reward = -0.2  # –®—Ç—Ä–∞—Ñ –∑–∞ –≤—ñ–¥—Å—É—Ç–Ω—ñ—Å—Ç—å –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
        
        # 2. –î–æ–¥–∞—î–º–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç, —â–æ –∑–º—ñ–Ω—é—î—Ç—å—Å—è –≤—ñ–¥ –∫—Ä–æ–∫—É –¥–æ –∫—Ä–æ–∫—É
        step_variation = np.sin(current_step * 0.1) * 0.3  # –í–∞—Ä—ñ–∞—Ü—ñ—è ¬±0.3
        
        # 3. –î–æ–¥–∞—î–º–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –∑–∞–ª–µ–∂–Ω–∏–π –≤—ñ–¥ –∑–∞–≥–∞–ª—å–Ω–æ—ó –ø—Ä–∏–±—É—Ç–∫–æ–≤–æ—Å—Ç—ñ
        performance_bonus = 0.0
        if total_return_percent > 0.1:  # > 10% –ø—Ä–∏–±—É—Ç–æ–∫
            performance_bonus = 2.0
        elif total_return_percent > 0.05:  # > 5% –ø—Ä–∏–±—É—Ç–æ–∫
            performance_bonus = 1.0
        elif total_return_percent < -0.1:  # > 10% –∑–±–∏—Ç–æ–∫
            performance_bonus = -2.0
        elif total_return_percent < -0.05:  # > 5% –∑–±–∏—Ç–æ–∫
            performance_bonus = -1.0
        
        # 4. –î–æ–¥–∞—î–º–æ –Ω–µ–≤–µ–ª–∏–∫–∏–π —Ä–∞–Ω–¥–æ–º–Ω–∏–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –¥–ª—è –≥–∞—Ä–∞–Ω—Ç–æ–≤–∞–Ω–æ—ó –≤–∞—Ä—ñ–∞—Ü—ñ—ó
        random_component = np.random.uniform(-0.2, 0.2)
        
        # –ü—ñ–¥—Å—É–º–∫–æ–≤–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞
        final_reward = base_reward + step_variation + performance_bonus + random_component
        
        # –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–µ –ª–æ–≥—É–≤–∞–Ω–Ω—è - —Ç—ñ–ª—å–∫–∏ –ø—Ä–∏ –∑–Ω–∞—á–Ω–∏—Ö –∑–º—ñ–Ω–∞—Ö –∞–±–æ –ø–µ—Ä—ñ–æ–¥–∏—á–Ω–æ
        if current_step % 1000 == 0 and current_step > 0:  # –í–∏–≤–æ–¥–∏–º–æ —Ç—ñ–ª—å–∫–∏ –∫–æ–∂–Ω—ñ 1000 –∫—Ä–æ–∫—ñ–≤
            print(f"Step {current_step}: Portfolio: {portfolio_value:.2f}, Return: {total_return_percent:.2%}, Reward: {final_reward:.2f}")
        
        self.last_portfolio_value = portfolio_value
        
        # –û–±–º–µ–∂—É—î–º–æ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥—É –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ –Ω–∞–≤—á–∞–Ω–Ω—è
        final_reward = np.clip(final_reward * self.weight, -10.0, 10.0)
        return final_reward
    
    def reset(self):
        self.step_count = 0
        self.last_portfolio_value = None
        # –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø: –ù–ï —Å–∫–∏–¥–∞—î–º–æ initial_balance, —â–æ–± –∑–±–µ—Ä–µ–≥—Ç–∏ —Å—Ç–∞—Ç–∏—á–Ω—ñ—Å—Ç—å
        # self.initial_balance = None  # –ó–∞–∫–æ–º–µ–Ω—Ç–æ–≤–∞–Ω–æ –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Å—Ç–∞—Ç–∏—á–Ω–æ–≥–æ –±–∞–ª–∞–Ω—Å—É


class AdaptiveTradeOffReward(BaseRewardScheme):
    """
    –ê–¥–∞–ø—Ç–∏–≤–Ω–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ —â–æ –¥–æ–∑–≤–æ–ª—è—î –º–æ–¥–µ–ª—ñ –≤–∏–±–∏—Ä–∞—Ç–∏ –º—ñ–∂ —Ä—ñ–∑–Ω–∏–º–∏ —Ü—ñ–ª—è–º–∏:
    - –ü—Ä–∏–±—É—Ç–æ–∫ vs –ü—Ä–æ—Å–∞–¥–∫–∞
    - –ê–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å vs –Ø–∫—ñ—Å—Ç—å —É–≥–æ–¥
    - –°—Ç–∞–±—ñ–ª—å–Ω—ñ—Å—Ç—å vs –ê–≥—Ä–µ—Å–∏–≤–Ω—ñ—Å—Ç—å
    """
    
    def __init__(self, weight: float = 10.0, adaptation_window: int = 50):
        super().__init__(weight)
        self.adaptation_window = adaptation_window
        self.performance_history = []
        self.current_strategy = "balanced"  # balanced, profit_focused, risk_focused
        self.strategy_counter = 0
        
    def calculate(self, env_state: Dict) -> float:
        portfolio_value = env_state.get('portfolio_value', 10000)
        total_return = env_state.get('total_return', 0.0)
        max_drawdown = env_state.get('max_drawdown', 0.0)
        win_rate = env_state.get('win_rate', 0.0)
        total_trades = env_state.get('total_trades', 0)
        
        # –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø: –§—ñ–ª—å—Ç—Ä—É—î–º–æ –±–µ–∑–≥–ª—É–∑–¥—ñ –∑–∞–ø–∏—Å–∏ –∑ –Ω—É–ª—å–æ–≤–∏–º–∏ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ç—ñ–ª—å–∫–∏ –∑–Ω–∞—á—É—â—ñ –¥–∞–Ω—ñ (–∑ —Ç–æ—Ä–≥–æ–≤–æ—é –∞–∫—Ç–∏–≤–Ω—ñ—Å—Ç—é –∞–±–æ –∑–Ω–∞—á—É—â–∏–º–∏ –∑–º—ñ–Ω–∞–º–∏)
        is_meaningful_data = (
            total_trades > 0 or  # –Ñ —Ç–æ—Ä–≥–æ–≤–∞ –∞–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å
            abs(total_return) > 0.001 or  # –Ñ –∑–Ω–∞—á—É—â—ñ –∑–º—ñ–Ω–∏ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—ñ (>0.1%)
            max_drawdown > 0.001  # –Ñ –∑–Ω–∞—á—É—â–∞ –ø—Ä–æ—Å–∞–¥–∫–∞ (>0.1%)
        )
        
        if is_meaningful_data:
            self.performance_history.append({
                'return': total_return,
                'drawdown': max_drawdown,
                'win_rate': win_rate,
                'trades': total_trades
            })
            
            # –û–±–º–µ–∂—É—î–º–æ —ñ—Å—Ç–æ—Ä—ñ—é
            if len(self.performance_history) > self.adaptation_window:
                self.performance_history = self.performance_history[-self.adaptation_window:]
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω–∏–π –≤–∏–±—ñ—Ä —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó –∫–æ–∂–Ω—ñ 20 –∫—Ä–æ–∫—ñ–≤ (—Ç—ñ–ª—å–∫–∏ —è–∫—â–æ —î –¥–æ—Å—Ç–∞—Ç–Ω—å–æ –∑–Ω–∞—á—É—â–∏—Ö –¥–∞–Ω–∏—Ö)
        self.strategy_counter += 1
        if self.strategy_counter % 20 == 0 and len(self.performance_history) > 5:
            self.current_strategy = self._choose_strategy()
        
        # –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥—É –∑–≥—ñ–¥–Ω–æ –ø–æ—Ç–æ—á–Ω–æ—ó —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó
        reward = 0.0
        
        if self.current_strategy == "profit_focused":
            # –ê–∫—Ü–µ–Ω—Ç –Ω–∞ –ø—Ä–∏–±—É—Ç–æ–∫
            reward = total_return * 15.0  # –°–∏–ª—å–Ω–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑–∞ –ø—Ä–∏–±—É—Ç–æ–∫
            if max_drawdown > 0.15:  # –¢—ñ–ª—å–∫–∏ –∫—Ä–∏—Ç–∏—á–Ω–∞ –ø—Ä–æ—Å–∞–¥–∫–∞ –∫–∞—Ä–∞—î—Ç—å—Å—è
                reward -= max_drawdown * 5.0
                
        elif self.current_strategy == "risk_focused":
            # –ê–∫—Ü–µ–Ω—Ç –Ω–∞ –∫–æ–Ω—Ç—Ä–æ–ª—å —Ä–∏–∑–∏–∫—ñ–≤
            reward = total_return * 8.0  # –ü–æ–º—ñ—Ä–Ω–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑–∞ –ø—Ä–∏–±—É—Ç–æ–∫
            reward -= max_drawdown * 20.0  # –°–∏–ª—å–Ω–µ –ø–æ–∫–∞—Ä–∞–Ω–Ω—è –∑–∞ –ø—Ä–æ—Å–∞–¥–∫—É
            if win_rate > 0.7:  # –ë–æ–Ω—É—Å –∑–∞ –≤–∏—Å–æ–∫—É —è–∫—ñ—Å—Ç—å
                reward += win_rate * 3.0
                
        else:  # balanced
            # –ó–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω–∏–π –ø—ñ–¥—Ö—ñ–¥ –∑ trade-off –º—ñ–∂ —Ü—ñ–ª—è–º–∏
            profit_component = total_return * 12.0
            risk_component = -max_drawdown * 10.0
            quality_component = win_rate * 2.0 if total_trades > 0 else 0
            
            # –î–æ–∑–≤–æ–ª—è—î–º–æ –º–æ–¥–µ–ª—ñ –≤–∏–±—Ä–∞—Ç–∏ –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç
            if total_return > 0.05:  # –ü—Ä–∏ —Ö–æ—Ä–æ—à–æ–º—É –ø—Ä–∏–±—É—Ç–∫—É - –¥–æ–∑–≤–æ–ª—è—î–º–æ –±—ñ–ª—å—à—É –ø—Ä–æ—Å–∞–¥–∫—É
                risk_component *= 0.5
            elif max_drawdown < 0.03:  # –ü—Ä–∏ –Ω–∏–∑—å–∫—ñ–π –ø—Ä–æ—Å–∞–¥—Ü—ñ - –∑–∞–æ—Ö–æ—á—É—î–º–æ –±—ñ–ª—å—à–∏–π —Ä–∏–∑–∏–∫
                profit_component *= 1.5
                
            reward = profit_component + risk_component + quality_component
        
        return reward * self.weight
    
    def _choose_strategy(self) -> str:
        """–í–∏–±–∏—Ä–∞—î –Ω–∞–π–∫—Ä–∞—â—É —Å—Ç—Ä–∞—Ç–µ–≥—ñ—é –Ω–∞ –æ—Å–Ω–æ–≤—ñ –Ω–µ–¥–∞–≤–Ω—å–æ—ó –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ."""
        if len(self.performance_history) < 10:
            return "balanced"
        
        recent_performance = self.performance_history[-10:]
        
        # –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø: –ë–µ–∑–ø–µ—á–Ω–∏–π —Ä–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ —Å–µ—Ä–µ–¥–Ω—ñ—Ö –∑–Ω–∞—á–µ–Ω—å –∑ –ø–µ—Ä–µ–≤—ñ—Ä–∫–æ—é –Ω–∞ –ø—É—Å—Ç—ñ –º–∞—Å–∏–≤–∏
        returns = [p['return'] for p in recent_performance]
        drawdowns = [p['drawdown'] for p in recent_performance]
        win_rates = [p['win_rate'] for p in recent_performance if p['win_rate'] > 0]
        
        # –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ —Å–µ—Ä–µ–¥–Ω—ñ –∑ –ø–µ—Ä–µ–≤—ñ—Ä–∫–æ—é –Ω–∞ –ø—É—Å—Ç—ñ —Å–ø–∏—Å–∫–∏
        avg_return = np.mean(returns) if len(returns) > 0 else 0.0
        avg_drawdown = np.mean(drawdowns) if len(drawdowns) > 0 else 0.0
        avg_win_rate = np.mean(win_rates) if len(win_rates) > 0 else 0.0
        
        # –õ–æ–≥—ñ–∫–∞ –≤–∏–±–æ—Ä—É —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó
        if avg_return < -0.02 and avg_drawdown > 0.1:
            # –ü–æ–≥–∞–Ω–∞ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å - —Ñ–æ–∫—É—Å –Ω–∞ —Ä–∏–∑–∏–∫–∏
            print(f"üõ°Ô∏è STRATEGY: Switching to RISK_FOCUSED (return={avg_return:.2%}, dd={avg_drawdown:.2%})")
            return "risk_focused"
        elif avg_return > 0.05 and avg_drawdown < 0.08:
            # –•–æ—Ä–æ—à–∞ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å - —Ñ–æ–∫—É—Å –Ω–∞ –ø—Ä–∏–±—É—Ç–æ–∫
            print(f"üí∞ STRATEGY: Switching to PROFIT_FOCUSED (return={avg_return:.2%}, dd={avg_drawdown:.2%})")
            return "profit_focused"
        else:
            # –ó–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω–∏–π –ø—ñ–¥—Ö—ñ–¥
            print(f"‚öñÔ∏è STRATEGY: Staying BALANCED (return={avg_return:.2%}, dd={avg_drawdown:.2%})")
            return "balanced"
    
    def reset(self):
        self.performance_history = []
        self.current_strategy = "balanced"
        self.strategy_counter = 0


def create_optimized_reward_scheme() -> CompositeRewardScheme:
    """
    –ü–û–ö–†–ê–©–ï–ù–ê —Å—Ö–µ–º–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥ –¥–ª—è –º–∞–∫—Å–∏–º—ñ–∑–∞—Ü—ñ—ó –ø—Ä–∏–±—É—Ç–∫–æ–≤–æ—Å—Ç—ñ:
    - –§–æ–∫—É—Å –Ω–∞ –ø—Ä–∏–±—É—Ç–æ–∫ –∑ –º—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–º–∏ —à—Ç—Ä–∞—Ñ–∞–º–∏
    - –ó–º–µ–Ω—à–µ–Ω—ñ penalty –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏
    - –ó–±—ñ–ª—å—à–µ–Ω–∞ —Ç–æ–ª–µ—Ä–∞–Ω—Ç–Ω—ñ—Å—Ç—å –¥–æ —Ä–∏–∑–∏–∫—ñ–≤
    """
    schemes = [
        # –ì–û–õ–û–í–ù–ò–ô –ö–û–ú–ü–û–ù–ï–ù–¢: –ü—Ä–∏–±—É—Ç–æ–∫ –∑ –ø—ñ–¥–≤–∏—â–µ–Ω–æ—é –≤–∞–≥–æ—é
        ProfitReward(weight=2.0, normalize=True),  # –ó–±—ñ–ª—å—à–µ–Ω–æ –≤–∞–≥—É –ø—Ä–∏–±—É—Ç–∫—É
        
        # –ú'—è–∫–∏–π –∫–æ–Ω—Ç—Ä–æ–ª—å –ø—Ä–æ—Å–∞–¥–∫–∏ —Ç—ñ–ª—å–∫–∏ –ø—Ä–∏ –∫—Ä–∏—Ç–∏—á–Ω–∏—Ö —Ä—ñ–≤–Ω—è—Ö
        DrawdownPenalty(weight=-0.3, max_drawdown_threshold=0.25),  # –ó–±—ñ–ª—å—à–µ–Ω–æ –ø–æ—Ä—ñ–≥ –¥–æ 25%
        
        # –í–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑–∞ —è–∫—ñ—Å—Ç—å —É–≥–æ–¥ –∑ –º'—è–∫–∏–º–∏ —É–º–æ–≤–∞–º–∏
        TradeQualityReward(weight=0.3, min_trades=1),  # –ó–º–µ–Ω—à–µ–Ω–æ –º—ñ–Ω—ñ–º—É–º —É–≥–æ–¥
        
        # –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–π Sharpe bonus
        SharpeRatioReward(weight=0.2, window=30),  # –ó–º–µ–Ω—à–µ–Ω–∞ –≤–∞–≥–∞
        
        # –í–ò–ú–ö–ù–ï–ù–û –∞–≥—Ä–µ—Å–∏–≤–Ω—ñ penalty –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏:
        # - PerformanceDeclineReward (–∑–∞–Ω–∞–¥—Ç–æ —à—Ç—Ä–∞—Ñ—É—î –∑–∞ –ø—Ä–∏—Ä–æ–¥–Ω—ñ –∫–æ–ª–∏–≤–∞–Ω–Ω—è)
        # - VolatilityPenalty (–æ–±–º–µ–∂—É—î —Ç–æ—Ä–≥–æ–≤—É –∞–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å)
        # - ConsistencyReward (–º–æ–∂–µ —à—Ç—Ä–∞—Ñ—É–≤–∞—Ç–∏ –∑–∞ –∞–≥—Ä–µ—Å–∏–≤–Ω—É —Ç–æ—Ä–≥—ñ–≤–ª—é)
    ]
    
    # –í–ò–ú–ö–ù–ï–ù–û –¥–∏–Ω–∞–º—ñ—á–Ω–µ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è –¥–ª—è –±—ñ–ª—å—à–æ—ó —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ
    composite = CompositeRewardScheme(schemes, enable_dynamic_scaling=False)
    
    return composite


def create_bear_market_optimized_reward_scheme() -> CompositeRewardScheme:
    """
    –°–ü–ï–¶–Ü–ê–õ–¨–ù–ê —Å—Ö–µ–º–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥ –¥–ª—è –ø–∞–¥–∞—é—á–∏—Ö —Ä–∏–Ω–∫—ñ–≤ - –í–ò–ü–†–ê–í–õ–ï–ù–ê:
    - –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–µ –∑–∞–æ—Ö–æ—á–µ–Ω–Ω—è –¢–Ü–õ–¨–ö–ò –∑–∞ —Ä–µ–∞–ª—å–Ω–∏–π –ø—Ä–∏–±—É—Ç–æ–∫ –ø–æ—Ä—Ç—Ñ–µ–ª—è
    - –ú—ñ–Ω—ñ–º–∞–ª—å–Ω—ñ —à—Ç—Ä–∞—Ñ–∏ –∑–∞ —Ä–∏–∑–∏–∫–∏ –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
    - –ë–æ–Ω—É—Å–∏ –∑–∞ –ø—Ä–∏–±—É—Ç–∫–æ–≤—É —Ç–æ—Ä–≥—ñ–≤–ª—é –≤ —Å–∫–ª–∞–¥–Ω–∏—Ö —Ä–∏–Ω–∫–æ–≤–∏—Ö —É–º–æ–≤–∞—Ö
    - –í–ò–ú–ö–ù–ï–ù–û –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∏ –∑–∞ —Å–∞–º–µ –ø–∞–¥—ñ–Ω–Ω—è —Ü—ñ–Ω
    """
    schemes = [
        # –ì–û–õ–û–í–ù–ò–ô –ö–û–ú–ü–û–ù–ï–ù–¢: –ê–≥—Ä–µ—Å–∏–≤–Ω–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑–∞ –ø—Ä–∏–±—É—Ç–æ–∫ –ø–æ—Ä—Ç—Ñ–µ–ª—è
        ProfitReward(weight=3.0, normalize=True),  # –ü–æ—Ç—Ä—ñ–π–Ω–∞ –≤–∞–≥–∞ - –æ—Å–Ω–æ–≤–Ω–∏–π –¥—Ä–∞–π–≤–µ—Ä –Ω–∞–≤—á–∞–Ω–Ω—è
        
        # –ú–Ü–ù–Ü–ú–ê–õ–¨–ù–ò–ô –∫–æ–Ω—Ç—Ä–æ–ª—å –ø—Ä–æ—Å–∞–¥–∫–∏ —Ç—ñ–ª—å–∫–∏ –ø—Ä–∏ –∫—Ä–∏—Ç–∏—á–Ω–∏—Ö —Ä—ñ–≤–Ω—è—Ö
        DrawdownPenalty(weight=-0.1, max_drawdown_threshold=0.35),  # –î—É–∂–µ –≤–∏—Å–æ–∫–∏–π –ø–æ—Ä—ñ–≥ 35% - –Ω–µ –∑–∞–≤–∞–∂–∞—î –∞–∫—Ç–∏–≤–Ω—ñ–π —Ç–æ—Ä–≥—ñ–≤–ª—ñ
        
        # –í–ò–ù–ê–ì–û–†–û–î–ê –∑–∞ —è–∫—ñ—Å—Ç—å —É–≥–æ–¥ (–≤–∏—Å–æ–∫–∏–π –≤–∏–Ω—Ä–µ–π—Ç)
        TradeQualityReward(weight=0.5, min_trades=1),  # –ó–∞–æ—Ö–æ—á—É—î–º–æ —è–∫—ñ—Å–Ω—ñ —Ä—ñ—à–µ–Ω–Ω—è
        
        # –í–ò–ü–†–ê–í–õ–ï–ù–ò–ô –ë–û–ù–£–° –∑–∞ –ø—Ä–∏–±—É—Ç–∫–æ–≤—É —Ç–æ—Ä–≥—ñ–≤–ª—é –ø—Ä–∏ –ø–∞–¥–∞—é—á–æ–º—É —Ä–∏–Ω–∫—É
        BearMarketActivityReward(weight=1.0),  # –¢–µ–ø–µ—Ä –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∂—É—î –¢–Ü–õ–¨–ö–ò –∑–∞ –ø—Ä–∏–±—É—Ç–æ–∫, –∞ –Ω–µ –∑–∞ –ø–∞–¥—ñ–Ω–Ω—è
        
        # –í–ò–ü–†–ê–í–õ–ï–ù–ò–ô –ë–û–ù–£–° –∑–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–∏–π —Ç–∞–π–º–∏–Ω–≥ —Ç–∞ –ø—Ä–∏–±—É—Ç–∫–æ–≤—ñ –æ–ø–µ—Ä–∞—Ü—ñ—ó
        MarketTimingReward(weight=0.8),  # –¢–µ–ø–µ—Ä –±–∞–∑—É—î—Ç—å—Å—è –Ω–∞ –ø—Ä–∏–±—É—Ç–∫–æ–≤–æ—Å—Ç—ñ –ø–æ—Ä—Ç—Ñ–µ–ª—è
    ]
    
    # –ë–µ–∑ –¥–∏–Ω–∞–º—ñ—á–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ—ó —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ –Ω–∞–≤—á–∞–Ω–Ω—è
    composite = CompositeRewardScheme(schemes, enable_dynamic_scaling=False)
    
    return composite


class BearMarketActivityReward(BaseRewardScheme):
    """–í–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑–∞ –ø—Ä–∏–±—É—Ç–∫–æ–≤—É —Ç–æ—Ä–≥—ñ–≤–ª—é –ø—ñ–¥ —á–∞—Å –ø–∞–¥–∞—é—á–æ–≥–æ —Ä–∏–Ω–∫—É - –í–ò–ü–†–ê–í–õ–ï–ù–ê –õ–û–ì–Ü–ö–ê."""
    
    def __init__(self, weight: float = 1.0, market_decline_threshold: float = -0.05):
        super().__init__(weight)
        self.market_decline_threshold = market_decline_threshold  # -5% —Å–ø–∞–¥ –∑–∞ –ø–µ—Ä—ñ–æ–¥ –¥–ª—è –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –≤–µ–¥–º–µ–∂–æ–≥–æ —Ä–∏–Ω–∫—É
        self.price_history = []
        self.last_portfolio_value = None
        
    def calculate(self, env_state: Dict) -> float:
        current_price = env_state.get('current_price', 0)
        total_trades = env_state.get('total_trades', 0)
        portfolio_value = env_state.get('portfolio_value', 0)
        
        if current_price <= 0:
            return 0.0
            
        # –í–µ–¥–µ–º–æ —ñ—Å—Ç–æ—Ä—ñ—é —Ü—ñ–Ω –¥–ª—è –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Ä–∏–Ω–∫–æ–≤–æ–≥–æ —Ç—Ä–µ–Ω–¥—É
        self.price_history.append(current_price)
        if len(self.price_history) > 20:  # –û—Å—Ç–∞–Ω–Ω—ñ 20 –∫—Ä–æ–∫—ñ–≤
            self.price_history = self.price_history[-20:]
            
        if len(self.price_history) < 10:
            # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î–º–æ –ø–æ—á–∞—Ç–∫–æ–≤–µ –∑–Ω–∞—á–µ–Ω–Ω—è –ø–æ—Ä—Ç—Ñ–µ–ª—è
            if self.last_portfolio_value is None:
                self.last_portfolio_value = portfolio_value
            return 0.0
            
        # –í–∏–∑–Ω–∞—á–∞—î–º–æ —á–∏ —Ä–∏–Ω–æ–∫ –ø–∞–¥–∞—î (–≤–µ–¥–º–µ–∂–∏–π —Ç—Ä–µ–Ω–¥)
        price_change = (self.price_history[-1] - self.price_history[0]) / self.price_history[0]
        is_bear_market = price_change < self.market_decline_threshold
        
        # –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ –∑–º—ñ–Ω—É –ø–æ—Ä—Ç—Ñ–µ–ª—è –∑ –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ –∫—Ä–æ–∫—É
        portfolio_change = 0.0
        if self.last_portfolio_value is not None and self.last_portfolio_value > 0:
            portfolio_change = portfolio_value - self.last_portfolio_value
        
        reward = 0.0
        
        if is_bear_market:
            # –í–ò–ü–†–ê–í–õ–ï–ù–û: –í–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –¢–Ü–õ–¨–ö–ò –∑–∞ —Ä–µ–∞–ª—å–Ω–∏–π –ü–†–ò–ë–£–¢–û–ö –ø–æ—Ä—Ç—Ñ–µ–ª—è –ø—Ä–∏ –ø–∞–¥–∞—é—á–æ–º—É —Ä–∏–Ω–∫—É
            if portfolio_change > 0:
                # –í–µ–ª–∏–∫–∏–π –±–æ–Ω—É—Å –∑–∞ –∑–±—ñ–ª—å—à–µ–Ω–Ω—è –ø–æ—Ä—Ç—Ñ–µ–ª—è –ø—ñ–¥ —á–∞—Å –ø–∞–¥—ñ–Ω–Ω—è —Ä–∏–Ω–∫—É
                profit_percentage = portfolio_change / self.last_portfolio_value
                profit_bonus = profit_percentage * 50.0  # 1% –ø—Ä–∏–±—É—Ç–∫—É = +0.5 –±–æ–Ω—É—Å—É
                reward += min(profit_bonus, 8.0)  # –ú–∞–∫—Å–∏–º—É–º +8.0
                
                # –î–æ–¥–∞—Ç–∫–æ–≤–∏–π –±–æ–Ω—É—Å –∑–∞ —Ç–æ—Ä–≥–æ–≤—É –∞–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å –ø—Ä–∏ –ø—Ä–∏–±—É—Ç–∫–æ–≤—ñ–π —Ç–æ—Ä–≥—ñ–≤–ª—ñ
                if total_trades > 0:
                    activity_bonus = min(total_trades * 0.1, 1.5)  # –î–æ +1.5 –∑–∞ –∞–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å
                    reward += activity_bonus
                    
            # –í–ò–ü–†–ê–í–õ–ï–ù–û: –ú–∞–ª–∏–π —à—Ç—Ä–∞—Ñ –∑–∞ –≤—Ç—Ä–∞—Ç–∏ –ø—Ä–∏ –ø–∞–¥–∞—é—á–æ–º—É —Ä–∏–Ω–∫—É (–º'—è–∫–∏–π —Å–ø–æ—Å—ñ–± –Ω–∞–≤—á–∏—Ç–∏ —É–Ω–∏–∫–∞—Ç–∏ –≤—Ç—Ä–∞—Ç)
            elif portfolio_change < -100:  # –¢—ñ–ª—å–∫–∏ –ø—Ä–∏ –∑–Ω–∞—á–Ω–∏—Ö –≤—Ç—Ä–∞—Ç–∞—Ö > $100
                loss_percentage = abs(portfolio_change) / self.last_portfolio_value
                loss_penalty = -loss_percentage * 5.0  # –ú'—è–∫–∏–π —à—Ç—Ä–∞—Ñ –∑–∞ –≤—Ç—Ä–∞—Ç–∏
                reward += max(loss_penalty, -2.0)  # –ú–∞–∫—Å–∏–º—É–º -2.0 —à—Ç—Ä–∞—Ñ—É
            
            # –í–ò–ú–ö–ù–ï–ù–û: –ù–ï–ü–†–ê–í–ò–õ–¨–ù–ê –ª–æ–≥—ñ–∫–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥ –∑–∞ –ø–∞–¥—ñ–Ω–Ω—è —Ü—ñ–Ω
            # –ù–ï –î–û–î–ê–Ñ–ú–û –±–æ–Ω—É—Å–∏ –∑–∞ —Å–∞–º–æ –ø–∞–¥—ñ–Ω–Ω—è —Ä–∏–Ω–∫—É!
            # –ê–≥–µ–Ω—Ç –º–∞—î –æ—Ç—Ä–∏–º—É–≤–∞—Ç–∏ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∏ —Ç—ñ–ª—å–∫–∏ –∑–∞ –ø—Ä–∏–±—É—Ç–æ–∫ –ø–æ—Ä—Ç—Ñ–µ–ª—è!
            
        # –û–Ω–æ–≤–ª—é—î–º–æ —ñ—Å—Ç–æ—Ä—ñ—é –ø–æ—Ä—Ç—Ñ–µ–ª—è
        self.last_portfolio_value = portfolio_value
        
        return reward * self.weight
        
    def reset(self):
        self.price_history = []


class MarketTimingReward(BaseRewardScheme):
    """–í–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–∏–π —Ç–∞–π–º–∏–Ω–≥ –æ–ø–µ—Ä–∞—Ü—ñ–π - –í–ò–ü–†–ê–í–õ–ï–ù–ê –õ–û–ì–Ü–ö–ê."""
    
    def __init__(self, weight: float = 0.8):
        super().__init__(weight)
        self.last_action = None
        self.last_price = None
        self.last_portfolio_value = None
        
    def calculate(self, env_state: Dict) -> float:
        current_price = env_state.get('current_price', 0)
        current_action = env_state.get('last_action', 0)  # 0=—É—Ç—Ä–∏–º—É–≤–∞—Ç–∏, 1=–∫—É–ø—É–≤–∞—Ç–∏, 2=–ø—Ä–æ–¥–∞–≤–∞—Ç–∏
        portfolio_value = env_state.get('portfolio_value', 10000)
        
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –ø—Ä–∏ –ø–µ—Ä—à–æ–º—É –≤–∏–∫–ª–∏–∫—É
        if current_price <= 0 or self.last_price is None:
            self.last_price = current_price
            self.last_action = current_action
            self.last_portfolio_value = portfolio_value
            return 0.0
            
        # –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ –∑–º—ñ–Ω—É —Ü—ñ–Ω–∏ —Ç–∞ –ø–æ—Ä—Ç—Ñ–µ–ª—è
        price_change = (current_price - self.last_price) / self.last_price
        portfolio_change = 0.0
        if self.last_portfolio_value is not None and self.last_portfolio_value > 0:
            portfolio_change = portfolio_value - self.last_portfolio_value
        
        reward = 0.0
        
        # –í–ò–ü–†–ê–í–õ–ï–ù–û: –í–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –¢–Ü–õ–¨–ö–ò –∑–∞ —Ä–µ–∞–ª—å–Ω–µ –∑–±—ñ–ª—å—à–µ–Ω–Ω—è –ø–æ—Ä—Ç—Ñ–µ–ª—è
        if portfolio_change > 0:
            # –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ –≤—ñ–¥—Å–æ—Ç–æ–∫ –ø—Ä–∏–±—É—Ç–∫—É
            profit_percentage = portfolio_change / self.last_portfolio_value
            
            # –ë–∞–∑–æ–≤–∏–π –±–æ–Ω—É—Å –∑–∞ –ø—Ä–∏–±—É—Ç–æ–∫
            base_profit_bonus = profit_percentage * 20.0  # 1% –ø—Ä–∏–±—É—Ç–∫—É = +0.2 –±–æ–Ω—É—Å—É
            reward += min(base_profit_bonus, 5.0)  # –ú–∞–∫—Å–∏–º—É–º +5.0
            
            # –î–û–î–ê–¢–ö–û–í–ò–ô –±–æ–Ω—É—Å –∑–∞ –ø—Ä–∏–±—É—Ç–æ–∫ –ø—Ä–∏ –ø–∞–¥–∞—é—á–æ–º—É —Ä–∏–Ω–∫—É (—Å–∫–ª–∞–¥–Ω—ñ—à–∞ —É–º–æ–≤–∞)
            if price_change < -0.02:  # –¶—ñ–Ω–∞ –≤–ø–∞–ª–∞ –±—ñ–ª—å—à–µ –Ω—ñ–∂ –Ω–∞ 2%
                bear_market_bonus = profit_percentage * 30.0  # –î–æ–¥–∞—Ç–∫–æ–≤–∏–π –º–Ω–æ–∂–Ω–∏–∫ –∑–∞ —Ç–æ—Ä–≥—ñ–≤–ª—é –ø—Ä–æ—Ç–∏ —Ç—Ä–µ–Ω–¥—É
                reward += min(bear_market_bonus, 3.0)  # –ú–∞–∫—Å–∏–º—É–º +3.0 –¥–æ–¥–∞—Ç–∫–æ–≤–æ
                
            # –ë–û–ù–£–° –∑–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–∏–π —Ç–∞–π–º–∏–Ω–≥ —É–≥–æ–¥ (—è–∫—â–æ –¥—ñ—è –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î —Ä–∏–Ω–∫—É)
            if self.last_action == 1 and price_change > 0.01:  # –ö—É–ø—É–≤–∞–ª–∏ –ø–µ—Ä–µ–¥ –∑—Ä–æ—Å—Ç–∞–Ω–Ω—è–º
                timing_bonus = profit_percentage * 15.0
                reward += min(timing_bonus, 2.0)  # –ú–∞–∫—Å–∏–º—É–º +2.0
            elif self.last_action == 2 and price_change < -0.01:  # –ü—Ä–æ–¥–∞–≤–∞–ª–∏ –ø–µ—Ä–µ–¥ –ø–∞–¥—ñ–Ω–Ω—è–º
                timing_bonus = profit_percentage * 15.0
                reward += min(timing_bonus, 2.0)  # –ú–∞–∫—Å–∏–º—É–º +2.0
        
        # –í–ò–ü–†–ê–í–õ–ï–ù–û: –ú'—è–∫–∏–π —à—Ç—Ä–∞—Ñ –∑–∞ –≤—Ç—Ä–∞—Ç–∏ (—Ç—ñ–ª—å–∫–∏ –ø—Ä–∏ –∑–Ω–∞—á–Ω–∏—Ö –≤—Ç—Ä–∞—Ç–∞—Ö)
        elif portfolio_change < -50:  # –í—Ç—Ä–∞—Ç–∏ –±—ñ–ª—å—à–µ $50
            loss_percentage = abs(portfolio_change) / self.last_portfolio_value
            loss_penalty = -loss_percentage * 10.0  # –ú'—è–∫–∏–π —à—Ç—Ä–∞—Ñ –∑–∞ –≤—Ç—Ä–∞—Ç–∏
            reward += max(loss_penalty, -3.0)  # –ú–∞–∫—Å–∏–º—É–º -3.0 —à—Ç—Ä–∞—Ñ—É
        
        # –í–ò–ú–ö–ù–ï–ù–û: –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞ –ª–æ–≥—ñ–∫–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥ –∑–∞ –ø–∞–¥—ñ–Ω–Ω—è —Ü—ñ–Ω
        # –ê–≥–µ–Ω—Ç –ù–ï –º–∞—î –æ—Ç—Ä–∏–º—É–≤–∞—Ç–∏ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∏ –∑–∞ —Å–∞–º–µ –ø–∞–¥—ñ–Ω–Ω—è —Ä–∏–Ω–∫—É!
        # –í–∏–Ω–∞–≥–æ—Ä–æ–¥–∏ –º–∞—é—Ç—å –±–∞–∑—É–≤–∞—Ç–∏—Å—è –¢–Ü–õ–¨–ö–ò –Ω–∞ –ø—Ä–∏–±—É—Ç–∫–æ–≤–æ—Å—Ç—ñ –ø–æ—Ä—Ç—Ñ–µ–ª—è!
        
        # –û–Ω–æ–≤–ª—é—î–º–æ —Å—Ç–∞–Ω –¥–ª—è –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ –∫—Ä–æ–∫—É
        self.last_price = current_price
        self.last_action = current_action
        self.last_portfolio_value = portfolio_value
        
        return reward * self.weight
        
    def reset(self):
        """–°–∫–∏–¥–∞–Ω–Ω—è —Å—Ç–∞–Ω—É —Å—Ö–µ–º–∏ –ø—Ä–∏ –ø–æ—á–∞—Ç–∫—É –Ω–æ–≤–æ–≥–æ –µ–ø—ñ–∑–æ–¥—É."""
        self.last_action = None
        self.last_price = None
        self.last_portfolio_value = None


def create_static_reward_scheme(initial_balance: float = 10000.0) -> StaticReward:
    """–°—Ç–≤–æ—Ä–∏—Ç–∏ –ø—Ä–æ—Å—Ç—É —Å—Ç–∞—Ç–∏—á–Ω—É —Å—Ö–µ–º—É –≤–∏–Ω–∞–≥–æ—Ä–æ–¥ –∑ —Ñ—ñ–∫—Å–æ–≤–∞–Ω–∏–º –ø–æ—á–∞—Ç–∫–æ–≤–∏–º –±–∞–ª–∞–Ω—Å–æ–º."""
    return StaticReward(weight=1.0, static_initial_balance=initial_balance)