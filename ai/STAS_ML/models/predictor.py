"""
–û—Å–Ω–æ–≤–Ω—ã–µ ML –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–Ω—ã—Ö —Ü–µ–Ω.
"""

import numpy as np
import pandas as pd
import joblib
from typing import Dict, Any, Optional, Tuple
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score, classification_report
import warnings
warnings.filterwarnings('ignore')

try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False

try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import DataLoader, TensorDataset
    PYTORCH_AVAILABLE = True
except ImportError:
    PYTORCH_AVAILABLE = False


class LSTMModel(nn.Module):
    """LSTM –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤."""
    
    def __init__(self, input_size: int, hidden_size: int = 50, num_layers: int = 2, 
                 output_size: int = 1, dropout: float = 0.2):
        super(LSTMModel, self).__init__()
        
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, 
                           dropout=dropout, batch_first=True)
        self.dropout = nn.Dropout(dropout)
        self.linear = nn.Linear(hidden_size, output_size)
        
    def forward(self, x):
        # x shape: (batch_size, sequence_length, input_size)
        lstm_out, _ = self.lstm(x)
        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—ã—Ö–æ–¥
        lstm_out = lstm_out[:, -1, :]
        lstm_out = self.dropout(lstm_out)
        predictions = self.linear(lstm_out)
        return predictions


class CryptoPricePredictor:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è ML –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–Ω—ã—Ö —Ü–µ–Ω."""
    
    def __init__(self, config):
        self.config = config
        self.model = None
        self.model_type = config.model_type
        self.is_classification = config.target_type == 'direction'
        
    def _create_model(self):
        """–°–æ–∑–¥–∞—Ç—å –º–æ–¥–µ–ª—å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞."""
        if self.model_type == 'xgboost':
            if not XGBOOST_AVAILABLE:
                raise ImportError("XGBoost –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install xgboost")
            
            if self.is_classification:
                self.model = xgb.XGBClassifier(**self.config.xgb_params)
            else:
                self.model = xgb.XGBRegressor(**self.config.xgb_params)
                
        elif self.model_type == 'random_forest':
            if self.is_classification:
                self.model = RandomForestClassifier(**self.config.rf_params)
            else:
                self.model = RandomForestRegressor(**self.config.rf_params)
                
        elif self.model_type == 'linear':
            if self.is_classification:
                self.model = LogisticRegression(random_state=self.config.random_state, max_iter=1000)
            else:
                self.model = LinearRegression()
                
        elif self.model_type == 'lstm':
            if not PYTORCH_AVAILABLE:
                raise ImportError("PyTorch –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install torch")
            # LSTM –º–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–µ—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ –≤ –º–µ—Ç–æ–¥–µ train
            pass
        else:
            raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø –º–æ–¥–µ–ª–∏: {self.model_type}")
    
    def train(self, X_train: np.ndarray, y_train: np.ndarray, 
              X_val: np.ndarray, y_val: np.ndarray) -> Dict[str, Any]:
        """–û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å."""
        print(f"üöÄ –û–±—É—á–∞–µ–º {self.model_type} –º–æ–¥–µ–ª—å...")
        
        if self.model_type == 'lstm':
            return self._train_lstm(X_train, y_train, X_val, y_val)
        else:
            return self._train_sklearn_model(X_train, y_train, X_val, y_val)
    
    def _train_sklearn_model(self, X_train: np.ndarray, y_train: np.ndarray,
                           X_val: np.ndarray, y_val: np.ndarray) -> Dict[str, Any]:
        """–û–±—É—á–∏—Ç—å sklearn/xgboost –º–æ–¥–µ–ª—å."""
        self._create_model()
        
        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
        self.model.fit(X_train, y_train)
        
        # –î–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        train_pred = self.model.predict(X_train)
        val_pred = self.model.predict(X_val)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        metrics = self._calculate_metrics(y_train, train_pred, y_val, val_pred)
        
        print(f"‚úÖ –ú–æ–¥–µ–ª—å {self.model_type} –æ–±—É—á–µ–Ω–∞!")
        self._print_metrics(metrics)
        
        return metrics
    
    def _train_lstm(self, X_train: np.ndarray, y_train: np.ndarray,
                   X_val: np.ndarray, y_val: np.ndarray) -> Dict[str, Any]:
        """–û–±—É—á–∏—Ç—å LSTM –º–æ–¥–µ–ª—å."""
        # –ü–µ—Ä–µ—Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è LSTM
        # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º —á—Ç–æ X_train –∏–º–µ–µ—Ç shape (samples, features)
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ (samples, sequence_length, features_per_timestep)
        
        sequence_length = self.config.lookback_window
        features_per_timestep = X_train.shape[1] // sequence_length
        
        X_train_lstm = X_train.reshape(X_train.shape[0], sequence_length, features_per_timestep)
        X_val_lstm = X_val.reshape(X_val.shape[0], sequence_length, features_per_timestep)
        
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
        input_size = features_per_timestep
        output_size = 1
        
        self.model = LSTMModel(
            input_size=input_size,
            hidden_size=self.config.lstm_params['hidden_size'],
            num_layers=self.config.lstm_params['num_layers'],
            output_size=output_size,
            dropout=self.config.lstm_params['dropout']
        )
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—É—á–µ–Ω–∏—è
        criterion = nn.MSELoss() if not self.is_classification else nn.BCEWithLogitsLoss()
        optimizer = optim.Adam(self.model.parameters(), lr=self.config.lstm_params['learning_rate'])
        
        # –°–æ–∑–¥–∞–µ–º DataLoader
        train_dataset = TensorDataset(
            torch.FloatTensor(X_train_lstm), 
            torch.FloatTensor(y_train.reshape(-1, 1))
        )
        train_loader = DataLoader(train_dataset, batch_size=self.config.lstm_params['batch_size'], shuffle=True)
        
        # –û–±—É—á–µ–Ω–∏–µ
        self.model.train()
        train_losses = []
        
        for epoch in range(self.config.lstm_params['epochs']):
            epoch_loss = 0
            for batch_X, batch_y in train_loader:
                optimizer.zero_grad()
                outputs = self.model(batch_X)
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()
                epoch_loss += loss.item()
            
            avg_loss = epoch_loss / len(train_loader)
            train_losses.append(avg_loss)
            
            if (epoch + 1) % 20 == 0:
                print(f"Epoch {epoch+1}/{self.config.lstm_params['epochs']}, Loss: {avg_loss:.6f}")
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        self.model.eval()
        with torch.no_grad():
            train_pred = self.model(torch.FloatTensor(X_train_lstm)).numpy().flatten()
            val_pred = self.model(torch.FloatTensor(X_val_lstm)).numpy().flatten()
            
            if self.is_classification:
                train_pred = (torch.sigmoid(torch.FloatTensor(train_pred)) > 0.5).numpy().astype(int)
                val_pred = (torch.sigmoid(torch.FloatTensor(val_pred)) > 0.5).numpy().astype(int)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        metrics = self._calculate_metrics(y_train, train_pred, y_val, val_pred)
        metrics['train_losses'] = train_losses
        
        print(f"‚úÖ LSTM –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞!")
        self._print_metrics(metrics)
        
        return metrics
    
    def _calculate_metrics(self, y_train: np.ndarray, train_pred: np.ndarray,
                          y_val: np.ndarray, val_pred: np.ndarray) -> Dict[str, Any]:
        """–í—ã—á–∏—Å–ª–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª–∏."""
        metrics = {}
        
        if self.is_classification:
            # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            metrics['train_accuracy'] = accuracy_score(y_train, train_pred)
            metrics['val_accuracy'] = accuracy_score(y_val, val_pred)
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            try:
                from sklearn.metrics import precision_score, recall_score, f1_score
                metrics['val_precision'] = precision_score(y_val, val_pred, average='weighted')
                metrics['val_recall'] = recall_score(y_val, val_pred, average='weighted')
                metrics['val_f1'] = f1_score(y_val, val_pred, average='weighted')
            except:
                pass
                
        else:
            # –†–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            metrics['train_mse'] = mean_squared_error(y_train, train_pred)
            metrics['val_mse'] = mean_squared_error(y_val, val_pred)
            metrics['train_mae'] = mean_absolute_error(y_train, train_pred)
            metrics['val_mae'] = mean_absolute_error(y_val, val_pred)
            
            # R¬≤ score
            try:
                from sklearn.metrics import r2_score
                metrics['train_r2'] = r2_score(y_train, train_pred)
                metrics['val_r2'] = r2_score(y_val, val_pred)
            except:
                pass
        
        return metrics
    
    def _print_metrics(self, metrics: Dict[str, Any]):
        """–í—ã–≤–µ—Å—Ç–∏ –º–µ—Ç—Ä–∏–∫–∏."""
        print("\nüìä –ú–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª–∏:")
        print("-" * 40)
        
        if self.is_classification:
            print(f"Train Accuracy: {metrics.get('train_accuracy', 0):.4f}")
            print(f"Val Accuracy:   {metrics.get('val_accuracy', 0):.4f}")
            if 'val_f1' in metrics:
                print(f"Val F1-score:   {metrics['val_f1']:.4f}")
        else:
            print(f"Train MSE: {metrics.get('train_mse', 0):.6f}")
            print(f"Val MSE:   {metrics.get('val_mse', 0):.6f}")
            print(f"Train MAE: {metrics.get('train_mae', 0):.6f}")
            print(f"Val MAE:   {metrics.get('val_mae', 0):.6f}")
            if 'val_r2' in metrics:
                print(f"Val R¬≤:    {metrics['val_r2']:.4f}")
        
        print("-" * 40)
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """–°–¥–µ–ª–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è."""
        if self.model is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞! –í—ã–∑–æ–≤–∏—Ç–µ —Å–Ω–∞—á–∞–ª–∞ train()")
        
        if self.model_type == 'lstm':
            # –î–ª—è LSTM –Ω—É–∂–Ω–æ –ø–µ—Ä–µ—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ
            sequence_length = self.config.lookback_window
            features_per_timestep = X.shape[1] // sequence_length
            X_lstm = X.reshape(X.shape[0], sequence_length, features_per_timestep)
            
            self.model.eval()
            with torch.no_grad():
                predictions = self.model(torch.FloatTensor(X_lstm)).numpy().flatten()
                
                if self.is_classification:
                    predictions = (torch.sigmoid(torch.FloatTensor(predictions)) > 0.5).numpy().astype(int)
        else:
            predictions = self.model.predict(X)
        
        return predictions
    
    def predict_proba(self, X: np.ndarray) -> np.ndarray:
        """–ü–æ–ª—É—á–∏—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏."""
        if not self.is_classification:
            raise ValueError("predict_proba –¥–æ—Å—Ç—É–ø–µ–Ω —Ç–æ–ª—å–∫–æ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏")
        
        if self.model_type == 'lstm':
            sequence_length = self.config.lookback_window
            features_per_timestep = X.shape[1] // sequence_length
            X_lstm = X.reshape(X.shape[0], sequence_length, features_per_timestep)
            
            self.model.eval()
            with torch.no_grad():
                logits = self.model(torch.FloatTensor(X_lstm)).numpy().flatten()
                probabilities = torch.sigmoid(torch.FloatTensor(logits)).numpy()
                
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –æ–±–µ–∏—Ö –∫–ª–∞—Å—Å–æ–≤
            return np.column_stack([1 - probabilities, probabilities])
        else:
            return self.model.predict_proba(X)
    
    def save(self, filepath: str):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å."""
        if self.model is None:
            raise ValueError("–ù–µ—Ç –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
        
        model_data = {
            'model': self.model,
            'model_type': self.model_type,
            'config': self.config,
            'is_classification': self.is_classification
        }
        
        if self.model_type == 'lstm':
            # –î–ª—è PyTorch –º–æ–¥–µ–ª–µ–π —Å–æ—Ö—Ä–∞–Ω—è–µ–º state_dict
            torch.save({
                'model_state_dict': self.model.state_dict(),
                'model_config': {
                    'input_size': self.model.lstm.input_size,
                    'hidden_size': self.model.hidden_size,
                    'num_layers': self.model.num_layers,
                    'output_size': 1,
                    'dropout': self.config.lstm_params['dropout']
                },
                'config': self.config,
                'model_type': self.model_type,
                'is_classification': self.is_classification
            }, filepath)
        else:
            joblib.dump(model_data, filepath)
        
        print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {filepath}")
    
    def load(self, filepath: str):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å."""
        if self.model_type == 'lstm':
            checkpoint = torch.load(filepath)
            model_config = checkpoint['model_config']
            
            self.model = LSTMModel(**model_config)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.config = checkpoint['config']
            self.is_classification = checkpoint['is_classification']
        else:
            model_data = joblib.load(filepath)
            self.model = model_data['model']
            self.model_type = model_data['model_type']
            self.config = model_data['config']
            self.is_classification = model_data['is_classification']
        
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {filepath}")
    
    def get_feature_importance(self) -> Optional[Dict[str, float]]:
        """–ü–æ–ª—É—á–∏—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–¥–ª—è tree-based –º–æ–¥–µ–ª–µ–π)."""
        if self.model_type in ['xgboost', 'random_forest'] and hasattr(self.model, 'feature_importances_'):
            return dict(zip(range(len(self.model.feature_importances_)), self.model.feature_importances_))
        else:
            print("–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ —Ç–∏–ø–∞ –º–æ–¥–µ–ª–∏")
            return None