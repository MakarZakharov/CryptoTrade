"""
–£–ª—å—Ç—Ä–∞-–∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π PPO –∞–≥–µ–Ω—Ç –¥–ª—è –≤—ã—Å–æ–∫–æ—á–∞—Å—Ç–æ—Ç–Ω–æ–π —Ç–æ—Ä–≥–æ–≤–ª–∏ –∏ —Å–∫–∞–ª—å–ø–∏–Ω–≥–∞.
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è 300% –≥–æ–¥–æ–≤–æ–π –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏ –Ω–∞ 15-–º–∏–Ω—É—Ç–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞—Ö.
"""

import os
import numpy as np
import torch
import torch.nn as nn
from stable_baselines3 import PPO
from stable_baselines3.common.policies import ActorCriticPolicy
from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv
from stable_baselines3.common.callbacks import BaseCallback, EvalCallback
from stable_baselines3.common.logger import configure
from stable_baselines3.common.utils import set_random_seed
from typing import Dict, Any, Optional, Callable
import time
from .base_agent import BaseAgent


class UltraAggressiveCallback(BaseCallback):
    """Callback –¥–ª—è —É–ª—å—Ç—Ä–∞-–∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º –∫–ª—é—á–µ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫."""
    
    def __init__(self, 
                 target_annual_return: float = 3.0,  # 300% –≥–æ–¥–æ–≤—ã—Ö
                 max_drawdown_limit: float = 0.20,   # 20% –º–∞–∫—Å–∏–º—É–º
                 min_win_rate: float = 0.60,         # 60% –º–∏–Ω–∏–º—É–º
                 auto_stop_training: bool = True,
                 verbose: int = 1):
        super(UltraAggressiveCallback, self).__init__(verbose)
        self.target_annual_return = target_annual_return
        self.max_drawdown_limit = max_drawdown_limit
        self.min_win_rate = min_win_rate
        self.auto_stop_training = auto_stop_training
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
        self.best_mean_reward = -np.inf
        self.episodes_completed = 0
        self.performance_history = []
        self.last_eval_timestep = 0
        self.eval_frequency = 10000  # –ö–∞–∂–¥—ã–µ 10k —à–∞–≥–æ–≤
        
        # –§–ª–∞–≥–∏ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
        self.target_reached = False
        self.risk_exceeded = False
        
    def _on_step(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ."""
        # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        if self.num_timesteps - self.last_eval_timestep >= self.eval_frequency:
            self._evaluate_performance()
            self.last_eval_timestep = self.num_timesteps
            
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–∏ —Ü–µ–ª–µ–π –∏–ª–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–∏ —Ä–∏—Å–∫–æ–≤
        if self.auto_stop_training and (self.target_reached or self.risk_exceeded):
            if self.verbose >= 1:
                reason = "—Ü–µ–ª—å –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞" if self.target_reached else "–ø—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç —Ä–∏—Å–∫–æ–≤"
                print(f"\nüõë –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {reason}")
            return False
            
        return True
    
    def _evaluate_performance(self):
        """–û—Ü–µ–Ω–∫–∞ —Ç–µ–∫—É—â–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–∞."""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —Å—Ä–µ–¥—ã
            if hasattr(self.training_env, 'get_attr'):
                env_infos = self.training_env.get_attr('_get_info')
                if env_infos and len(env_infos) > 0:
                    info = env_infos[0]()
                    
                    total_return = info.get('total_return', 0)
                    max_drawdown = info.get('max_drawdown', 0)
                    win_rate = info.get('win_rate', 0)
                    
                    # –ü—Ä–æ–µ—Ü–∏—Ä—É–µ–º –≥–æ–¥–æ–≤—É—é –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å
                    steps_per_day = 96  # 24—á * 4 –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞/—á–∞—Å
                    days_simulated = max(1, self.num_timesteps / steps_per_day / len(self.training_env.envs))
                    projected_annual = (1 + total_return) ** (365 / days_simulated) - 1
                    
                    performance = {
                        'timestep': self.num_timesteps,
                        'total_return': total_return,
                        'projected_annual': projected_annual,
                        'max_drawdown': max_drawdown,
                        'win_rate': win_rate,
                        'days_simulated': days_simulated
                    }
                    
                    self.performance_history.append(performance)
                    
                    # –õ–æ–≥–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏
                    self.logger.record("ultra_aggressive/total_return", total_return)
                    self.logger.record("ultra_aggressive/projected_annual_return", projected_annual)
                    self.logger.record("ultra_aggressive/max_drawdown", max_drawdown)
                    self.logger.record("ultra_aggressive/win_rate", win_rate)
                    
                    if self.verbose >= 1:
                        print(f"\nüìä –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ [–®–∞–≥ {self.num_timesteps:,}]:")
                        print(f"   –¢–µ–∫—É—â–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å: {total_return:.2%}")
                        print(f"   –ü—Ä–æ–µ–∫—Ü–∏—è –Ω–∞ –≥–æ–¥: {projected_annual:.1%}")
                        print(f"   –ú–∞–∫—Å. –ø—Ä–æ—Å–∞–¥–∫–∞: {max_drawdown:.2%}")
                        print(f"   Win rate: {win_rate:.1%}")
                        print(f"   –î–Ω–µ–π —Å–∏–º—É–ª—è—Ü–∏–∏: {days_simulated:.1f}")
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å–ª–æ–≤–∏—è –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
                    if projected_annual >= self.target_annual_return and win_rate >= self.min_win_rate:
                        if self.verbose >= 1:
                            print(f"üéØ –¶–µ–ª—å –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞! –î–æ—Ö–æ–¥–Ω–æ—Å—Ç—å: {projected_annual:.1%}, Win rate: {win_rate:.1%}")
                        self.target_reached = True
                        
                    if max_drawdown > self.max_drawdown_limit:
                        if self.verbose >= 1:
                            print(f"‚ö†Ô∏è –ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –ø—Ä–æ—Å–∞–¥–∫–∏: {max_drawdown:.2%} > {self.max_drawdown_limit:.2%}")
                        self.risk_exceeded = True
        
        except Exception as e:
            if self.verbose >= 1:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {e}")


class ScalpingPolicy(ActorCriticPolicy):
    """–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–æ–ª–∏—Ç–∏–∫–∞ –¥–ª—è —Å–∫–∞–ª—å–ø–∏–Ω–≥–∞ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π."""
    
    def __init__(self, *args, **kwargs):
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è —Å–∫–∞–ª—å–ø–∏–Ω–≥–∞
        super(ScalpingPolicy, self).__init__(*args, **kwargs)
    
    def _build_mlp_extractor(self) -> None:
        """–°–æ–∑–¥–∞—Ç—å —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–µ—Ç—å –¥–ª—è —Å–∫–∞–ª—å–ø–∏–Ω–≥–∞."""
        self.mlp_extractor = ScalpingMlpExtractor(
            self.features_dim,
            net_arch=dict(pi=[256, 256, 128], vf=[256, 256, 128]),  # –ë–æ–ª–µ–µ –≥–ª—É–±–æ–∫–∞—è —Å–µ—Ç—å
            activation_fn=nn.LeakyReLU,  # LeakyReLU –¥–ª—è –ª—É—á—à–∏—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
            device=self.device
        )


class ScalpingMlpExtractor(nn.Module):
    """–ò–∑–≤–ª–µ–∫–∞—Ç–µ–ª—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–ª—è —Å–∫–∞–ª—å–ø–∏–Ω–≥–∞."""
    
    def __init__(self, features_dim: int, net_arch: Dict, activation_fn, device):
        super(ScalpingMlpExtractor, self).__init__()
        
        # –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ policy network (actor)
        policy_layers = []
        last_layer_dim = features_dim
        
        for layer_size in net_arch['pi']:
            policy_layers.append(nn.Linear(last_layer_dim, layer_size))
            policy_layers.append(activation_fn())
            policy_layers.append(nn.Dropout(0.1))  # –ù–µ–±–æ–ª—å—à–æ–π dropout –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
            last_layer_dim = layer_size
        
        self.policy_net = nn.Sequential(*policy_layers)
        
        # –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ value network (critic)
        value_layers = []
        last_layer_dim = features_dim
        
        for layer_size in net_arch['vf']:
            value_layers.append(nn.Linear(last_layer_dim, layer_size))
            value_layers.append(activation_fn())
            value_layers.append(nn.Dropout(0.1))
            last_layer_dim = layer_size
            
        self.value_net = nn.Sequential(*value_layers)
        
        self.latent_dim_pi = net_arch['pi'][-1]
        self.latent_dim_vf = net_arch['vf'][-1]
    
    def forward(self, features):
        return self.forward_actor(features), self.forward_critic(features)
    
    def forward_actor(self, features):
        return self.policy_net(features)
    
    def forward_critic(self, features):
        return self.value_net(features)


class UltraAggressivePPOAgent(BaseAgent):
    """
    –£–ª—å—Ç—Ä–∞-–∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π PPO –∞–≥–µ–Ω—Ç –¥–ª—è –≤—ã—Å–æ–∫–æ—á–∞—Å—Ç–æ—Ç–Ω–æ–π —Ç–æ—Ä–≥–æ–≤–ª–∏.
    
    –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
    - –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è 15-–º–∏–Ω—É—Ç–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤
    - –ù–∞—Ü–µ–ª–µ–Ω –Ω–∞ 300% –≥–æ–¥–æ–≤—É—é –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å  
    - –°—Ç—Ä–æ–≥–∏–π –∫–æ–Ω—Ç—Ä–æ–ª—å —Ä–∏—Å–∫–æ–≤ (–º–∞–∫—Å 20% –ø—Ä–æ—Å–∞–¥–∫–∞)
    - –í—ã—Å–æ–∫–æ—á–∞—Å—Ç–æ—Ç–Ω–æ–µ –ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏–π
    - GPU —É—Å–∫–æ—Ä–µ–Ω–∏–µ
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–∏ —Ü–µ–ª–µ–π
    """
    
    def __init__(self, config, use_gpu: bool = True, multi_env: bool = True):
        super().__init__(config)
        self.use_gpu = use_gpu
        self.multi_env = multi_env
        self.model = None
        self.vec_env = None
        self.device = self._setup_device()
        
    def _setup_device(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π."""
        if self.use_gpu and torch.cuda.is_available():
            device = "cuda"
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"üöÄ –£–ª—å—Ç—Ä–∞-–∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –∞–≥–µ–Ω—Ç: GPU {gpu_name}")
            print(f"üíæ –î–æ—Å—Ç—É–ø–Ω–æ –ø–∞–º—è—Ç–∏: {gpu_memory:.1f} GB")
            
            # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è GPU –¥–ª—è –±—ã—Å—Ç—Ä—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
        else:
            device = "cpu"
            print("üîß –£–ª—å—Ç—Ä–∞-–∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –∞–≥–µ–Ω—Ç: CPU —Ä–µ–∂–∏–º")
            
        return device
    
    def create_model(self, env, model_config: Optional[Dict] = None):
        """–°–æ–∑–¥–∞—Ç—å —É–ª—å—Ç—Ä–∞-–∞–≥—Ä–µ—Å—Å–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å PPO."""
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º—É–ª—å—Ç–∏-–ø—Ä–æ—Ü–µ—Å—Å–Ω–æ–π —Å—Ä–µ–¥—ã –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        if self.multi_env and hasattr(env, 'unwrapped'):
            n_envs = min(4, os.cpu_count())  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–æ 4 –ø—Ä–æ—Ü–µ—Å—Å–æ–≤
            print(f"üîÑ –°–æ–∑–¥–∞–Ω–∏–µ {n_envs} –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö —Å—Ä–µ–¥ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è")
            
            def make_env(rank: int, seed: int = 0):
                def _init():
                    env_copy = type(env)(env.config)
                    env_copy.seed(seed + rank)
                    return env_copy
                set_random_seed(seed)
                return _init
            
            self.vec_env = SubprocVecEnv([make_env(i) for i in range(n_envs)])
        else:
            self.vec_env = DummyVecEnv([lambda: env])
        
        # –£–ª—å—Ç—Ä–∞-–∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è 300% –≥–æ–¥–æ–≤—ã—Ö
        ultra_aggressive_config = {
            # –û–±—É—á–µ–Ω–∏–µ
            'learning_rate': 5e-5,  # –ë–æ–ª–µ–µ –º–µ–¥–ª–µ–Ω–Ω–æ–µ, –Ω–æ —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
            'n_steps': 2048,  # –ë–æ–ª—å—à–µ —à–∞–≥–æ–≤ –¥–ª—è –ª—É—á—à–µ–≥–æ —Å–±–æ—Ä–∞ –æ–ø—ã—Ç–∞
            'batch_size': 256,  # –ë–æ–ª—å—à–∏–µ –±–∞—Ç—á–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            'n_epochs': 6,  # –ë–æ–ª—å—à–µ —ç–ø–æ—Ö –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è
            
            # –î–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞
            'gamma': 0.999,  # –û—á–µ–Ω—å –≤—ã—Å–æ–∫–æ–µ –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
            'gae_lambda': 0.99,  # –í—ã—Å–æ–∫–∏–π GAE –¥–ª—è —Ç–æ—á–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤
            
            # –ü–æ–ª–∏—Ç–∏–∫–∞
            'clip_range': 0.1,  # –ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–π –∫–ª–∏–ø–ø–∏–Ω–≥ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            'clip_range_vf': 0.1,  # –ö–ª–∏–ø–ø–∏–Ω–≥ —Ñ—É–Ω–∫—Ü–∏–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏
            
            # –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
            'ent_coef': 0.001,  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è –¥–ª—è —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏–∏
            'vf_coef': 0.5,  # –°—Ä–µ–¥–Ω–∏–π –≤–µ—Å —Ñ—É–Ω–∫—Ü–∏–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏
            'max_grad_norm': 0.5,  # –°—Ç—Ä–æ–≥–∏–π –∫–ª–∏–ø–ø–∏–Ω–≥ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
            
            # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
            'use_sde': False,  # –û—Ç–∫–ª—é—á–∞–µ–º —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ
            'sde_sample_freq': -1,
            'target_kl': 0.005,  # –û—á–µ–Ω—å —Å—Ç—Ä–æ–≥–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ KL
            'normalize_advantage': True,
            
            # –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
            'device': self.device,
            'verbose': 1,
            
            # –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–æ–ª–∏—Ç–∏–∫–∞
            'policy_kwargs': {
                'net_arch': dict(pi=[512, 256, 128], vf=[512, 256, 128]),
                'activation_fn': torch.nn.LeakyReLU,
                'ortho_init': True,
                'log_std_init': -1.5,  # –ë–æ–ª–µ–µ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ
                'full_std': False,
                'use_expln': True
            }
        }
        
        # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        if model_config:
            ultra_aggressive_config.update(model_config)
        
        print("üî• –°–æ–∑–¥–∞–Ω–∏–µ —É–ª—å—Ç—Ä–∞-–∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π PPO –º–æ–¥–µ–ª–∏...")
        print(f"   üìã –®–∞–≥–æ–≤ –Ω–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ: {ultra_aggressive_config['n_steps']}")
        print(f"   üì¶ –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {ultra_aggressive_config['batch_size']}")
        print(f"   üîÑ –≠–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è: {ultra_aggressive_config['n_epochs']}")
        print(f"   üéØ –¶–µ–ª–µ–≤–æ–π KL: {ultra_aggressive_config['target_kl']}")
        print(f"   üíª –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
        self.model = PPO(
            "MlpPolicy",
            self.vec_env,
            **ultra_aggressive_config
        )
        
        return self.model
    
    def train(self, 
              total_timesteps: int = 1000000,
              target_annual_return: float = 3.0,
              max_drawdown_limit: float = 0.20,
              min_win_rate: float = 0.60,
              auto_stop_training: bool = True,
              save_checkpoints: bool = True,
              checkpoint_frequency: int = 100000):
        """
        –û–±—É—á–∏—Ç—å —É–ª—å—Ç—Ä–∞-–∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞.
        
        Args:
            total_timesteps: –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è
            target_annual_return: –¶–µ–ª–µ–≤–∞—è –≥–æ–¥–æ–≤–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å (3.0 = 300%)
            max_drawdown_limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–æ–ø—É—Å—Ç–∏–º–∞—è –ø—Ä–æ—Å–∞–¥–∫–∞
            min_win_rate: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π win rate
            auto_stop_training: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–∏ —Ü–µ–ª–µ–π
            save_checkpoints: –°–æ—Ö—Ä–∞–Ω—è—Ç—å —á–µ–∫–ø–æ–∏–Ω—Ç—ã
            checkpoint_frequency: –ß–∞—Å—Ç–æ—Ç–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤
        """
        if not self.model:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ —Å–æ–∑–¥–∞–Ω–∞. –í—ã–∑–æ–≤–∏—Ç–µ create_model() —Å–Ω–∞—á–∞–ª–∞.")
        
        print(f"üî• –ó–∞–ø—É—Å–∫ —É–ª—å—Ç—Ä–∞-–∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è:")
        print(f"   üéØ –¶–µ–ª—å: {target_annual_return:.0%} –≥–æ–¥–æ–≤—ã—Ö")
        print(f"   üõ°Ô∏è –ú–∞–∫—Å. –ø—Ä–æ—Å–∞–¥–∫–∞: {max_drawdown_limit:.1%}")
        print(f"   üèÜ –ú–∏–Ω. win rate: {min_win_rate:.1%}")
        print(f"   üìà –í—Å–µ–≥–æ —à–∞–≥–æ–≤: {total_timesteps:,}")
        print(f"   ‚è±Ô∏è –†–∞—Å—á–µ—Ç–Ω–æ–µ –≤—Ä–µ–º—è: {total_timesteps / 10000:.0f} –º–∏–Ω—É—Ç")
        
        # Callback –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
        callback = UltraAggressiveCallback(
            target_annual_return=target_annual_return,
            max_drawdown_limit=max_drawdown_limit,
            min_win_rate=min_win_rate,
            auto_stop_training=auto_stop_training,
            verbose=1
        )
        
        # Callback –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤
        callbacks = [callback]
        
        if save_checkpoints:
            # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤
            checkpoint_dir = f"CryptoTrade/ai/DRL/models/ultra_aggressive_{int(time.time())}/checkpoints"
            os.makedirs(checkpoint_dir, exist_ok=True)
            
            # Callback –¥–ª—è –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            checkpoint_callback = CheckpointCallback(
                save_freq=checkpoint_frequency,
                save_path=checkpoint_dir,
                name_prefix="ultra_aggressive"
            )
            callbacks.append(checkpoint_callback)
        
        # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
        start_time = time.time()
        
        try:
            self.model.learn(
                total_timesteps=total_timesteps,
                callback=callbacks,
                progress_bar=True
            )
            
            training_time = time.time() - start_time
            print(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {training_time/60:.1f} –º–∏–Ω—É—Ç")
            
            # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
            if hasattr(callback, 'performance_history') and callback.performance_history:
                final_perf = callback.performance_history[-1]
                print(f"\nüèÜ –§–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
                print(f"   –î–æ—Ö–æ–¥–Ω–æ—Å—Ç—å: {final_perf['total_return']:.2%}")
                print(f"   –ì–æ–¥–æ–≤–∞—è –ø—Ä–æ–µ–∫—Ü–∏—è: {final_perf['projected_annual']:.1%}")
                print(f"   –ú–∞–∫—Å. –ø—Ä–æ—Å–∞–¥–∫–∞: {final_perf['max_drawdown']:.2%}")
                print(f"   Win rate: {final_perf['win_rate']:.1%}")
                
                if final_perf['projected_annual'] >= target_annual_return:
                    print("üéØ –¶–µ–ª—å –ø–æ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞!")
                if final_perf['win_rate'] >= min_win_rate:
                    print("üèÜ –¶–µ–ª—å –ø–æ win rate –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞!")
                if final_perf['max_drawdown'] <= max_drawdown_limit:
                    print("üõ°Ô∏è –†–∏—Å–∫–∏ –ø–æ–¥ –∫–æ–Ω—Ç—Ä–æ–ª–µ–º!")
            
        except KeyboardInterrupt:
            print("\n‚èπÔ∏è –û–±—É—á–µ–Ω–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            training_time = time.time() - start_time
            print(f"üìä –û–±—É—á–µ–Ω–æ –∑–∞ {training_time/60:.1f} –º–∏–Ω—É—Ç")
        
        return self.model
    
    def act(self, state):
        """–î–µ–π—Å—Ç–≤–∏–µ –∞–≥–µ–Ω—Ç–∞ - –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏."""
        if not self.model:
            return np.array([0.0])
        
        # –ë—ã—Å—Ç—Ä–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –±–µ–∑ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫
        action, _ = self.model.predict(state, deterministic=True)
        return action
    
    def save(self, path: str, save_replay_buffer: bool = False):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å."""
        if self.model:
            self.model.save(path)
            if save_replay_buffer and hasattr(self.model, 'replay_buffer'):
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –±—É—Ñ–µ—Ä–∞ –æ–ø—ã—Ç–∞ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                pass
            print(f"üíæ –£–ª—å—Ç—Ä–∞-–∞–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {path}")
    
    def load(self, path: str, env=None):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å."""
        if env:
            if self.multi_env:
                self.vec_env = SubprocVecEnv([lambda: env] * min(4, os.cpu_count()))
            else:
                self.vec_env = DummyVecEnv([lambda: env])
        
        self.model = PPO.load(path, env=self.vec_env)
        print(f"üìÅ –£–ª—å—Ç—Ä–∞-–∞–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {path}")
        return self.model


class CheckpointCallback(BaseCallback):
    """Callback –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤."""
    
    def __init__(self, save_freq: int, save_path: str, name_prefix: str = "model"):
        super(CheckpointCallback, self).__init__()
        self.save_freq = save_freq
        self.save_path = save_path
        self.name_prefix = name_prefix
    
    def _on_step(self) -> bool:
        if self.num_timesteps % self.save_freq == 0:
            path = os.path.join(self.save_path, f"{self.name_prefix}_{self.num_timesteps}_steps")
            self.model.save(path)
            if self.verbose >= 1:
                print(f"üíæ –ß–µ–∫–ø–æ–∏–Ω—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {path}")
        return True


def create_ultra_aggressive_agent(config, use_gpu: bool = True, multi_env: bool = True) -> UltraAggressivePPOAgent:
    """–ë—ã—Å—Ç—Ä–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ —É–ª—å—Ç—Ä–∞-–∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞."""
    return UltraAggressivePPOAgent(config, use_gpu=use_gpu, multi_env=multi_env)