import os
import numpy as np
import torch
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.callbacks import BaseCallback
from .base_agent import BaseAgent

class PPOAgent(BaseAgent):
    def __init__(self, config):
        super().__init__(config)
        self.model = None
        self.vec_env = None
        self.device = self._get_device()
        
    def _get_device(self):
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (GPU –∏–ª–∏ CPU)."""
        if torch.cuda.is_available():
            device = "cuda"
            gpu_name = torch.cuda.get_device_name(0)
            print(f"üöÄ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è GPU: {gpu_name}")
            print(f"üíæ –î–æ—Å—Ç—É–ø–Ω–∞—è –≤–∏–¥–µ–æ–ø–∞–º—è—Ç—å: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        else:
            device = "cpu"
            print("üîß GPU –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")
        return device
        
    def create_model(self, env, model_config=None):
        """–°–æ–∑–¥–∞—Ç—å –º–æ–¥–µ–ª—å PPO."""
        # –û–±–æ—Ä–∞—á–∏–≤–∞–µ–º —Å—Ä–µ–¥—É
        self.vec_env = DummyVecEnv([lambda: env])
        
        # –ü–û–ö–†–ê–©–ï–ù–Ü –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–ª—è –º–∞–∫—Å–∏–º—ñ–∑–∞—Ü—ñ—ó –ø—Ä–∏–±—É—Ç–∫–æ–≤–æ—Å—Ç—ñ
        default_config = {
            'learning_rate': 3e-4,  # –ó–±—ñ–ª—å—à–µ–Ω–æ –¥–ª—è —à–≤–∏–¥—à–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è
            'n_steps': 2048,  # –ó–±—ñ–ª—å—à–µ–Ω–æ –¥–ª—è –∫—Ä–∞—â–æ–≥–æ sampling
            'batch_size': 64,  # –ó–º–µ–Ω—à–µ–Ω–æ –¥–ª—è —á–∞—Å—Ç—ñ—à–∏—Ö –æ–Ω–æ–≤–ª–µ–Ω—å
            'n_epochs': 10,  # –ó–±—ñ–ª—å—à–µ–Ω–æ –¥–ª—è –≥–ª–∏–±—à–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è
            'gamma': 0.995,  # –ó–±—ñ–ª—å—à–µ–Ω–æ –¥–ª—è –¥–æ–≤–≥–æ—Å—Ç—Ä–æ–∫–æ–≤–æ–≥–æ –ø–ª–∞–Ω—É–≤–∞–Ω–Ω—è
            'gae_lambda': 0.98,  # –ó–±—ñ–ª—å—à–µ–Ω–æ –¥–ª—è –∫—Ä–∞—â–æ—ó –æ—Ü—ñ–Ω–∫–∏ –ø–µ—Ä–µ–≤–∞–≥
            'clip_range': 0.2,  # –ó–±—ñ–ª—å—à–µ–Ω–æ –¥–ª—è –±—ñ–ª—å—à–æ—ó –≥–Ω—É—á–∫–æ—Å—Ç—ñ
            'ent_coef': 0.01,  # –ó–±—ñ–ª—å—à–µ–Ω–æ –¥–ª—è –±—ñ–ª—å—à–æ—ó –µ–∫—Å–ø–ª–æ—Ä–∞—Ü—ñ—ó
            'vf_coef': 0.5,  # –ó–±—ñ–ª—å—à–µ–Ω–æ –¥–ª—è –∫—Ä–∞—â–æ—ó value function
            'max_grad_norm': 0.5,  # –ó–∞–ª–∏—à–µ–Ω–æ –æ–ø—Ç–∏–º–∞–ª—å–Ω–∏–º
            'verbose': 1,
            # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–ª—è –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
            'use_sde': False,
            'sde_sample_freq': -1,
            'target_kl': 0.01,  # –ó–º–µ–Ω—à–µ–Ω–æ –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ
            'normalize_advantage': True,
            # –ü–æ–∫—Ä–∞—â–µ–Ω–∞ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è —Ç–æ—Ä–≥—ñ–≤–ª—ñ
            'policy_kwargs': {
                'net_arch': [256, 128, 64],  # –ë—ñ–ª—å—à–∞ –º–µ—Ä–µ–∂–∞ –¥–ª—è —Å–∫–ª–∞–¥–Ω—ñ—à–∏—Ö —Å—Ç—Ä–∞—Ç–µ–≥—ñ–π
                'activation_fn': torch.nn.Tanh,  # Tanh –∫—Ä–∞—â–µ –¥–ª—è —Ñ—ñ–Ω–∞–Ω—Å–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö
                'normalize_images': False,
                'ortho_init': True,  # –ö—Ä–∞—â–∞—è —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è
                'log_std_init': -0.5  # –ó–±—ñ–ª—å—à–µ–Ω–∞ –ø–æ—á–∞—Ç–∫–æ–≤–∞ –¥–∏—Å–ø–µ—Ä—Å—ñ—è –¥–ª—è –µ–∫—Å–ø–ª–æ—Ä–∞—Ü—ñ—ó
            }
        }
        
        if model_config:
            default_config.update(model_config)
        
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å PPO
        self.model = PPO(
            "MlpPolicy",
            self.vec_env,
            device=self.device,
            **default_config
        )
        
        return self.model
    
    def train(self, total_timesteps=100000, callback=None):
        """–û–±—É—á–∏—Ç—å –∞–≥–µ–Ω—Ç–∞."""
        if not self.model:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ —Å–æ–∑–¥–∞–Ω–∞. –í—ã–∑–æ–≤–∏—Ç–µ create_model() —Å–Ω–∞—á–∞–ª–∞.")
        
        self.model.learn(
            total_timesteps=total_timesteps,
            callback=callback
        )
        
        return self.model
    
    def act(self, state):
        """–í—ã–±—Ä–∞—Ç—å –¥–µ–π—Å—Ç–≤–∏–µ."""
        if not self.model:
            return np.array([0.0])
        
        action, _ = self.model.predict(state, deterministic=True)
        return action
    
    def save(self, path):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å."""
        if self.model:
            self.model.save(path)
    
    def load(self, path, env=None):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å."""
        if env:
            self.vec_env = DummyVecEnv([lambda: env])
        
        self.model = PPO.load(path, env=self.vec_env)
        return self.model 