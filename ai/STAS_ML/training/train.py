"""
–û—Å–Ω–æ–≤–Ω–∏–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è STAS_ML-–∞–≥–µ–Ω—Ç–∞.
"""

import os
import sys
import numpy as np
import pandas as pd
from datetime import datetime
from typing import Optional, Dict, Any
import torch

# –î–æ–¥–∞—î–º–æ —à–ª—è—Ö –¥–æ –º–æ–¥—É–ª—ñ–≤ –ø—Ä–æ–µ–∫—Ç—É
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..', '..'))
sys.path.insert(0, project_root)

from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, CallbackList
from stable_baselines3.common.monitor import Monitor

from CryptoTrade.ai.STAS_ML.config.trading_config import TradingConfig
from CryptoTrade.ai.STAS_ML.environment.trading_env import TradingEnv
from CryptoTrade.ai.STAS_ML.agents.dqn_agent import DQNAgent
from CryptoTrade.ai.STAS_ML.agents.ppo_agent import PPOAgent
from CryptoTrade.ai.STAS_ML.training.callbacks import TradingCallback, TensorboardCallback


class DRLTrainer:
    """–ö–ª–∞—Å –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è STAS_ML –∞–≥–µ–Ω—Ç—ñ–≤."""
    
    def __init__(self, config: TradingConfig, save_dir: str = "models", resume_training: bool = True, custom_model_name: str = None):
        self.config = config
        self.save_dir = save_dir
        self.resume_training = resume_training
        
        # –î–æ–∑–≤–æ–ª—è—î–º–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—É –≤–∫–∞–∑–∞—Ç–∏ –≤–ª–∞—Å–Ω–µ —ñ–º'—è –º–æ–¥–µ–ª—ñ
        if custom_model_name:
            self.experiment_name = custom_model_name
        else:
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ñ—ñ–∫—Å–æ–≤–∞–Ω–µ —ñ–º'—è –±–µ–∑ timestamp –¥–ª—è –ø–æ—Å—Ç—ñ–π–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è –æ–¥–Ω—ñ—î—ó –º–æ–¥–µ–ª—ñ
            self.experiment_name = f"{config.symbol}_{config.timeframe}_{config.reward_scheme}"
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó
        os.makedirs(save_dir, exist_ok=True)
        os.makedirs(f"logs/{self.experiment_name}", exist_ok=True)
        
    def prepare_environment(self, train_split: float = 0.8, validation_split: float = 0.1):
        """–ü—ñ–¥–≥–æ—Ç—É–≤–∞—Ç–∏ —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞ –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è —Ç–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó."""
        # –°—Ç–≤–æ—Ä—é—î–º–æ –±–∞–∑–æ–≤–µ —Å–µ—Ä–µ–¥–æ–≤–∏—â–µ
        full_env = TradingEnv(self.config)
        
        # –†–æ–∑–¥—ñ–ª—è—î–º–æ –¥–∞–Ω—ñ –Ω–∞ train/validation/test
        data_len = len(full_env.data)
        train_end = int(data_len * train_split)
        val_end = int(data_len * (train_split + validation_split))
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö –ø–µ—Ä—ñ–æ–¥—ñ–≤
        train_config = TradingConfig(**self.config.__dict__)
        val_config = TradingConfig(**self.config.__dict__)
        test_config = TradingConfig(**self.config.__dict__)
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞
        self.train_env = TradingEnv(train_config)
        self.train_env.data = self.train_env.data.iloc[:train_end]
        self.train_env = Monitor(self.train_env, f"logs/{self.experiment_name}/train")
        
        self.val_env = TradingEnv(val_config)
        self.val_env.data = self.val_env.data.iloc[train_end:val_end]
        self.val_env = Monitor(self.val_env, f"logs/{self.experiment_name}/val")
        
        self.test_env = TradingEnv(test_config)
        self.test_env.data = self.test_env.data.iloc[val_end:]
        
        print(f"–ü—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ñ —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞:")
        print(f"  Train: {len(self.train_env.unwrapped.data)} –∑–∞–ø–∏—Å—ñ–≤")
        print(f"  Validation: {len(self.val_env.unwrapped.data)} –∑–∞–ø–∏—Å—ñ–≤")
        print(f"  Test: {len(self.test_env.data)} –∑–∞–ø–∏—Å—ñ–≤")
        
        return self.train_env, self.val_env, self.test_env
    
    def create_agent(self, agent_type: str = "PPO", model_config: Optional[Dict] = None):
        """–°—Ç–≤–æ—Ä–∏—Ç–∏ –∞–≥–µ–Ω—Ç–∞."""
        if agent_type.upper() == "DQN":
            self.agent = DQNAgent(self.config)
        elif agent_type.upper() == "PPO":
            self.agent = PPOAgent(self.config)
        else:
            raise ValueError(f"–ù–µ–ø—ñ–¥—Ç—Ä–∏–º—É–≤–∞–Ω–∏–π —Ç–∏–ø –∞–≥–µ–Ω—Ç–∞: {agent_type}")
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —î —ñ—Å–Ω—É—é—á–∞ –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è –Ω–∞–≤—á–∞–Ω–Ω—è
        model_dir = f"{self.save_dir}/{self.experiment_name}"
        possible_model_paths = [
            f"{model_dir}/final_model.zip",
            f"{model_dir}/best_model.zip",
            f"{model_dir}/final_model",
            f"{model_dir}/best_model"
        ]
        
        # –®—É–∫–∞—î–º–æ —ñ—Å–Ω—É—é—á—É –º–æ–¥–µ–ª—å
        existing_model_path = None
        if self.resume_training:
            for model_path in possible_model_paths:
                if os.path.exists(model_path):
                    existing_model_path = model_path
                    break
            
            # –¢–∞–∫–æ–∂ –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ checkpoints
            if not existing_model_path:
                checkpoint_dir = f"{model_dir}/checkpoints"
                if os.path.exists(checkpoint_dir):
                    checkpoint_files = [f for f in os.listdir(checkpoint_dir) if f.endswith('.zip')]
                    if checkpoint_files:
                        checkpoint_files.sort(key=lambda x: int(x.split('_')[-2]) if '_' in x and x.split('_')[-2].isdigit() else 0)
                        existing_model_path = os.path.join(checkpoint_dir, checkpoint_files[-1])
        
        if existing_model_path:
            print(f"üîÑ –ó–Ω–∞–π–¥–µ–Ω–∞ —ñ—Å–Ω—É—é—á–∞ –º–æ–¥–µ–ª—å: {existing_model_path}")
            try:
                self.agent.load(existing_model_path, self.train_env)
                print(f"‚úÖ –ú–æ–¥–µ–ª—å {agent_type} –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞ –¥–ª—è –ø—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è –Ω–∞–≤—á–∞–Ω–Ω—è")
            except Exception as e:
                print(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ: {e}")
                print(f"üÜï –°—Ç–≤–æ—Ä—é—î–º–æ –Ω–æ–≤—É –º–æ–¥–µ–ª—å...")
                self.agent.create_model(self.train_env, model_config)
                print(f"‚úÖ –°—Ç–≤–æ—Ä–µ–Ω–∞ –Ω–æ–≤–∞ –º–æ–¥–µ–ª—å {agent_type}")
        else:
            # –°—Ç–≤–æ—Ä—é—î–º–æ –Ω–æ–≤—É –º–æ–¥–µ–ª—å
            self.agent.create_model(self.train_env, model_config)
            print(f"üÜï –°—Ç–≤–æ—Ä–µ–Ω–∞ –Ω–æ–≤–∞ –º–æ–¥–µ–ª—å {agent_type}")
        
        return self.agent
    
    def create_callbacks(self, eval_freq: int = 10000, save_freq: int = 20000):
        """–°—Ç–≤–æ—Ä–∏—Ç–∏ callbacks –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è - –û–ü–¢–ò–ú–Ü–ó–û–í–ê–ù–û –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ."""
        callbacks = []
        
        # Callback –¥–ª—è –æ—Ü—ñ–Ω–∫–∏ –º–æ–¥–µ–ª—ñ - —Ä—ñ–¥—à–µ –æ—Ü—ñ–Ω–∫–∞ –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ
        eval_callback = EvalCallback(
            self.val_env,
            best_model_save_path=f"{self.save_dir}/{self.experiment_name}",
            log_path=f"logs/{self.experiment_name}",
            eval_freq=eval_freq,  # –ó–±—ñ–ª—å—à–µ–Ω–æ —ñ–Ω—Ç–µ—Ä–≤–∞–ª –æ—Ü—ñ–Ω–∫–∏
            n_eval_episodes=5,  # –ó–º–µ–Ω—à–µ–Ω–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ø—ñ–∑–æ–¥—ñ–≤ –æ—Ü—ñ–Ω–∫–∏
            deterministic=True,
            render=False,
            verbose=1,
            warn=False
        )
        callbacks.append(eval_callback)
        
        # Checkpoint callback
        checkpoint_callback = CheckpointCallback(
            save_freq=save_freq,
            save_path=f"{self.save_dir}/{self.experiment_name}/checkpoints",
            name_prefix="model",
            verbose=1
        )
        callbacks.append(checkpoint_callback)
        
        # –ö–∞—Å—Ç–æ–º–Ω—ñ callbacks –∑ –ø–æ–∫—Ä–∞—â–µ–Ω–∏–º —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω–∏–º –≤–∏–≤–æ–¥–æ–º
        trading_callback = TradingCallback(
            log_dir=f"logs/{self.experiment_name}",
            experiment_name=self.experiment_name
        )
        # –ù–∞–ª–∞—à—Ç–æ–≤—É—î–º–æ —ñ–Ω—Ç–µ—Ä–≤–∞–ª –∑–≤—ñ—Ç–Ω–æ—Å—Ç—ñ (–∫–æ–∂–Ω—ñ 5000 –∫—Ä–æ–∫—ñ–≤ –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ)
        trading_callback.report_interval = 5000
        callbacks.append(trading_callback)
        
        # Tensorboard callback
        tensorboard_callback = TensorboardCallback(
            log_dir=f"logs/{self.experiment_name}/tensorboard"
        )
        callbacks.append(tensorboard_callback)
        
        return CallbackList(callbacks)
    
    def train(self, total_timesteps: int = 500000, eval_freq: int = 5000, 
              save_freq: int = 10000, agent_type: str = "PPO", 
              model_config: Optional[Dict] = None):
        """–ù–∞–≤—á–∏—Ç–∏ –∞–≥–µ–Ω—Ç–∞."""
        
        print(f"üöÄ –ü–æ—á–∏–Ω–∞—î–º–æ –Ω–∞–≤—á–∞–Ω–Ω—è {agent_type} –∞–≥–µ–Ω—Ç–∞")
        print(f"–ï–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç: {self.experiment_name}")
        print(f"–ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫—Ä–æ–∫—ñ–≤: {total_timesteps:,}")
        print(f"–°–∏–º–≤–æ–ª: {self.config.symbol}, –¢–∞–π–º—Ñ—Ä–µ–π–º: {self.config.timeframe}")
        print(f"–°—Ö–µ–º–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥: {self.config.reward_scheme}")
        print("-" * 50)
        
        # –ü—ñ–¥–≥–æ—Ç–æ–≤—É—î–º–æ —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞
        self.prepare_environment()
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –∞–≥–µ–Ω—Ç–∞
        self.create_agent(agent_type, model_config)
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ callbacks
        callbacks = self.create_callbacks(eval_freq, save_freq)
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é
        self.save_config()
        
        try:
            # –ù–∞–≤—á–∞—î–º–æ –º–æ–¥–µ–ª—å
            self.agent.train(
                total_timesteps=total_timesteps,
                callback=callbacks
            )
            
            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ñ—ñ–Ω–∞–ª—å–Ω—É –º–æ–¥–µ–ª—å
            final_model_path = f"{self.save_dir}/{self.experiment_name}/final_model"
            self.agent.save(final_model_path)
            
            print(f"‚úÖ –ù–∞–≤—á–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
            print(f"–ú–æ–¥–µ–ª—å –∑–±–µ—Ä–µ–∂–µ–Ω–∞: {final_model_path}")
            
            return self.agent
            
        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è: {e}")
            raise
    
    def save_config(self):
        """–ó–±–µ—Ä–µ–≥—Ç–∏ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—É."""
        config_dict = {
            'symbol': self.config.symbol,
            'timeframe': self.config.timeframe,
            'exchange': self.config.exchange,
            'initial_balance': self.config.initial_balance,
            'commission_rate': self.config.commission_rate,
            'slippage_rate': self.config.slippage_rate,
            'spread_rate': self.config.spread_rate,
            'reward_scheme': self.config.reward_scheme,
            'lookback_window': self.config.lookback_window,
            'experiment_name': self.experiment_name,
            'created_at': datetime.now().isoformat()
        }
        
        config_df = pd.DataFrame([config_dict])
        config_df.to_csv(f"logs/{self.experiment_name}/config.csv", index=False)
        
        # –¢–∞–∫–æ–∂ –∑–±–µ—Ä—ñ–≥–∞—î–º–æ —è–∫ JSON –¥–ª—è –∑—Ä—É—á–Ω–æ—Å—Ç—ñ
        import json
        with open(f"logs/{self.experiment_name}/config.json", 'w') as f:
            json.dump(config_dict, f, indent=2)


def quick_train(symbol: str = "BTCUSDT", timeframe: str = "1d", 
                agent_type: str = "PPO", timesteps: int = 200000,
                reward_scheme: str = "optimized"):
    """–®–≤–∏–¥–∫–∏–π –∑–∞–ø—É—Å–∫ –Ω–∞–≤—á–∞–Ω–Ω—è –∑ –º—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–º–∏ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è–º–∏."""
    
    config = TradingConfig(
        symbol=symbol,
        timeframe=timeframe,
        reward_scheme=reward_scheme,
        initial_balance=10000.0
    )
    
    trainer = DRLTrainer(config)
    return trainer.train(
        total_timesteps=timesteps,
        agent_type=agent_type
    )


if __name__ == "__main__":
    # –ü—Ä–∏–∫–ª–∞–¥ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è
    import argparse
    
    parser = argparse.ArgumentParser(description='–ù–∞–≤—á–∞–Ω–Ω—è STAS_ML –∞–≥–µ–Ω—Ç–∞ –¥–ª—è —Ç–æ—Ä–≥—ñ–≤–ª—ñ')
    parser.add_argument('--symbol', default='BTCUSDT', help='–¢–æ—Ä–≥–æ–≤–∞ –ø–∞—Ä–∞')
    parser.add_argument('--timeframe', default='1d', help='–¢–∞–π–º—Ñ—Ä–µ–π–º')
    parser.add_argument('--agent', default='PPO', choices=['PPO', 'DQN'], help='–¢–∏–ø –∞–≥–µ–Ω—Ç–∞')
    parser.add_argument('--timesteps', type=int, default=200000, help='–ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫—Ä–æ–∫—ñ–≤ –Ω–∞–≤—á–∞–Ω–Ω—è')
    parser.add_argument('--reward', default='optimized', help='–°—Ö–µ–º–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥')
    
    args = parser.parse_args()
    
    quick_train(
        symbol=args.symbol,
        timeframe=args.timeframe,
        agent_type=args.agent,
        timesteps=args.timesteps,
        reward_scheme=args.reward
    )