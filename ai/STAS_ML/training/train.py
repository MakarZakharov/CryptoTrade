"""
–û—Å–Ω–æ–≤–Ω–æ–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è STAS_ML-–∞–≥–µ–Ω—Ç–∞.
"""

import os
import sys
import numpy as np
import pandas as pd
from datetime import datetime
from typing import Optional, Dict, Any
import torch

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º –ø—Ä–æ–µ–∫—Ç–∞
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..', '..'))
sys.path.insert(0, project_root)

from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, CallbackList
from stable_baselines3.common.monitor import Monitor

from CryptoTrade.ai.STAS_ML.config.trading_config import TradingConfig
from CryptoTrade.ai.STAS_ML.environment.trading_env import TradingEnv
from CryptoTrade.ai.STAS_ML.agents.dqn_agent import DQNAgent
from CryptoTrade.ai.STAS_ML.agents.ppo_agent import PPOAgent
from CryptoTrade.ai.STAS_ML.training.callbacks import TradingCallback, TensorboardCallback


class DRLTrainer:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è STAS_ML –∞–≥–µ–Ω—Ç–æ–≤."""
    
    def __init__(self, config: TradingConfig, save_dir: str = "models", resume_training: bool = True):
        self.config = config
        self.save_dir = save_dir
        self.resume_training = resume_training
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏–º—è –±–µ–∑ timestamp –¥–ª—è –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏
        self.experiment_name = f"{config.symbol}_{config.timeframe}_{config.reward_scheme}"
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        os.makedirs(save_dir, exist_ok=True)
        os.makedirs(f"logs/{self.experiment_name}", exist_ok=True)
        
    def prepare_environment(self, train_split: float = 0.8, validation_split: float = 0.1):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å —Å—Ä–µ–¥—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏."""
        # –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—É—é —Å—Ä–µ–¥—É
        full_env = TradingEnv(self.config)
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ train/validation/test
        data_len = len(full_env.data)
        train_end = int(data_len * train_split)
        val_end = int(data_len * (train_split + validation_split))
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –ø–µ—Ä–∏–æ–¥–æ–≤
        train_config = TradingConfig(**self.config.__dict__)
        val_config = TradingConfig(**self.config.__dict__)
        test_config = TradingConfig(**self.config.__dict__)
        
        # –°–æ–∑–¥–∞–µ–º —Å—Ä–µ–¥—ã
        self.train_env = TradingEnv(train_config)
        self.train_env.data = self.train_env.data.iloc[:train_end]
        self.train_env = Monitor(self.train_env, f"logs/{self.experiment_name}/train")
        
        self.val_env = TradingEnv(val_config)
        self.val_env.data = self.val_env.data.iloc[train_end:val_end]
        self.val_env = Monitor(self.val_env, f"logs/{self.experiment_name}/val")
        
        self.test_env = TradingEnv(test_config)
        self.test_env.data = self.test_env.data.iloc[val_end:]
        
        print(f"–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã —Å—Ä–µ–¥—ã:")
        print(f"  Train: {len(self.train_env.unwrapped.data)} –∑–∞–ø–∏—Å–µ–π")
        print(f"  Validation: {len(self.val_env.unwrapped.data)} –∑–∞–ø–∏—Å–µ–π")
        print(f"  Test: {len(self.test_env.data)} –∑–∞–ø–∏—Å–µ–π")
        
        return self.train_env, self.val_env, self.test_env
    
    def create_agent(self, agent_type: str = "PPO", model_config: Optional[Dict] = None):
        """–°–æ–∑–¥–∞—Ç—å –∞–≥–µ–Ω—Ç–∞."""
        if agent_type.upper() == "DQN":
            self.agent = DQNAgent(self.config)
        elif agent_type.upper() == "PPO":
            self.agent = PPOAgent(self.config)
        else:
            raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø –∞–≥–µ–Ω—Ç–∞: {agent_type}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è
        model_dir = f"{self.save_dir}/{self.experiment_name}"
        possible_model_paths = [
            f"{model_dir}/final_model.zip",
            f"{model_dir}/best_model.zip",
            f"{model_dir}/final_model",
            f"{model_dir}/best_model"
        ]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å observation space –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π –º–æ–¥–µ–ª–∏
        test_env = TradingEnv(self.config)
        current_obs_shape = test_env.observation_space.shape
        
        # –ò—â–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –º–æ–¥–µ–ª—å
        existing_model_path = None
        if self.resume_training:
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å –Ω–æ–≤—ã–º –∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ–º
            for model_path in possible_model_paths:
                if os.path.exists(model_path):
                    existing_model_path = model_path
                    break
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –∏—â–µ–º –ø–∞–ø–∫–∏ —Å–æ —Å—Ç–∞—Ä—ã–º –∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ–º (—Å timestamp)
            if not existing_model_path and os.path.exists(self.save_dir):
                # –ò—â–µ–º –ø–∞–ø–∫–∏ –≤–∏–¥–∞ SYMBOL_TIMEFRAME_YYYYMMDD_HHMMSS
                prefix = f"{self.config.symbol}_{self.config.timeframe}_"
                matching_dirs = []
                
                for item in os.listdir(self.save_dir):
                    item_path = os.path.join(self.save_dir, item)
                    if os.path.isdir(item_path) and item.startswith(prefix):
                        matching_dirs.append(item)
                
                if matching_dirs:
                    # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –ø–æ –≤—Ä–µ–º–µ–Ω–∏ —Å–æ–∑–¥–∞–Ω–∏—è (–∞–ª—Ñ–∞–≤–∏—Ç–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è timestamp)
                    latest_dir = sorted(matching_dirs)[-1]
                    old_model_dir = f"{self.save_dir}/{latest_dir}"
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–æ–¥–µ–ª–∏ –≤ —Å—Ç–∞—Ä–æ–π –ø–∞–ø–∫–µ
                    old_possible_paths = [
                        f"{old_model_dir}/final_model.zip",
                        f"{old_model_dir}/best_model.zip",
                        f"{old_model_dir}/final_model",
                        f"{old_model_dir}/best_model"
                    ]
                    
                    for model_path in old_possible_paths:
                        if os.path.exists(model_path):
                            existing_model_path = model_path
                            break
                    
                    # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º checkpoints –≤ —Å—Ç–∞—Ä–æ–π –ø–∞–ø–∫–µ
                    if not existing_model_path:
                        old_checkpoint_dir = f"{old_model_dir}/checkpoints"
                        if os.path.exists(old_checkpoint_dir):
                            checkpoint_files = [f for f in os.listdir(old_checkpoint_dir) if f.endswith('.zip')]
                            if checkpoint_files:
                                checkpoint_files.sort(key=lambda x: int(x.split('_')[-2]) if '_' in x and x.split('_')[-2].isdigit() else 0)
                                existing_model_path = os.path.join(old_checkpoint_dir, checkpoint_files[-1])
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞–ø–∫—É checkpoints –≤ –Ω–æ–≤–æ–π –ø–∞–ø–∫–µ
            if not existing_model_path:
                checkpoint_dir = f"{model_dir}/checkpoints"
                if os.path.exists(checkpoint_dir):
                    checkpoint_files = [f for f in os.listdir(checkpoint_dir) if f.endswith('.zip')]
                    if checkpoint_files:
                        checkpoint_files.sort(key=lambda x: int(x.split('_')[-2]) if '_' in x and x.split('_')[-2].isdigit() else 0)
                        existing_model_path = os.path.join(checkpoint_dir, checkpoint_files[-1])
        
        if existing_model_path:
            print(f"üîÑ –ù–∞–π–¥–µ–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è –º–æ–¥–µ–ª—å: {existing_model_path}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å observation space
            try:
                print(f"üìä –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å observation space...")
                print(f"   –¢–µ–∫—É—â–∏–π: {current_obs_shape}")
                
                # –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
                temp_agent = type(self.agent)(self.config)
                temp_agent.create_model(test_env)
                temp_agent.load(existing_model_path, test_env)
                
                print(f"‚úÖ Observation space —Å–æ–≤–º–µ—Å—Ç–∏–º, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")
                self.agent.load(existing_model_path, self.train_env)
                print(f"‚úÖ –ú–æ–¥–µ–ª—å {agent_type} –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è")
                
            except Exception as e:
                if "Observation spaces do not match" in str(e):
                    print(f"‚ö†Ô∏è Observation space –Ω–µ —Å–æ–≤–º–µ—Å—Ç–∏–º —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –º–æ–¥–µ–ª—å—é")
                    print(f"   –û—à–∏–±–∫–∞: {e}")
                    print(f"üÜï –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å...")
                    self.agent.create_model(self.train_env, model_config)
                    print(f"‚úÖ –°–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å {agent_type} –∏–∑-–∑–∞ –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏")
                else:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
                    print(f"üÜï –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å...")
                    self.agent.create_model(self.train_env, model_config)
                    print(f"‚úÖ –°–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å {agent_type}")
        else:
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å
            self.agent.create_model(self.train_env, model_config)
            if self.resume_training:
                print(f"üÜï –°—É—â–µ—Å—Ç–≤—É—é—â–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å {agent_type}")
                print(f"üí° –ò—Å–∫–∞–ª–∏ –≤: {model_dir}")
            else:
                print(f"üÜï –°–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å {agent_type} —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π: {model_config or 'default'}")
        
        return self.agent
    
    def create_callbacks(self, eval_freq: int = 5000, save_freq: int = 10000):
        """–°—Ç–≤–æ—Ä–∏—Ç–∏ –ø–æ–∫—Ä–∞—â–µ–Ω—ñ callbacks –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è."""
        callbacks = []
        
        # –ü–æ–∫—Ä–∞—â–µ–Ω–∏–π callback –¥–ª—è –æ—Ü—ñ–Ω–∫–∏ –º–æ–¥–µ–ª—ñ –∑ —á–∞—Å—Ç—ñ—à–æ—é –ø–µ—Ä–µ–≤—ñ—Ä–∫–æ—é
        eval_callback = EvalCallback(
            self.val_env,
            best_model_save_path=f"{self.save_dir}/{self.experiment_name}",
            log_path=f"logs/{self.experiment_name}",
            eval_freq=eval_freq,  # –ß–∞—Å—Ç—ñ—à–µ –æ—Ü—ñ–Ω—é–≤–∞–Ω–Ω—è
            n_eval_episodes=10,  # –ë—ñ–ª—å—à–µ –µ–ø—ñ–∑–æ–¥—ñ–≤ –¥–ª—è –Ω–∞–¥—ñ–π–Ω–æ—ó –æ—Ü—ñ–Ω–∫–∏
            deterministic=True,
            render=False,
            verbose=1,
            warn=False
        )
        callbacks.append(eval_callback)
        
        # –ß–∞—Å—Ç—ñ—à—ñ checkpoint-–∏
        checkpoint_callback = CheckpointCallback(
            save_freq=save_freq,
            save_path=f"{self.save_dir}/{self.experiment_name}/checkpoints",
            name_prefix="model",
            verbose=1
        )
        callbacks.append(checkpoint_callback)
        
        # –†–∞–Ω–Ω—ñ–π –∑—É–ø–∏–Ω –¥–ª—è –∑–∞–ø–æ–±—ñ–≥–∞–Ω–Ω—è –ø–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—é
        from .callbacks import EarlyStoppingCallback
        early_stopping_callback = EarlyStoppingCallback(
            patience=30,  # –ó—É–ø–∏–Ω–∫–∞ –ø—ñ—Å–ª—è 30 –æ—Ü—ñ–Ω–æ–∫ –±–µ–∑ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è
            min_improvement=0.005,  # –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–µ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è 0.5%
            verbose=1
        )
        callbacks.append(early_stopping_callback)
        
        # –ö–∞—Å—Ç–æ–º–Ω—ñ callbacks
        trading_callback = TradingCallback(
            log_dir=f"logs/{self.experiment_name}",
            experiment_name=self.experiment_name
        )
        callbacks.append(trading_callback)
        
        # Tensorboard callback
        tensorboard_callback = TensorboardCallback(
            log_dir=f"logs/{self.experiment_name}/tensorboard"
        )
        callbacks.append(tensorboard_callback)
        
        # –ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
        from .callbacks import PerformanceMonitorCallback
        performance_callback = PerformanceMonitorCallback(
            log_freq=5000,
            verbose=1
        )
        callbacks.append(performance_callback)
        
        return CallbackList(callbacks)
    
    def train(self, total_timesteps: int = 500000, eval_freq: int = 5000, 
              save_freq: int = 10000, agent_type: str = "PPO", 
              model_config: Optional[Dict] = None):
        """–û–±—É—á–∏—Ç—å –∞–≥–µ–Ω—Ç–∞."""
        
        print(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ {agent_type} –∞–≥–µ–Ω—Ç–∞")
        print(f"–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç: {self.experiment_name}")
        print(f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤: {total_timesteps:,}")
        print(f"–°–∏–º–≤–æ–ª: {self.config.symbol}, –¢–∞–π–º—Ñ—Ä–µ–π–º: {self.config.timeframe}")
        print(f"–°—Ö–µ–º–∞ –Ω–∞–≥—Ä–∞–¥: {self.config.reward_scheme}")
        print("-" * 50)
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Å—Ä–µ–¥—ã
        self.prepare_environment()
        
        # –°–æ–∑–¥–∞–µ–º –∞–≥–µ–Ω—Ç–∞
        self.create_agent(agent_type, model_config)
        
        # –°–æ–∑–¥–∞–µ–º callbacks
        callbacks = self.create_callbacks(eval_freq, save_freq)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        self.save_config()
        
        try:
            # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
            self.agent.train(
                total_timesteps=total_timesteps,
                callback=callbacks
            )
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
            final_model_path = f"{self.save_dir}/{self.experiment_name}/final_model"
            self.agent.save(final_model_path)
            
            print(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
            print(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {final_model_path}")
            
            return self.agent
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {e}")
            raise
    
    def save_config(self):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞."""
        config_dict = {
            'symbol': self.config.symbol,
            'timeframe': self.config.timeframe,
            'exchange': self.config.exchange,
            'initial_balance': self.config.initial_balance,
            'commission_rate': self.config.commission_rate,
            'slippage_rate': self.config.slippage_rate,
            'spread_rate': self.config.spread_rate,
            'reward_scheme': self.config.reward_scheme,
            'lookback_window': self.config.lookback_window,
            'experiment_name': self.experiment_name,
            'created_at': datetime.now().isoformat()
        }
        
        config_df = pd.DataFrame([config_dict])
        config_df.to_csv(f"logs/{self.experiment_name}/config.csv", index=False)
        
        # –¢–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ JSON –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
        import json
        with open(f"logs/{self.experiment_name}/config.json", 'w') as f:
            json.dump(config_dict, f, indent=2)


def quick_train(symbol: str = "BTCUSDT", timeframe: str = "1d", 
                agent_type: str = "PPO", timesteps: int = 100000,
                reward_scheme: str = "optimized"):
    """–ë—ã—Å—Ç—Ä—ã–π –∑–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏."""
    
    config = TradingConfig(
        symbol=symbol,
        timeframe=timeframe,
        reward_scheme=reward_scheme,
        initial_balance=10000.0
    )
    
    trainer = DRLTrainer(config)
    return trainer.train(
        total_timesteps=timesteps,
        agent_type=agent_type
    )


if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    import argparse
    
    parser = argparse.ArgumentParser(description='–û–±—É—á–µ–Ω–∏–µ STAS_ML –∞–≥–µ–Ω—Ç–∞ –¥–ª—è —Ç–æ—Ä–≥–æ–≤–ª–∏')
    parser.add_argument('--symbol', default='BTCUSDT', help='–¢–æ—Ä–≥–æ–≤–∞—è –ø–∞—Ä–∞')
    parser.add_argument('--timeframe', default='1d', help='–¢–∞–π–º—Ñ—Ä–µ–π–º')
    parser.add_argument('--agent', default='PPO', choices=['PPO', 'DQN'], help='–¢–∏–ø –∞–≥–µ–Ω—Ç–∞')
    parser.add_argument('--timesteps', type=int, default=500000, help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è')
    parser.add_argument('--reward', default='optimized', help='–°—Ö–µ–º–∞ –Ω–∞–≥—Ä–∞–¥')
    
    args = parser.parse_args()
    
    quick_train(
        symbol=args.symbol,
        timeframe=args.timeframe,
        agent_type=args.agent,
        timesteps=args.timesteps,
        reward_scheme=args.reward
    ) 