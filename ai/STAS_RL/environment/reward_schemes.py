"""
–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ñ —Å—Ö–µ–º–∏ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥ –¥–ª—è —Ç–æ—Ä–≥–æ–≤–æ–≥–æ —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞.
–ü—ñ–¥—Ç—Ä–∏–º—É—î —Ä—ñ–∑–Ω—ñ –º–µ—Ç–æ–¥–∏ —Ä–æ–∑—Ä–∞—Ö—É–Ω–∫—É –≤–∏–Ω–∞–≥–æ—Ä–æ–¥ —Ç–∞ —ó—Ö –∫–æ–º–±—ñ–Ω—É–≤–∞–Ω–Ω—è.
"""

import numpy as np
import pandas as pd
from typing import List, Dict, Optional, Callable
from abc import ABC, abstractmethod


class BaseRewardScheme(ABC):
    """–ë–∞–∑–æ–≤–∏–π –∫–ª–∞—Å –¥–ª—è —Å—Ö–µ–º –≤–∏–Ω–∞–≥–æ—Ä–æ–¥."""
    
    def __init__(self, weight: float = 1.0):
        self.weight = weight
    
    @abstractmethod
    def calculate(self, env_state: Dict) -> float:
        """–†–æ–∑—Ä–∞—Ö—É–≤–∞—Ç–∏ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥—É –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Å—Ç–∞–Ω—É —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞."""
        pass
    
    def reset(self):
        """–°–∫–∏–Ω—É—Ç–∏ —Å—Ç–∞–Ω —Å—Ö–µ–º–∏ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥."""
        pass


class ProfitReward(BaseRewardScheme):
    """–í–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –ø—Ä–∏–±—É—Ç–∫—É –ø–æ—Ä—Ç—Ñ–µ–ª—è."""
    
    def __init__(self, weight: float = 1.0, normalize: bool = True):
        super().__init__(weight)
        self.normalize = normalize
        self.last_portfolio_value = None
    
    def calculate(self, env_state: Dict) -> float:
        portfolio_value = env_state['portfolio_value']
        
        if self.last_portfolio_value is None:
            self.last_portfolio_value = portfolio_value
            return 0.0
        
        # –í—ñ–¥–Ω–æ—Å–Ω–∞ –∑–º—ñ–Ω–∞ –ø–æ—Ä—Ç—Ñ–µ–ª—è
        profit_change = (portfolio_value - self.last_portfolio_value) / self.last_portfolio_value
        self.last_portfolio_value = portfolio_value
        
        if self.normalize:
            # –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –ø–æ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—ñ
            reward = np.tanh(profit_change * 100) * self.weight
        else:
            reward = profit_change * self.weight
        
        return reward
    
    def reset(self):
        self.last_portfolio_value = None


class DrawdownPenalty(BaseRewardScheme):
    """–®—Ç—Ä–∞—Ñ –∑–∞ –ø—Ä–æ—Å–∞–¥–∫—É."""
    
    def __init__(self, weight: float = -0.5, max_drawdown_threshold: float = 0.1):
        super().__init__(weight)
        self.max_drawdown_threshold = max_drawdown_threshold
    
    def calculate(self, env_state: Dict) -> float:
        max_drawdown = env_state.get('max_drawdown', 0.0)
        
        if max_drawdown > self.max_drawdown_threshold:
            # –ï–∫—Å–ø–æ–Ω–µ–Ω—Ü—ñ–π–Ω–∏–π —à—Ç—Ä–∞—Ñ –∑–∞ –ø–µ—Ä–µ–≤–∏—â–µ–Ω–Ω—è –ø–æ—Ä–æ–≥—É –ø—Ä–æ—Å–∞–¥–∫–∏
            penalty = np.exp((max_drawdown - self.max_drawdown_threshold) * 10) - 1
            return -penalty * abs(self.weight)
        
        return 0.0


class DynamicRewardScaler:
    """
    –î–∏–Ω–∞–º—ñ—á–Ω–µ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è –≤–∏–Ω–∞–≥–æ—Ä–æ–¥ –Ω–∞ –æ—Å–Ω–æ–≤—ñ —ñ—Å—Ç–æ—Ä–∏—á–Ω–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤.
    –ó–∞–ø–æ–±—ñ–≥–∞—î –∑–∞–≤–µ–ª–∏–∫–∏–º –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞–º —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–∏–≤–Ω—É –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—é.
    """
    
    def __init__(self, history_window: int = 100, target_range: tuple = (-2.0, 2.0)):
        self.history_window = history_window
        self.target_range = target_range
        self.reward_history = []
        self.running_mean = 0.0
        self.running_std = 1.0
        self.alpha = 0.1  # –ö–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç –∑–≥–ª–∞–¥–∂—É–≤–∞–Ω–Ω—è –¥–ª—è –µ–∫—Å–ø–æ–Ω–µ–Ω—Ü—ñ–π–Ω–æ–≥–æ —Å–µ—Ä–µ–¥–Ω—å–æ–≥–æ
    
    def scale_reward(self, raw_reward: float) -> float:
        """–ú–∞—Å—à—Ç–∞–±—É—î –≤–∏–Ω–∞–≥–æ—Ä–æ–¥—É –Ω–∞ –æ—Å–Ω–æ–≤—ñ —ñ—Å—Ç–æ—Ä–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö."""
        # –î–æ–¥–∞—î–º–æ –ø–æ—Ç–æ—á–Ω—É –≤–∏–Ω–∞–≥–æ—Ä–æ–¥—É –¥–æ —ñ—Å—Ç–æ—Ä—ñ—ó
        self.reward_history.append(raw_reward)
        
        # –û–±–º–µ–∂—É—î–º–æ —Ä–æ–∑–º—ñ—Ä —ñ—Å—Ç–æ—Ä—ñ—ó
        if len(self.reward_history) > self.history_window:
            self.reward_history = self.reward_history[-self.history_window:]
        
        # –û–Ω–æ–≤–ª—é—î–º–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –µ–∫—Å–ø–æ–Ω–µ–Ω—Ü—ñ–π–Ω–∏–º –∑–≥–ª–∞–¥–∂—É–≤–∞–Ω–Ω—è–º
        if len(self.reward_history) > 1:
            current_mean = np.mean(self.reward_history[-10:])  # –û—Å—Ç–∞–Ω–Ω—ñ 10 –∑–Ω–∞—á–µ–Ω—å
            current_std = np.std(self.reward_history[-10:]) + 1e-8  # –ó–∞–ø–æ–±—ñ–≥–∞—î–º–æ –¥—ñ–ª–µ–Ω–Ω—é –Ω–∞ 0
            
            # –ï–∫—Å–ø–æ–Ω–µ–Ω—Ü—ñ–π–Ω–µ –∑–≥–ª–∞–¥–∂—É–≤–∞–Ω–Ω—è
            self.running_mean = (1 - self.alpha) * self.running_mean + self.alpha * current_mean
            self.running_std = (1 - self.alpha) * self.running_std + self.alpha * current_std
        
        # Z-score –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –∑ –∞–¥–∞–ø—Ç–∏–≤–Ω–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        if self.running_std > 0:
            normalized_reward = (raw_reward - self.running_mean) / self.running_std
        else:
            normalized_reward = 0.0
        
        # –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è –¥–æ —Ü—ñ–ª—å–æ–≤–æ–≥–æ –¥—ñ–∞–ø–∞–∑–æ–Ω—É –∑ tanh –¥–ª—è –º'—è–∫–æ–≥–æ –æ–±–º–µ–∂–µ–Ω–Ω—è
        target_min, target_max = self.target_range
        target_center = (target_max + target_min) / 2
        target_scale = (target_max - target_min) / 4  # tanh(¬±2) ‚âà ¬±0.96
        
        scaled_reward = target_center + target_scale * np.tanh(normalized_reward)
        
        # –õ–æ–≥—É–≤–∞–Ω–Ω—è –¥–ª—è –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ (—Ç—ñ–ª—å–∫–∏ –ø—Ä–∏ –≤–µ–ª–∏–∫–∏—Ö –≤—ñ–¥—Ö–∏–ª–µ–Ω–Ω—è—Ö)
        if abs(raw_reward) > 10:
            print(f"üîß REWARD SCALING: {raw_reward:.2f} -> {scaled_reward:.2f} (Œº={self.running_mean:.2f}, œÉ={self.running_std:.2f})")
        
        return scaled_reward
    
    def reset(self):
        """–°–∫–∏–¥–∞–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ (—á–∞—Å—Ç–∫–æ–≤–æ –∑–±–µ—Ä—ñ–≥–∞—î–º–æ —ñ—Å—Ç–æ—Ä—ñ—é –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ)."""
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –ø–æ–ª–æ–≤–∏–Ω—É —ñ—Å—Ç–æ—Ä—ñ—ó –¥–ª—è –∫—Ä–∞—â–æ—ó —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ
        if len(self.reward_history) > 20:
            self.reward_history = self.reward_history[-20:]
        # –ù–µ —Å–∫–∏–¥–∞—î–º–æ running_mean —ñ running_std –ø–æ–≤–Ω—ñ—Å—Ç—é –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ


class CompositeRewardScheme:
    """–ö–æ–º–±—ñ–Ω–æ–≤–∞–Ω–∞ —Å—Ö–µ–º–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥ –∑ –¥–∏–Ω–∞–º—ñ—á–Ω–∏–º –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è–º."""
    
    def __init__(self, schemes: List[BaseRewardScheme], enable_dynamic_scaling: bool = True):
        self.schemes = schemes
        self.reward_history = []
        self.enable_dynamic_scaling = enable_dynamic_scaling
        
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î–º–æ –¥–∏–Ω–∞–º—ñ—á–Ω–∏–π –º–∞—Å—à—Ç–∞–±—É–≤–∞—á
        if self.enable_dynamic_scaling:
            self.scaler = DynamicRewardScaler(
                history_window=50,  # –ú–µ–Ω—à–µ –≤—ñ–∫–Ω–æ –¥–ª—è —à–≤–∏–¥—à–æ—ó –∞–¥–∞–ø—Ç–∞—Ü—ñ—ó
                target_range=(-3.0, 3.0)  # –†–æ–∑—É–º–Ω–∏–π –¥—ñ–∞–ø–∞–∑–æ–Ω –¥–ª—è PPO
            )
        
        # –°–ò–°–¢–ï–ú–ê –ü–†–û–ì–†–ï–°–ò–í–ù–ò–• –ù–ê–ì–û–†–û–î –í–ò–ú–ö–ù–ï–ù–ê —á–µ—Ä–µ–∑ –Ω–µ–∫–æ—Ä–µ–∫—Ç–Ω—ñ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
        self.last_performance = {
            'return': 0.0,
            'drawdown': 0.0,
            'trades': 0,
            'win_rate': 0.0
        }
        self.improvement_bonus_count = 0
        
        # –°–ò–°–¢–ï–ú–ê –ï–°–ö–ê–õ–ê–¶–Ü–á –®–¢–†–ê–§–Ü–í –ó–ê –ü–û–°–õ–Ü–î–û–í–ù–£ –ü–û–ì–ê–ù–£ –ü–†–û–î–£–ö–¢–ò–í–ù–Ü–°–¢–¨
        self.consecutive_poor_episodes = 0
        self.poor_performance_threshold = -0.15  # -15% —è–∫ –ø–æ–≥–∞–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç (–∑–±—ñ–ª—å—à–µ–Ω–æ –∑ -2%)
    
    def calculate(self, env_state: Dict) -> float:
        """–†–æ–∑—Ä–∞—Ö—É–≤–∞—Ç–∏ –∑–∞–≥–∞–ª—å–Ω—É –≤–∏–Ω–∞–≥–æ—Ä–æ–¥—É –∑ –∂–æ—Ä—Å—Ç–∫–∏–º –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –∑–±–∏—Ç–∫—ñ–≤."""
        total_reward = 0.0
        component_rewards = {}
        
        for scheme in self.schemes:
            component_reward = scheme.calculate(env_state)
            # –ú'—è–∫–µ –æ–±–º–µ–∂–µ–Ω–Ω—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤ –ø–µ—Ä–µ–¥ –ø—ñ–¥—Å—É–º–æ–≤—É–≤–∞–Ω–Ω—è–º
            component_reward = np.clip(component_reward, -15.0, 15.0)
            component_rewards[scheme.__class__.__name__] = component_reward
            total_reward += component_reward
        
        # –ö–†–ò–¢–ò–ß–ù–û –í–ê–ñ–õ–ò–í–ò–ô –ö–û–ù–¢–†–û–õ–¨ –ó–ë–ò–¢–ö–Ü–í
        total_return = env_state.get('total_return', 0.0)
        total_trades = env_state.get('total_trades', 0)
        
        # üéØ –°–¢–ê–ë–Ü–õ–¨–ù–ê –ê–î–ê–ü–¢–ò–í–ù–ê –°–ò–°–¢–ï–ú–ê –í–ò–ù–ê–ì–û–†–û–î üéØ
        # –†–æ–∑—É–º–Ω–µ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è –∑ –ø—Ä–æ–≥—Ä–µ—Å–∏–≤–Ω–∏–º–∏ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞–º–∏
        
        # –ê–î–ê–ü–¢–ò–í–ù–ê –±–∞–∑–æ–≤–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑ –º'—è–∫–∏–º –Ω–∞—Å–∏—á–µ–Ω–Ω—è–º
        if abs(total_return) <= 0.05:  # –ú–∞–ª—ñ –∑–º—ñ–Ω–∏: –ª—ñ–Ω—ñ–π–Ω–µ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è
            base_reward = total_return * 10.0  # 1% = 0.1 –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞
        elif abs(total_return) <= 0.20:  # –ü–æ–º—ñ—Ä–Ω—ñ –∑–º—ñ–Ω–∏: –∑–º–µ–Ω—à–µ–Ω–µ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è  
            sign = 1 if total_return > 0 else -1
            scaled_return = abs(total_return)
            base_reward = sign * (0.5 + (scaled_return - 0.05) * 6.67)  # –ü–ª–∞–≤–Ω–∏–π –ø–µ—Ä–µ—Ö—ñ–¥
        else:  # –í–µ–ª–∏–∫—ñ –∑–º—ñ–Ω–∏: –ª–æ–≥–∞—Ä–∏—Ñ–º—ñ—á–Ω–µ –Ω–∞—Å–∏—á–µ–Ω–Ω—è
            sign = 1 if total_return > 0 else -1
            scaled_return = abs(total_return)
            base_reward = sign * (1.5 + np.log(scaled_return * 5) * 0.8)  # –ú'—è–∫–µ –Ω–∞—Å–∏—á–µ–Ω–Ω—è
        
        # –°–¢–ê–ë–Ü–õ–Ü–ó–£–Æ–ß–Ü –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏ –¥–ª—è –∑–º–µ–Ω—à–µ–Ω–Ω—è –≤–∞—Ä—ñ–∞—Ç–∏–≤–Ω–æ—Å—Ç—ñ
        win_rate = env_state.get('win_rate', 0.5)
        max_drawdown = env_state.get('max_drawdown', 0.0)
        
        # –ë–æ–Ω—É—Å –∑–∞ —Å—Ç–∞–±—ñ–ª—å–Ω—ñ—Å—Ç—å (–∑–Ω–∏–∂—É—î –≤–∞—Ä—ñ–∞—Ç–∏–≤–Ω—ñ—Å—Ç—å)
        stability_bonus = 0.0
        if total_trades > 5:  # –¢—ñ–ª—å–∫–∏ –ø—Ä–∏ –¥–æ—Å—Ç–∞—Ç–Ω—ñ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
            # –í–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑–∞ –∑–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω—É —Ç–æ—Ä–≥—ñ–≤–ª—é
            if 0.4 <= win_rate <= 0.7 and max_drawdown < 0.15:
                stability_bonus = 0.3 * (1 - abs(win_rate - 0.55) * 4)  # –ú–∞–∫—Å–∏–º—É–º –ø—Ä–∏ 55% –≤–∏–Ω—Ä–µ–π—Ç
            
            # –ú'—è–∫–∏–π —à—Ç—Ä–∞—Ñ –∑–∞ –∫—Ä–∞–π–Ω–æ—â—ñ
            if win_rate < 0.3 or max_drawdown > 0.25:
                stability_bonus -= 0.2
        
        # –§–Ü–ù–ê–õ–¨–ù–ê –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑ –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –≤–∞—Ä—ñ–∞—Ç–∏–≤–Ω–æ—Å—Ç—ñ
        final_reward = base_reward + stability_bonus
        
        # –ó–ë–Ü–õ–¨–®–ï–ù–ò–ô –∞–¥–∞–ø—Ç–∏–≤–Ω–∏–π —à—É–º –¥–ª—è –¥–∏—Ñ–µ—Ä–µ–Ω—Ü—ñ–∞—Ü—ñ—ó —Å—Ö–æ–∂–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
        adaptive_noise = np.random.normal(0, 0.1)  # –ó–ë–Ü–õ–¨–®–ï–ù–û —à—É–º –¥–æ ¬±0.1 –¥–ª—è –∫—Ä–∞—â–æ—ó –≤–∞—Ä—ñ–∞—Ç–∏–≤–Ω–æ—Å—Ç—ñ
        final_reward += adaptive_noise
        
        # –ö–ê–†–î–ò–ù–ê–õ–¨–ù–û –†–û–ó–®–ò–†–ï–ù–Ü –º–µ–∂—ñ –¥–ª—è –¥–∏—Ñ–µ—Ä–µ–Ω—Ü—ñ–∞—Ü—ñ—ó —Å—Ö–æ–∂–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
        final_reward = np.clip(final_reward, -15.0, 25.0)  # –ó–ë–Ü–õ–¨–®–ï–ù–û –¥—ñ–∞–ø–∞–∑–æ–Ω –¥–ª—è –∫—Ä–∞—â–æ—ó –¥–∏—Ñ–µ—Ä–µ–Ω—Ü—ñ–∞—Ü—ñ—ó —Å—Ç—Ä–∞—Ç–µ–≥—ñ–π
        
        self.reward_history.append(final_reward)
        return final_reward
                
        # –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è —Ç—ñ–ª—å–∫–∏ –¥–ª—è –ø—Ä–∏–±—É—Ç–∫–æ–≤–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ (>0.1%)
        if total_reward > 0:
            # –î–ª—è –ø–æ–∑–∏—Ç–∏–≤–Ω–∏—Ö –≤–∏–Ω–∞–≥–æ—Ä–æ–¥ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –º'—è–∫–µ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è
            scaled_reward = np.tanh(total_reward / 5.0) * 2.0
        else:
            # –î–ª—è –Ω–µ–≥–∞—Ç–∏–≤–Ω–∏—Ö –≤–∏–Ω–∞–≥–æ—Ä–æ–¥ –∑–±–µ—Ä—ñ–≥–∞—î–º–æ –ø–æ–≤–Ω—É —Å–∏–ª—É —à—Ç—Ä–∞—Ñ—É
            scaled_reward = max(total_reward, -3.0)  # –ú–∞–∫—Å–∏–º—É–º -3.0 —à—Ç—Ä–∞—Ñ—É
        
        # –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–π —à—É–º
        noise = np.random.normal(0, 0.005)
        final_reward = scaled_reward + noise
        
        self.reward_history.append(final_reward)
        return final_reward
    
    def _calculate_improvement_bonus(self, env_state: Dict) -> float:
        """–í–ò–ü–†–ê–í–õ–ï–ù–ê –ª–æ–≥—ñ–∫–∞ –±–æ–Ω—É—Å—ñ–≤ - –í–ò–ú–ö–ù–ï–ù–û –º—ñ–∂–µ–ø—ñ–∑–æ–¥–Ω—ñ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è."""
        # –ü–û–í–ù–Ü–°–¢–Æ –í–ò–ú–ö–ù–ï–ù–û —Å–∏—Å—Ç–µ–º—É –ø–æ–∫—Ä–∞—â–µ–Ω—å —á–µ—Ä–µ–∑ –Ω–µ–∫–æ—Ä–µ–∫—Ç–Ω—ñ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
        # –°–∏—Å—Ç–µ–º–∞ –ø–æ—Ä—ñ–≤–Ω—é–≤–∞–ª–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ —Ä—ñ–∑–Ω–∏—Ö –µ–ø—ñ–∑–æ–¥—ñ–≤, —Å—Ç–≤–æ—Ä—é—é—á–∏ –ª–æ–∂–Ω—ñ "–ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è"
        # –ù–∞–ø—Ä–∏–∫–ª–∞–¥: -50% –≤ –æ–¥–Ω–æ–º—É –µ–ø—ñ–∑–æ–¥—ñ ‚Üí +20% –≤ –Ω–∞—Å—Ç—É–ø–Ω–æ–º—É = "–ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è" –Ω–∞ 70%
        # –¶–µ –ø—Ä–∏–∑–≤–æ–¥–∏–ª–æ –¥–æ —Ä–æ–∑–±—ñ–∂–Ω–æ—Å—Ç–µ–π –º—ñ–∂ –ø–æ–∫–∞–∑–Ω–∏–∫–∞–º–∏ total_return —Ç–∞ PROFIT IMPROVEMENT
        
        return 0.0  # –í–ò–ú–ö–ù–ï–ù–û –≤—Å—ñ –±–æ–Ω—É—Å–∏ –∑–∞ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è
    
    def _calculate_escalation_penalty(self, env_state: Dict) -> float:
        """–†–æ–∑—Ä–∞—Ö–æ–≤—É—î –µ—Å–∫–∞–ª–∞—Ü—ñ–π–Ω–∏–π —à—Ç—Ä–∞—Ñ –∑–∞ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω—É –ø–æ–≥–∞–Ω—É –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å."""
        current_return = env_state.get('total_return', 0.0)
        current_step = env_state.get('step', 0)
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ –ø–æ—Ç–æ—á–Ω–∞ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å –ø–æ–≥–∞–Ω–∞
        if current_return < self.poor_performance_threshold:
            self.consecutive_poor_episodes += 1
        else:
            # –°–∫–∏–¥–∞—î–º–æ –ª—ñ—á–∏–ª—å–Ω–∏–∫ —è–∫—â–æ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å –ø–æ–∫—Ä–∞—â–∏–ª–∞—Å—è
            self.consecutive_poor_episodes = 0
            return 0.0
        
        # –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ –µ—Å–∫–∞–ª–∞—Ü—ñ–π–Ω–∏–π —à—Ç—Ä–∞—Ñ
        escalation_penalty = 0.0
        
        if self.consecutive_poor_episodes >= 10:  # 10+ –ø–æ–≥–∞–Ω–∏—Ö –µ–ø—ñ–∑–æ–¥—ñ–≤ –ø—ñ–¥—Ä—è–¥
            # –ñ–û–†–°–¢–ö–ò–ô –®–¢–†–ê–§ –∑–∞ —Ç—Ä–∏–≤–∞–ª—É –ø–æ–≥–∞–Ω—É –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å
            escalation_multiplier = min(self.consecutive_poor_episodes / 10.0, 5.0)  # –î–æ 5x –º–Ω–æ–∂–Ω–∏–∫–∞
            base_penalty = abs(current_return) * 30.0  # –ë–∞–∑–æ–≤–∏–π —à—Ç—Ä–∞—Ñ
            escalation_penalty = -base_penalty * escalation_multiplier
            escalation_penalty = max(escalation_penalty, -50.0)  # –ú–∞–∫—Å–∏–º—É–º -50.0 —à—Ç—Ä–∞—Ñ—É
            
            # –õ–æ–≥—É–≤–∞–Ω–Ω—è –∫–æ–∂–Ω—ñ 50 –ø–æ–≥–∞–Ω–∏—Ö –µ–ø—ñ–∑–æ–¥—ñ–≤
            if self.consecutive_poor_episodes % 50 == 0:
                print(f"üî• ESCALATION PENALTY: {self.consecutive_poor_episodes} consecutive poor episodes")
                print(f"   Return: {current_return:.2%}, Penalty: {escalation_penalty:.2f}")
        
        elif self.consecutive_poor_episodes >= 5:  # 5-9 –ø–æ–≥–∞–Ω–∏—Ö –µ–ø—ñ–∑–æ–¥—ñ–≤
            # –ü–æ–º—ñ—Ä–Ω–∏–π –¥–æ–¥–∞—Ç–∫–æ–≤–∏–π —à—Ç—Ä–∞—Ñ
            escalation_penalty = -abs(current_return) * 10.0
            escalation_penalty = max(escalation_penalty, -10.0)
        
        return escalation_penalty
    
    def reset(self):
        """–°–∫–∏–¥–∞–Ω–Ω—è —ñ—Å—Ç–æ—Ä—ñ—ó –≤–∏–Ω–∞–≥–æ—Ä–æ–¥."""
        self.reward_history = []
        if self.enable_dynamic_scaling:
            self.scaler.reset()
        for scheme in self.schemes:
            scheme.reset()
        
        # –í–ò–ü–†–ê–í–õ–ï–ù–û: –°–∫–∏–¥–∞—î–º–æ —Å–∏—Å—Ç–µ–º—É –ø—Ä–æ–≥—Ä–µ—Å–∏–≤–Ω–∏—Ö –Ω–∞–≥–æ—Ä–æ–¥ –¥–æ –†–ï–ê–õ–¨–ù–ò–• –ø–æ—á–∞—Ç–∫–æ–≤–∏—Ö –∑–Ω–∞—á–µ–Ω—å
        self.last_performance = {
            'return': 0.0,  # –í–ò–ü–†–ê–í–õ–ï–ù–û: –ü–æ—á–∏–Ω–∞—î–º–æ –∑ –Ω—É–ª—å–æ–≤–æ—ó –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—ñ (–Ω–µ -100%)
            'drawdown': 0.0,  # –í–ò–ü–†–ê–í–õ–ï–ù–û: –ü–æ—á–∏–Ω–∞—î–º–æ –±–µ–∑ –ø—Ä–æ—Å–∞–¥–∫–∏
            'trades': 0,
            'win_rate': 0.0
        }
        self.improvement_bonus_count = 0
        
        # –°–∫–∏–¥–∞—î–º–æ —Å–∏—Å—Ç–µ–º—É –µ—Å–∫–∞–ª–∞—Ü—ñ—ó —à—Ç—Ä–∞—Ñ—ñ–≤
        self.consecutive_poor_episodes = 0


class SharpeRatioReward(BaseRewardScheme):
    """–í–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∞ –®–∞—Ä–ø–∞."""
    
    def __init__(self, weight: float = 0.3, window: int = 50, risk_free_rate: float = 0.02):
        super().__init__(weight)
        self.window = window
        self.risk_free_rate = risk_free_rate / 252  # –î–µ–Ω–Ω–∞ –±–µ–∑—Ä–∏–∑–∏–∫–æ–≤–∞ —Å—Ç–∞–≤–∫–∞
        self.returns_history = []
    
    def calculate(self, env_state: Dict) -> float:
        portfolio_history = env_state.get('portfolio_history', [])
        
        if len(portfolio_history) < 2:
            return 0.0
        
        # –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ –¥–µ–Ω–Ω—ñ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—ñ
        returns = np.diff(portfolio_history) / portfolio_history[:-1]
        self.returns_history.extend(returns[-1:])  # –î–æ–¥–∞—î–º–æ —Ç—ñ–ª—å–∫–∏ –æ—Å—Ç–∞–Ω–Ω—é –¥–æ—Ö–æ–¥–Ω—ñ—Å—Ç—å
        
        # –û–±–º–µ–∂—É—î–º–æ —Ä–æ–∑–º—ñ—Ä —ñ—Å—Ç–æ—Ä—ñ—ó
        if len(self.returns_history) > self.window:
            self.returns_history = self.returns_history[-self.window:]
        
        if len(self.returns_history) < 10:  # –ú—ñ–Ω—ñ–º—É–º –¥–ª—è —Ä–æ–∑—Ä–∞—Ö—É–Ω–∫—É
            return 0.0
        
        # –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç –®–∞—Ä–ø–∞
        excess_returns = np.array(self.returns_history) - self.risk_free_rate
        if np.std(excess_returns) > 0:
            sharpe = np.mean(excess_returns) / np.std(excess_returns)
            return np.tanh(sharpe) * self.weight
        
        return 0.0
    
    def reset(self):
        self.returns_history = []


class TradeQualityReward(BaseRewardScheme):
    """–í–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑–∞ —è–∫—ñ—Å—Ç—å —É–≥–æ–¥."""
    
    def __init__(self, weight: float = 0.2, min_trades: int = 5):
        super().__init__(weight)
        self.min_trades = min_trades
    
    def calculate(self, env_state: Dict) -> float:
        total_trades = env_state.get('total_trades', 0)
        win_rate = env_state.get('win_rate', 0.0)
        
        if total_trades < self.min_trades:
            return 0.0
        
        # –ë–æ–Ω—É—Å –∑–∞ –≤–∏—Å–æ–∫—É –¥–æ–ª—é –ø—Ä–∏–±—É—Ç–∫–æ–≤–∏—Ö —É–≥–æ–¥
        if win_rate > 0.6:
            return (win_rate - 0.5) * 2 * self.weight
        elif win_rate < 0.4:
            return -(0.5 - win_rate) * 2 * self.weight
        
        return 0.0


class VolatilityPenalty(BaseRewardScheme):
    """–®—Ç—Ä–∞—Ñ –∑–∞ –≤–∏—Å–æ–∫—É –≤–æ–ª–∞—Ç–∏–ª—å–Ω—ñ—Å—Ç—å –ø–æ—Ä—Ç—Ñ–µ–ª—è."""
    
    def __init__(self, weight: float = -0.1, window: int = 20):
        super().__init__(weight)
        self.window = window
    
    def calculate(self, env_state: Dict) -> float:
        portfolio_history = env_state.get('portfolio_history', [])
        
        if len(portfolio_history) < self.window:
            return 0.0
        
        # –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ –≤–æ–ª–∞—Ç—ñ–ª—å–Ω—ñ—Å—Ç—å –æ—Å—Ç–∞–Ω–Ω—ñ—Ö –∑–Ω–∞—á–µ–Ω—å
        recent_values = portfolio_history[-self.window:]
        returns = np.diff(recent_values) / recent_values[:-1]
        volatility = np.std(returns)
        
        # –®—Ç—Ä–∞—Ñ –∑–∞ –≤–∏—Å–æ–∫—É –≤–æ–ª–∞—Ç—ñ–ª—å–Ω—ñ—Å—Ç—å
        if volatility > 0.05:  # 5% –¥–µ–Ω–Ω–∞ –≤–æ–ª–∞—Ç—ñ–ª—å–Ω—ñ—Å—Ç—å
            return -volatility * 10 * abs(self.weight)
        
        return 0.0


class ConsistencyReward(BaseRewardScheme):
    """–í–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑–∞ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ñ—Å—Ç—å –ø—Ä–∏–±—É—Ç–∫—É."""
    
    def __init__(self, weight: float = 0.15, window: int = 30):
        super().__init__(weight)
        self.window = window
    
    def calculate(self, env_state: Dict) -> float:
        portfolio_history = env_state.get('portfolio_history', [])
        
        if len(portfolio_history) < self.window:
            return 0.0
        
        # –ê–Ω–∞–ª—ñ–∑—É—î–º–æ –æ—Å—Ç–∞–Ω–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è
        recent_values = portfolio_history[-self.window:]
        returns = np.diff(recent_values) / recent_values[:-1]
        
        # –î–æ–ª—è –ø–æ–∑–∏—Ç–∏–≤–Ω–∏—Ö –¥–Ω—ñ–≤
        positive_days_ratio = np.sum(returns > 0) / len(returns)
        
        # –ë–æ–Ω—É—Å –∑–∞ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ñ—Å—Ç—å
        if positive_days_ratio > 0.6:
            return (positive_days_ratio - 0.5) * 2 * self.weight
        
        return 0.0


class TotalReturnReward(BaseRewardScheme):
    """–í–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑–∞ –∑–∞–≥–∞–ª—å–Ω—É –ø—Ä–∏–±—É—Ç–∫–æ–≤—ñ—Å—Ç—å –ø–æ—Ä—Ç—Ñ–µ–ª—è."""
    
    def __init__(self, weight: float = 3.0, initial_balance: float = 10000.0):
        super().__init__(weight)
        self.initial_balance = initial_balance
        
    def calculate(self, env_state: Dict) -> float:
        portfolio_value = env_state.get('portfolio_value', self.initial_balance)
        
        # –û—Ç—Ä–∏–º—É—î–º–æ –ø–æ—á–∞—Ç–∫–æ–≤–∏–π –±–∞–ª–∞–Ω—Å –∑ —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞ –∞–±–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –∑–Ω–∞—á–µ–Ω–Ω—è –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º
        initial_balance = env_state.get('initial_balance', self.initial_balance)
        
        # –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ –∑–∞–≥–∞–ª—å–Ω—É –¥–æ—Ö–æ–¥–Ω—ñ—Å—Ç—å
        total_return = (portfolio_value - initial_balance) / initial_balance
        
        # –ü—Ä–æ–≥—Ä–µ—Å–∏–≤–Ω–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑–∞ –ø—Ä–∏–±—É—Ç–æ–∫ –∑ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–∏–º –∑—Ä–æ—Å—Ç–∞–Ω–Ω—è–º
        if total_return > 0:
            # –ü–æ–∑–∏—Ç–∏–≤–Ω–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑–∞ –ø—Ä–∏–±—É—Ç–æ–∫ –∑ –±–æ–Ω—É—Å–æ–º –∑–∞ –≤–∏—Å–æ–∫—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
            if total_return > 0.5:  # >50% –ø—Ä–∏–±—É—Ç–æ–∫
                reward = (total_return * 15.0 + 5.0) * self.weight  # –í–µ–ª–∏–∫–∏–π –±–æ–Ω—É—Å
            elif total_return > 0.2:  # >20% –ø—Ä–∏–±—É—Ç–æ–∫
                reward = (total_return * 12.0 + 2.0) * self.weight  # –•–æ—Ä–æ—à–∏–π –±–æ–Ω—É—Å
            elif total_return > 0.1:  # >10% –ø—Ä–∏–±—É—Ç–æ–∫
                reward = (total_return * 10.0 + 1.0) * self.weight  # –ü–æ–º—ñ—Ä–Ω–∏–π –±–æ–Ω—É—Å
            else:  # 0-10% –ø—Ä–∏–±—É—Ç–æ–∫
                reward = total_return * 8.0 * self.weight  # –ë–∞–∑–æ–≤–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞
        else:
            # –ú'—è–∫–∏–π —à—Ç—Ä–∞—Ñ –∑–∞ –∑–±–∏—Ç–∫–∏ (–∑–Ω–∞—á–Ω–æ –º–µ–Ω—à–∏–π –Ω—ñ–∂ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑–∞ –ø—Ä–∏–±—É—Ç–æ–∫)
            reward = total_return * 3.0 * self.weight  # –ú–µ–Ω—à–∏–π —à—Ç—Ä–∞—Ñ –∑–∞–æ—Ö–æ—á—É—î —Ä–∏–∑–∏–∫
            
        return np.clip(reward, -5.0, 20.0)  # –û–±–º–µ–∂—É—î–º–æ –¥—ñ–∞–ø–∞–∑–æ–Ω
    
    def reset(self):
        pass


class StepProfitReward(BaseRewardScheme):
    """–í–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑–∞ –∫—Ä–æ–∫–æ–≤—ñ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è –ø–æ—Ä—Ç—Ñ–µ–ª—è (–∑–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω–∏–π –ø—ñ–¥—Ö—ñ–¥)."""
    
    def __init__(self, weight: float = 1.0):
        super().__init__(weight)
        self.last_portfolio_value = None
        
    def calculate(self, env_state: Dict) -> float:
        portfolio_value = env_state.get('portfolio_value', 10000)
        
        if self.last_portfolio_value is None:
            self.last_portfolio_value = portfolio_value
            return 0.0
            
        # –ö—Ä–æ–∫–æ–≤–∞ –∑–º—ñ–Ω–∞ –ø–æ—Ä—Ç—Ñ–µ–ª—è
        step_change = (portfolio_value - self.last_portfolio_value) / self.last_portfolio_value
        self.last_portfolio_value = portfolio_value
        
        # –ú'—è–∫–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑–∞ –∫—Ä–æ–∫–æ–≤—ñ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è
        if step_change > 0.005:  # >0.5% –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è
            reward = min(step_change * 20.0, 2.0) * self.weight  # –ú–∞–∫—Å–∏–º—É–º +2.0
        elif step_change > 0:  # –ú–∞–ª–µ–Ω—å–∫—ñ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è
            reward = step_change * 10.0 * self.weight
        elif step_change > -0.01:  # –ú–∞–ª–µ–Ω—å–∫—ñ –≤—Ç—Ä–∞—Ç–∏ (-1%)
            reward = step_change * 2.0 * self.weight  # –ú'—è–∫–∏–π —à—Ç—Ä–∞—Ñ
        else:  # –í–µ–ª–∏–∫—ñ –≤—Ç—Ä–∞—Ç–∏
            reward = step_change * 5.0 * self.weight  # –ü–æ–º—ñ—Ä–Ω–∏–π —à—Ç—Ä–∞—Ñ
            
        return np.clip(reward, -1.0, 2.0)  # –û–±–º–µ–∂—É—î–º–æ –¥—ñ–∞–ø–∞–∑–æ–Ω
    
    def reset(self):
        self.last_portfolio_value = None


class LossTradesPenalty(BaseRewardScheme):
    """–ê–ì–†–ï–°–ò–í–ù–Ü —à—Ç—Ä–∞—Ñ–∏ –∑–∞ –∑–±–∏—Ç–∫–æ–≤—ñ —É–≥–æ–¥–∏ —Ç–∞ –∑–∞–≥–∞–ª—å–Ω—ñ –≤—Ç—Ä–∞—Ç–∏."""
    
    def __init__(self, weight: float = -4.0):
        super().__init__(weight)
        
    def calculate(self, env_state: Dict) -> float:
        total_return = env_state.get('total_return', 0.0)
        total_trades = env_state.get('total_trades', 0)
        portfolio_value = env_state.get('portfolio_value', 10000)
        initial_balance = env_state.get('initial_balance', 10000)
        
        penalty = 0.0
        
        # 1. –ê–ì–†–ï–°–ò–í–ù–Ü —à—Ç—Ä–∞—Ñ–∏ –∑–∞ –∑–∞–≥–∞–ª—å–Ω—ñ –∑–±–∏—Ç–∫–∏
        if total_return < 0:
            # –ü—Ä–æ–≥—Ä–µ—Å–∏–≤–Ω–∏–π —à—Ç—Ä–∞—Ñ –∑–∞ –∑–±–∏—Ç–∫–∏
            if total_return < -0.10:  # >10% –∑–±–∏—Ç–∫—ñ–≤
                penalty += abs(total_return) * 15.0  # –ö—Ä–∏—Ç–∏—á–Ω–∏–π —à—Ç—Ä–∞—Ñ
            elif total_return < -0.05:  # >5% –∑–±–∏—Ç–∫—ñ–≤
                penalty += abs(total_return) * 10.0  # –í–∏—Å–æ–∫–∏–π —à—Ç—Ä–∞—Ñ
            else:  # <5% –∑–±–∏—Ç–∫—ñ–≤
                penalty += abs(total_return) * 5.0   # –ü–æ–º—ñ—Ä–Ω–∏–π —à—Ç—Ä–∞—Ñ
                
        # 2. –î–û–î–ê–¢–ö–û–í–Ü —à—Ç—Ä–∞—Ñ–∏ –∑–∞ –≤—Ç—Ä–∞—Ç–∏ –ø—Ä–∏ –∞–∫—Ç–∏–≤–Ω—ñ–π —Ç–æ—Ä–≥—ñ–≤–ª—ñ
        if total_trades > 5 and total_return < -0.02:  # >2% –∑–±–∏—Ç–∫—ñ–≤ –ø—Ä–∏ >5 —É–≥–æ–¥–∞—Ö
            activity_penalty = abs(total_return) * total_trades * 0.1
            penalty += min(activity_penalty, 3.0)  # –ú–∞–∫—Å–∏–º—É–º +3.0 —à—Ç—Ä–∞—Ñ—É
            
        # 3. –ö–†–ò–¢–ò–ß–ù–Ü —à—Ç—Ä–∞—Ñ–∏ –∑–∞ –∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ—ñ—á–Ω—ñ –≤—Ç—Ä–∞—Ç–∏
        if total_return < -0.15:  # >15% –∑–±–∏—Ç–∫—ñ–≤
            catastrophic_penalty = abs(total_return) * 20.0
            penalty += min(catastrophic_penalty, 5.0)  # –ú–∞–∫—Å–∏–º—É–º +5.0 —à—Ç—Ä–∞—Ñ—É
            
        # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ –Ω–µ–≥–∞—Ç–∏–≤–Ω—É –≤–∏–Ω–∞–≥–æ—Ä–æ–¥—É (—à—Ç—Ä–∞—Ñ)
        final_penalty = -penalty * abs(self.weight) if penalty > 0 else 0.0
        return np.clip(final_penalty, -15.0, 0.0)  # –û–±–º–µ–∂—É—î–º–æ –¥—ñ–∞–ø–∞–∑–æ–Ω —à—Ç—Ä–∞—Ñ—ñ–≤
    
    def reset(self):
        pass


class WinRatePenalty(BaseRewardScheme):
    """–®–¢–†–ê–§–ò –∑–∞ –Ω–∏–∑—å–∫–∏–π –≤–∏–Ω—Ä–µ–π—Ç –ø—Ä–∏ –∞–∫—Ç–∏–≤–Ω—ñ–π —Ç–æ—Ä–≥—ñ–≤–ª—ñ."""
    
    def __init__(self, weight: float = -2.0, min_trades: int = 5):
        super().__init__(weight)
        self.min_trades = min_trades
        
    def calculate(self, env_state: Dict) -> float:
        total_trades = env_state.get('total_trades', 0)
        win_rate = env_state.get('win_rate', 0.0)
        total_return = env_state.get('total_return', 0.0)
        
        # –¢—ñ–ª—å–∫–∏ —à—Ç—Ä–∞—Ñ—É—î–º–æ –ø—Ä–∏ –¥–æ—Å—Ç–∞—Ç–Ω—ñ–π —Ç–æ—Ä–≥–æ–≤—ñ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
        if total_trades < self.min_trades:
            return 0.0
            
        penalty = 0.0
        
        # 1. –®–¢–†–ê–§–ò –∑–∞ –Ω–∏–∑—å–∫–∏–π –≤–∏–Ω—Ä–µ–π—Ç
        if win_rate < 0.4:  # <40% –≤–∏–Ω—Ä–µ–π—Ç
            base_penalty = (0.4 - win_rate) * 5.0  # –®—Ç—Ä–∞—Ñ –ø—Ä–æ–ø–æ—Ä—Ü—ñ–π–Ω–∏–π –≤—ñ–¥—Ö–∏–ª–µ–Ω–Ω—é
            penalty += base_penalty
            
        # 2. –ü–û–î–í–Ü–ô–ù–Ü —à—Ç—Ä–∞—Ñ–∏ –∑–∞ –Ω–∏–∑—å–∫–∏–π –≤–∏–Ω—Ä–µ–π—Ç –ø—Ä–∏ –∑–±–∏—Ç–∫–∞—Ö
        if win_rate < 0.3 and total_return < 0:  # <30% –≤–∏–Ω—Ä–µ–π—Ç + –∑–±–∏—Ç–∫–∏
            double_penalty = (0.3 - win_rate) * 8.0  # –ü–æ–¥–≤—ñ–π–Ω–∏–π —à—Ç—Ä–∞—Ñ
            penalty += double_penalty
            
        # 3. –ö–†–ò–¢–ò–ß–ù–Ü —à—Ç—Ä–∞—Ñ–∏ –∑–∞ –∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ—ñ—á–Ω–æ –Ω–∏–∑—å–∫–∏–π –≤–∏–Ω—Ä–µ–π—Ç
        if win_rate < 0.2 and total_trades > 10:  # <20% –≤–∏–Ω—Ä–µ–π—Ç –ø—Ä–∏ –∞–∫—Ç–∏–≤–Ω—ñ–π —Ç–æ—Ä–≥—ñ–≤–ª—ñ
            critical_penalty = (0.2 - win_rate) * 12.0  # –ö—Ä–∏—Ç–∏—á–Ω–∏–π —à—Ç—Ä–∞—Ñ
            penalty += critical_penalty
            
        # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ –Ω–µ–≥–∞—Ç–∏–≤–Ω—É –≤–∏–Ω–∞–≥–æ—Ä–æ–¥—É (—à—Ç—Ä–∞—Ñ)
        final_penalty = -penalty * abs(self.weight) if penalty > 0 else 0.0
        return np.clip(final_penalty, -8.0, 0.0)  # –û–±–º–µ–∂—É—î–º–æ –¥—ñ–∞–ø–∞–∑–æ–Ω —à—Ç—Ä–∞—Ñ—ñ–≤
    
    def reset(self):
        pass


class ExplorationReward(BaseRewardScheme):
    """–í–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑–∞ —Ç–æ—Ä–≥–æ–≤—É –∞–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å —Ç–∞ –µ–∫—Å–ø–ª–æ—Ä–∞—Ü—ñ—é —Ä—ñ–∑–Ω–∏—Ö —Å—Ç—Ä–∞—Ç–µ–≥—ñ–π."""
    
    def __init__(self, weight: float = 0.5, target_trades_per_episode: int = 50):
        super().__init__(weight)
        self.target_trades_per_episode = target_trades_per_episode
        self.last_trades = 0
        
    def calculate(self, env_state: Dict) -> float:
        total_trades = env_state.get('total_trades', 0)
        current_step = env_state.get('step', 0)
        episode_length = env_state.get('ep_len_mean', 2000)
        
        # –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ –ø—Ä–æ–≥—Ä–µ—Å —Ç–æ—Ä–≥–æ–≤–æ—ó –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
        if episode_length > 0:
            expected_trades = (current_step / episode_length) * self.target_trades_per_episode
            trade_progress = total_trades / max(expected_trades, 1)
        else:
            trade_progress = 0
        
        reward = 0.0
        
        # 1. –í–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑–∞ –¥–æ—Å—è–≥–Ω–µ–Ω–Ω—è —Ü—ñ–ª—å–æ–≤–æ—ó –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
        if trade_progress >= 0.8:  # 80% –≤—ñ–¥ —Ü—ñ–ª—å–æ–≤–æ—ó –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
            reward += 1.0 * self.weight
        elif trade_progress >= 0.5:  # 50% –≤—ñ–¥ —Ü—ñ–ª—å–æ–≤–æ—ó –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
            reward += 0.5 * self.weight
        elif trade_progress >= 0.2:  # 20% –≤—ñ–¥ —Ü—ñ–ª—å–æ–≤–æ—ó –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
            reward += 0.2 * self.weight
        
        # 2. –ë–æ–Ω—É—Å –∑–∞ –Ω–æ–≤—ñ —É–≥–æ–¥–∏ (–∑–∞–æ—Ö–æ—á—É—î–º–æ –∞–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å)
        new_trades = total_trades - self.last_trades
        if new_trades > 0:
            activity_bonus = min(new_trades * 0.1, 0.5) * self.weight  # –ú–∞–∫—Å–∏–º—É–º +0.5
            reward += activity_bonus
        
        # 3. –ú'—è–∫–∏–π —à—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≤–Ω—É –ø–∞—Å–∏–≤–Ω—ñ—Å—Ç—å (—Ç—ñ–ª—å–∫–∏ —è–∫—â–æ –Ω–µ–º–∞—î —É–≥–æ–¥ –≤–∑–∞–≥–∞–ª—ñ)
        if total_trades == 0 and current_step > 500:  # –ü—ñ—Å–ª—è 500 –∫—Ä–æ–∫—ñ–≤ –±–µ–∑ —É–≥–æ–¥
            reward -= 0.2 * abs(self.weight)
        
        self.last_trades = total_trades
        return np.clip(reward, -0.5, 1.0)  # –û–±–º–µ–∂—É—î–º–æ –¥—ñ–∞–ø–∞–∑–æ–Ω
    
    def reset(self):
        self.last_trades = 0


class PerformanceDeclineReward(BaseRewardScheme):
    """
    –î–∏–Ω–∞–º—ñ—á–Ω–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ —â–æ –∑–º–µ–Ω—à—É—î—Ç—å—Å—è –ø—Ä–∏ –ø–æ–≥—ñ—Ä—à–µ–Ω–Ω—ñ –∫–ª—é—á–æ–≤–∏—Ö –º–µ—Ç—Ä–∏–∫:
    - –ö—ñ–ª—å–∫—ñ—Å—Ç—å —É–≥–æ–¥
    - Win rate (–≤—ñ–¥—Å–æ—Ç–æ–∫ –ø—Ä–∏–±—É—Ç–∫–æ–≤–∏—Ö —É–≥–æ–¥)
    - –ü—Ä–æ—Å–∞–¥–∫–∞
    """
    
    def __init__(self, weight: float = -2.0, history_window: int = 10, decline_threshold: float = 0.15):
        super().__init__(weight)
        self.history_window = history_window
        self.decline_threshold = decline_threshold  # 15% –ø–æ–≥—ñ—Ä—à–µ–Ω–Ω—è
        
        # –Ü—Å—Ç–æ—Ä—ñ—è –º–µ—Ç—Ä–∏–∫
        self.trades_history = []
        self.win_rate_history = []
        self.drawdown_history = []
        
        self.last_trades = 0
        self.last_win_rate = 0.0
        self.last_max_drawdown = 0.0
    
    def calculate(self, env_state: Dict) -> float:
        current_trades = env_state.get('total_trades', 0)
        current_win_rate = env_state.get('win_rate', 0.0)
        current_max_drawdown = env_state.get('max_drawdown', 0.0)
        
        # –û–Ω–æ–≤–ª—é—î–º–æ —ñ—Å—Ç–æ—Ä—ñ—é –º–µ—Ç—Ä–∏–∫
        self.trades_history.append(current_trades)
        self.win_rate_history.append(current_win_rate)
        self.drawdown_history.append(current_max_drawdown)
        
        # –û–±–º–µ–∂—É—î–º–æ —Ä–æ–∑–º—ñ—Ä —ñ—Å—Ç–æ—Ä—ñ—ó
        if len(self.trades_history) > self.history_window:
            self.trades_history = self.trades_history[-self.history_window:]
            self.win_rate_history = self.win_rate_history[-self.history_window:]
            self.drawdown_history = self.drawdown_history[-self.history_window:]
        
        # –ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ —ñ—Å—Ç–æ—Ä—ñ—ó –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É
        if len(self.trades_history) < 3:
            return 0.0
        
        total_penalty = 0.0
        
        # 1. –ê–ù–ê–õ–Ü–ó –ö–Ü–õ–¨–ö–û–°–¢–Ü –£–ì–û–î (–ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∑–º–µ–Ω—à–µ–Ω–Ω—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ)
        if len(self.trades_history) >= 6:
            recent_trades_data = self.trades_history[-3:]
            older_trades_data = self.trades_history[-6:-3]
            
            if len(recent_trades_data) >= 3 and len(older_trades_data) >= 3:
                recent_trades = np.mean(recent_trades_data)
                older_trades = np.mean(older_trades_data)
                
                if older_trades > 0:
                    trades_change = (recent_trades - older_trades) / older_trades
                    if trades_change < -self.decline_threshold:  # –ó–º–µ–Ω—à–µ–Ω–Ω—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ –Ω–∞ 15%+
                        penalty = abs(trades_change) * 2.0  # –®—Ç—Ä–∞—Ñ –ø—Ä–æ–ø–æ—Ä—Ü—ñ–π–Ω–∏–π –∑–º–µ–Ω—à–µ–Ω–Ω—é
                        total_penalty += penalty
                        print(f"üîª TRADE ACTIVITY DECLINE: {trades_change:.1%} -> Penalty: {penalty:.2f}")
        
        # 2. –ê–ù–ê–õ–Ü–ó WIN RATE (–ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∑–º–µ–Ω—à–µ–Ω–Ω—è –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ)
        if len(self.win_rate_history) >= 6:
            recent_win_rate_data = [x for x in self.win_rate_history[-3:] if x > 0]
            older_win_rate_data = [x for x in self.win_rate_history[-6:-3] if x > 0]
            
            # –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø: –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —â–æ —Å–ø–∏—Å–∫–∏ –Ω–µ –ø—É—Å—Ç—ñ –ø–µ—Ä–µ–¥ np.mean
            if len(recent_win_rate_data) > 0 and len(older_win_rate_data) > 0:
                recent_win_rate = np.mean(recent_win_rate_data)
                older_win_rate = np.mean(older_win_rate_data)
                
                if older_win_rate > 0.1 and recent_win_rate > 0:  # –¢—ñ–ª—å–∫–∏ —è–∫—â–æ —î –∑–Ω–∞—á—É—â—ñ –¥–∞–Ω—ñ
                    win_rate_change = (recent_win_rate - older_win_rate) / older_win_rate
                    if win_rate_change < -self.decline_threshold:  # –ó–º–µ–Ω—à–µ–Ω–Ω—è –≤–∏–Ω—Ä–µ–π—Ç—É –Ω–∞ 15%+
                        penalty = abs(win_rate_change) * 3.0  # –ë—ñ–ª—å—à–∏–π —à—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≥—ñ—Ä—à–µ–Ω–Ω—è —è–∫–æ—Å—Ç—ñ
                        total_penalty += penalty
                        print(f"üîª WIN RATE DECLINE: {win_rate_change:.1%} -> Penalty: {penalty:.2f}")
        
        # 3. –ê–ù–ê–õ–Ü–ó –ü–†–û–°–ê–î–ö–ò (–ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∑–±—ñ–ª—å—à–µ–Ω–Ω—è —Ä–∏–∑–∏–∫—É)
        if len(self.drawdown_history) >= 6:
            recent_drawdown_data = self.drawdown_history[-3:]
            older_drawdown_data = self.drawdown_history[-6:-3]
            
            if len(recent_drawdown_data) >= 3 and len(older_drawdown_data) >= 3:
                recent_drawdown = np.mean(recent_drawdown_data)
                older_drawdown = np.mean(older_drawdown_data)
                
                if older_drawdown > 0.001:  # –¢—ñ–ª—å–∫–∏ —è–∫—â–æ –±—É–ª–∞ –ø—Ä–æ—Å–∞–¥–∫–∞
                    drawdown_change = (recent_drawdown - older_drawdown) / older_drawdown
                    if drawdown_change > self.decline_threshold:  # –ó–±—ñ–ª—å—à–µ–Ω–Ω—è –ø—Ä–æ—Å–∞–¥–∫–∏ –Ω–∞ 15%+
                        penalty = drawdown_change * 2.5  # –®—Ç—Ä–∞—Ñ –∑–∞ –∑–±—ñ–ª—å—à–µ–Ω–Ω—è —Ä–∏–∑–∏–∫—É
                        total_penalty += penalty
                        print(f"üîª DRAWDOWN INCREASE: {drawdown_change:.1%} -> Penalty: {penalty:.2f}")
        
        # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ —à—Ç—Ä–∞—Ñ (–Ω–µ–≥–∞—Ç–∏–≤–Ω—É –≤–∏–Ω–∞–≥–æ—Ä–æ–¥—É)
        final_penalty = -total_penalty * abs(self.weight) if total_penalty > 0 else 0.0
        
        # –õ–æ–≥—É–≤–∞–Ω–Ω—è –∑–Ω–∞—á–Ω–∏—Ö —à—Ç—Ä–∞—Ñ—ñ–≤
        if final_penalty < -1.0:
            print(f"üìâ PERFORMANCE DECLINE PENALTY: {final_penalty:.2f}")
        
        return final_penalty
    
    def reset(self):
        self.trades_history = []
        self.win_rate_history = []
        self.drawdown_history = []
        self.last_trades = 0
        self.last_win_rate = 0.0
        self.last_max_drawdown = 0.0


def create_default_reward_scheme() -> CompositeRewardScheme:
    """–°—Ç–≤–æ—Ä–∏—Ç–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É —Å—Ö–µ–º—É –≤–∏–Ω–∞–≥–æ—Ä–æ–¥."""
    schemes = [
        ProfitReward(weight=1.0),
        DrawdownPenalty(weight=-0.5),
        SharpeRatioReward(weight=0.3),
        TradeQualityReward(weight=0.2),
        VolatilityPenalty(weight=-0.1),
        ConsistencyReward(weight=0.15)
    ]
    return CompositeRewardScheme(schemes)


def create_conservative_reward_scheme() -> CompositeRewardScheme:
    """–°—Ç–≤–æ—Ä–∏—Ç–∏ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—É —Å—Ö–µ–º—É –≤–∏–Ω–∞–≥–æ—Ä–æ–¥ (–∞–∫—Ü–µ–Ω—Ç –Ω–∞ —Å—Ç–∞–±—ñ–ª—å–Ω—ñ—Å—Ç—å)."""
    schemes = [
        ProfitReward(weight=0.7),
        DrawdownPenalty(weight=-1.0),
        SharpeRatioReward(weight=0.5),
        VolatilityPenalty(weight=-0.3),
        ConsistencyReward(weight=0.4)
    ]
    return CompositeRewardScheme(schemes)


def create_aggressive_reward_scheme() -> CompositeRewardScheme:
    """–°—Ç–≤–æ—Ä–∏—Ç–∏ –∞–≥—Ä–µ—Å–∏–≤–Ω—É —Å—Ö–µ–º—É –≤–∏–Ω–∞–≥–æ—Ä–æ–¥ (–∞–∫—Ü–µ–Ω—Ç –Ω–∞ –ø—Ä–∏–±—É—Ç–æ–∫)."""
    schemes = [
        ProfitReward(weight=1.5),
        DrawdownPenalty(weight=-0.2),
        TradeQualityReward(weight=0.3),
        SharpeRatioReward(weight=0.2)
    ]
    return CompositeRewardScheme(schemes)


class StaticReward(BaseRewardScheme):
    """–ü—Ä–æ—Å—Ç–∞ —Å—Ç–∞—Ç–∏—á–Ω–∞ —Å—Ö–µ–º–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥ –∑ —á—ñ—Ç–∫–∏–º–∏ –ø–æ–∑–∏—Ç–∏–≤–Ω–∏–º–∏/–Ω–µ–≥–∞—Ç–∏–≤–Ω–∏–º–∏ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞–º–∏."""
    
    def __init__(self, weight: float = 1.0, static_initial_balance: float = None):
        super().__init__(weight)
        self.step_count = 0
        self.last_portfolio_value = None
        self.static_initial_balance = static_initial_balance  # –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø: —Å—Ç–∞—Ç–∏—á–Ω–∏–π –ø–æ—á–∞—Ç–∫–æ–≤–∏–π –±–∞–ª–∞–Ω—Å
        self.initial_balance = None
        
    def calculate(self, env_state: Dict) -> float:
        portfolio_value = env_state['portfolio_value']
        current_step = env_state.get('step', 0)
        
        # –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø: –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –°–¢–ê–¢–ò–ß–ù–ò–ô –ø–æ—á–∞—Ç–∫–æ–≤–∏–π –±–∞–ª–∞–Ω—Å
        if self.initial_balance is None:
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Å—Ç–∞—Ç–∏—á–Ω–∏–π –±–∞–ª–∞–Ω—Å –∞–±–æ –±–µ—Ä–µ–º–æ –∑ env_state
            if self.static_initial_balance is not None:
                self.initial_balance = self.static_initial_balance
            else:
                # –û—Ç—Ä–∏–º—É—î–º–æ –ø–æ—á–∞—Ç–∫–æ–≤–∏–π –±–∞–ª–∞–Ω—Å –∑ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞
                self.initial_balance = env_state.get('initial_balance', 10000.0)
            self.last_portfolio_value = portfolio_value
            
        self.step_count += 1
        
        # –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø: –ë–µ–∑–ø–µ—á–Ω–∏–π —Ä–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –∑–º—ñ–Ω–∏ –≤–∞—Ä—Ç–æ—Å—Ç—ñ –ø–æ—Ä—Ç—Ñ–µ–ª—è
        step_change = portfolio_value - self.last_portfolio_value if self.last_portfolio_value is not None else 0
        step_change_percent = step_change / self.last_portfolio_value if self.last_portfolio_value is not None and self.last_portfolio_value > 0 else 0
        
        # –†–æ–∑—Ä–∞—Ö—É—î–º–æ –∑–∞–≥–∞–ª—å–Ω—É –ø—Ä–∏–±—É—Ç–∫–æ–≤—ñ—Å—Ç—å
        total_return_percent = (portfolio_value - self.initial_balance) / self.initial_balance
        
        # –î–£–ñ–ï –ü–†–û–°–¢–ê —Å—Ç–∞—Ç–∏—á–Ω–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑ –≥–∞—Ä–∞–Ω—Ç–æ–≤–∞–Ω–æ—é –≤–∞—Ä—ñ–∞—Ü—ñ—î—é
        base_reward = 0.0
        
        # 1. –û—Å–Ω–æ–≤–Ω–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ –∑–º—ñ–Ω–∏ –ø–æ—Ä—Ç—Ñ–µ–ª—è
        if abs(step_change_percent) > 0.005:  # –ó–Ω–∞—á–Ω–∞ –∑–º—ñ–Ω–∞ > 0.5%
            if step_change_percent > 0:
                base_reward = 3.0  # –í–µ–ª–∏–∫–∏–π –ø—Ä–∏–±—É—Ç–æ–∫
            else:
                base_reward = -3.0  # –í–µ–ª–∏–∫–∏–π –∑–±–∏—Ç–æ–∫
        elif abs(step_change_percent) > 0.001:  # –ü–æ–º—ñ—Ä–Ω–∞ –∑–º—ñ–Ω–∞ > 0.1%
            if step_change_percent > 0:
                base_reward = 1.0  # –ü–æ–º—ñ—Ä–Ω–∏–π –ø—Ä–∏–±—É—Ç–æ–∫
            else:
                base_reward = -1.0  # –ü–æ–º—ñ—Ä–Ω–∏–π –∑–±–∏—Ç–æ–∫
        elif abs(step_change_percent) > 0.0001:  # –ú–∞–ª–∞ –∑–º—ñ–Ω–∞ > 0.01%
            if step_change_percent > 0:
                base_reward = 0.5  # –ú–∞–ª–∏–π –ø—Ä–∏–±—É—Ç–æ–∫
            else:
                base_reward = -0.5  # –ú–∞–ª–∏–π –∑–±–∏—Ç–æ–∫
        else:
            base_reward = -0.2  # –®—Ç—Ä–∞—Ñ –∑–∞ –≤—ñ–¥—Å—É—Ç–Ω—ñ—Å—Ç—å –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
        
        # 2. –î–æ–¥–∞—î–º–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç, —â–æ –∑–º—ñ–Ω—é—î—Ç—å—Å—è –≤—ñ–¥ –∫—Ä–æ–∫—É –¥–æ –∫—Ä–æ–∫—É
        step_variation = np.sin(current_step * 0.1) * 0.3  # –í–∞—Ä—ñ–∞—Ü—ñ—è ¬±0.3
        
        # 3. –î–æ–¥–∞—î–º–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –∑–∞–ª–µ–∂–Ω–∏–π –≤—ñ–¥ –∑–∞–≥–∞–ª—å–Ω–æ—ó –ø—Ä–∏–±—É—Ç–∫–æ–≤–æ—Å—Ç—ñ
        performance_bonus = 0.0
        if total_return_percent > 0.1:  # > 10% –ø—Ä–∏–±—É—Ç–æ–∫
            performance_bonus = 2.0
        elif total_return_percent > 0.05:  # > 5% –ø—Ä–∏–±—É—Ç–æ–∫
            performance_bonus = 1.0
        elif total_return_percent < -0.1:  # > 10% –∑–±–∏—Ç–æ–∫
            performance_bonus = -2.0
        elif total_return_percent < -0.05:  # > 5% –∑–±–∏—Ç–æ–∫
            performance_bonus = -1.0
        
        # 4. –î–æ–¥–∞—î–º–æ –Ω–µ–≤–µ–ª–∏–∫–∏–π —Ä–∞–Ω–¥–æ–º–Ω–∏–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –¥–ª—è –≥–∞—Ä–∞–Ω—Ç–æ–≤–∞–Ω–æ—ó –≤–∞—Ä—ñ–∞—Ü—ñ—ó
        random_component = np.random.uniform(-0.2, 0.2)
        
        # –ü—ñ–¥—Å—É–º–∫–æ–≤–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞
        final_reward = base_reward + step_variation + performance_bonus + random_component
        
        # –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–µ –ª–æ–≥—É–≤–∞–Ω–Ω—è - —Ç—ñ–ª—å–∫–∏ –ø—Ä–∏ –∑–Ω–∞—á–Ω–∏—Ö –∑–º—ñ–Ω–∞—Ö –∞–±–æ –ø–µ—Ä—ñ–æ–¥–∏—á–Ω–æ
        if current_step % 1000 == 0 and current_step > 0:  # –í–∏–≤–æ–¥–∏–º–æ —Ç—ñ–ª—å–∫–∏ –∫–æ–∂–Ω—ñ 1000 –∫—Ä–æ–∫—ñ–≤
            print(f"Step {current_step}: Portfolio: {portfolio_value:.2f}, Return: {total_return_percent:.2%}, Reward: {final_reward:.2f}")
        
        self.last_portfolio_value = portfolio_value
        
        # –û–±–º–µ–∂—É—î–º–æ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥—É –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ –Ω–∞–≤—á–∞–Ω–Ω—è
        final_reward = np.clip(final_reward * self.weight, -10.0, 10.0)
        return final_reward
    
    def reset(self):
        self.step_count = 0
        self.last_portfolio_value = None
        # –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø: –ù–ï —Å–∫–∏–¥–∞—î–º–æ initial_balance, —â–æ–± –∑–±–µ—Ä–µ–≥—Ç–∏ —Å—Ç–∞—Ç–∏—á–Ω—ñ—Å—Ç—å
        # self.initial_balance = None  # –ó–∞–∫–æ–º–µ–Ω—Ç–æ–≤–∞–Ω–æ –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Å—Ç–∞—Ç–∏—á–Ω–æ–≥–æ –±–∞–ª–∞–Ω—Å—É


class SimpleProfitReward(BaseRewardScheme):
    """–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø—Ä–æ—Å—Ç–∞ —Å—Ö–µ–º–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥: –ø—Ä–∏–±—É—Ç–æ–∫ = –¥–æ–±—Ä–µ, –∑–±–∏—Ç–æ–∫ = –ø–æ–≥–∞–Ω–æ."""
    
    def __init__(self, weight: float = 1.0):
        super().__init__(weight)
        self.initial_balance = None
        
    def calculate(self, env_state: Dict) -> float:
        if self.initial_balance is None:
            self.initial_balance = env_state.get('initial_balance', 10000.0)
            
        portfolio_value = env_state.get('portfolio_value', self.initial_balance)
        total_return = (portfolio_value - self.initial_balance) / self.initial_balance
        
        # –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û –ü–†–û–°–¢–ê –õ–û–ì–Ü–ö–ê:
        # –ü—Ä–∏–±—É—Ç–æ–∫ = –ø–æ–∑–∏—Ç–∏–≤–Ω–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞
        # –ó–±–∏—Ç–æ–∫ = –Ω–µ–≥–∞—Ç–∏–≤–Ω–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞
        # –ë—ñ–ª—å—à–∏–π –ø—Ä–∏–±—É—Ç–æ–∫ = –±—ñ–ª—å—à–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞
        # –ë—ñ–ª—å—à–∏–π –∑–±–∏—Ç–æ–∫ = –±—ñ–ª—å—à–∏–π —à—Ç—Ä–∞—Ñ
        
        if total_return > 0:
            # –ü—Ä–æ–≥—Ä–µ—Å–∏–≤–Ω–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑–∞ –ø—Ä–∏–±—É—Ç–æ–∫
            if total_return > 0.20:      # >20% = –≤—ñ–¥–º—ñ–Ω–Ω–æ
                reward = 10.0
            elif total_return > 0.10:    # >10% = –¥—É–∂–µ –¥–æ–±—Ä–µ  
                reward = 5.0
            elif total_return > 0.05:    # >5% = –¥–æ–±—Ä–µ
                reward = 2.0
            else:                        # >0% = –¥–æ–±—Ä–µ
                reward = 1.0
        else:
            # –ü—Ä–æ–≥—Ä–µ—Å–∏–≤–Ω–∏–π —à—Ç—Ä–∞—Ñ –∑–∞ –∑–±–∏—Ç–∫–∏
            if total_return < -0.20:     # >20% –∑–±–∏—Ç–∫—ñ–≤ = –∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ–∞
                reward = -10.0
            elif total_return < -0.10:   # >10% –∑–±–∏—Ç–∫—ñ–≤ = –¥—É–∂–µ –ø–æ–≥–∞–Ω–æ
                reward = -5.0
            elif total_return < -0.05:   # >5% –∑–±–∏—Ç–∫—ñ–≤ = –ø–æ–≥–∞–Ω–æ
                reward = -2.0
            else:                        # <5% –∑–±–∏—Ç–∫—ñ–≤ = –Ω–µ —Å—Ç—Ä–∞—à–Ω–æ
                reward = -1.0
        
        # –õ–æ–≥—É–≤–∞–Ω–Ω—è –¥–ª—è –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥—É
        step = env_state.get('step', 0)
        if step % 500 == 0:
            print(f"üí° SIMPLE PROFIT REWARD: return={total_return:+.1%} ‚Üí reward={reward:.1f}")
            
        return reward * self.weight
    
    def reset(self):
        # –ù–ï —Å–∫–∏–¥–∞—î–º–æ initial_balance –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ
        pass


class AdaptiveTradeOffReward(BaseRewardScheme):
    """
    –ê–¥–∞–ø—Ç–∏–≤–Ω–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ —â–æ –¥–æ–∑–≤–æ–ª—è—î –º–æ–¥–µ–ª—ñ –≤–∏–±–∏—Ä–∞—Ç–∏ –º—ñ–∂ —Ä—ñ–∑–Ω–∏–º–∏ —Ü—ñ–ª—è–º–∏:
    - –ü—Ä–∏–±—É—Ç–æ–∫ vs –ü—Ä–æ—Å–∞–¥–∫–∞
    - –ê–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å vs –Ø–∫—ñ—Å—Ç—å —É–≥–æ–¥
    - –°—Ç–∞–±—ñ–ª—å–Ω—ñ—Å—Ç—å vs –ê–≥—Ä–µ—Å–∏–≤–Ω—ñ—Å—Ç—å
    """
    
    def __init__(self, weight: float = 10.0, adaptation_window: int = 50):
        super().__init__(weight)
        self.adaptation_window = adaptation_window
        self.performance_history = []
        self.current_strategy = "balanced"  # balanced, profit_focused, risk_focused
        self.strategy_counter = 0
        
    def calculate(self, env_state: Dict) -> float:
        portfolio_value = env_state.get('portfolio_value', 10000)
        total_return = env_state.get('total_return', 0.0)
        max_drawdown = env_state.get('max_drawdown', 0.0)
        win_rate = env_state.get('win_rate', 0.0)
        total_trades = env_state.get('total_trades', 0)
        
        # –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø: –§—ñ–ª—å—Ç—Ä—É—î–º–æ –±–µ–∑–≥–ª—É–∑–¥—ñ –∑–∞–ø–∏—Å–∏ –∑ –Ω—É–ª—å–æ–≤–∏–º–∏ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ç—ñ–ª—å–∫–∏ –∑–Ω–∞—á—É—â—ñ –¥–∞–Ω—ñ (–∑ —Ç–æ—Ä–≥–æ–≤–æ—é –∞–∫—Ç–∏–≤–Ω—ñ—Å—Ç—é –∞–±–æ –∑–Ω–∞—á—É—â–∏–º–∏ –∑–º—ñ–Ω–∞–º–∏)
        is_meaningful_data = (
            total_trades > 0 or  # –Ñ —Ç–æ—Ä–≥–æ–≤–∞ –∞–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å
            abs(total_return) > 0.001 or  # –Ñ –∑–Ω–∞—á—É—â—ñ –∑–º—ñ–Ω–∏ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—ñ (>0.1%)
            max_drawdown > 0.001  # –Ñ –∑–Ω–∞—á—É—â–∞ –ø—Ä–æ—Å–∞–¥–∫–∞ (>0.1%)
        )
        
        if is_meaningful_data:
            self.performance_history.append({
                'return': total_return,
                'drawdown': max_drawdown,
                'win_rate': win_rate,
                'trades': total_trades
            })
            
            # –û–±–º–µ–∂—É—î–º–æ —ñ—Å—Ç–æ—Ä—ñ—é
            if len(self.performance_history) > self.adaptation_window:
                self.performance_history = self.performance_history[-self.adaptation_window:]
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω–∏–π –≤–∏–±—ñ—Ä —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó –∫–æ–∂–Ω—ñ 20 –∫—Ä–æ–∫—ñ–≤ (—Ç—ñ–ª—å–∫–∏ —è–∫—â–æ —î –¥–æ—Å—Ç–∞—Ç–Ω—å–æ –∑–Ω–∞—á—É—â–∏—Ö –¥–∞–Ω–∏—Ö)
        self.strategy_counter += 1
        if self.strategy_counter % 20 == 0 and len(self.performance_history) > 5:
            self.current_strategy = self._choose_strategy()
        
        # –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥—É –∑–≥—ñ–¥–Ω–æ –ø–æ—Ç–æ—á–Ω–æ—ó —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó
        reward = 0.0
        
        if self.current_strategy == "profit_focused":
            # –ê–∫—Ü–µ–Ω—Ç –Ω–∞ –ø—Ä–∏–±—É—Ç–æ–∫
            reward = total_return * 15.0  # –°–∏–ª—å–Ω–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑–∞ –ø—Ä–∏–±—É—Ç–æ–∫
            if max_drawdown > 0.15:  # –¢—ñ–ª—å–∫–∏ –∫—Ä–∏—Ç–∏—á–Ω–∞ –ø—Ä–æ—Å–∞–¥–∫–∞ –∫–∞—Ä–∞—î—Ç—å—Å—è
                reward -= max_drawdown * 5.0
                
        elif self.current_strategy == "risk_focused":
            # –ê–∫—Ü–µ–Ω—Ç –Ω–∞ –∫–æ–Ω—Ç—Ä–æ–ª—å —Ä–∏–∑–∏–∫—ñ–≤
            reward = total_return * 8.0  # –ü–æ–º—ñ—Ä–Ω–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑–∞ –ø—Ä–∏–±—É—Ç–æ–∫
            reward -= max_drawdown * 20.0  # –°–∏–ª—å–Ω–µ –ø–æ–∫–∞—Ä–∞–Ω–Ω—è –∑–∞ –ø—Ä–æ—Å–∞–¥–∫—É
            if win_rate > 0.7:  # –ë–æ–Ω—É—Å –∑–∞ –≤–∏—Å–æ–∫—É —è–∫—ñ—Å—Ç—å
                reward += win_rate * 3.0
                
        else:  # balanced
            # –ó–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω–∏–π –ø—ñ–¥—Ö—ñ–¥ –∑ trade-off –º—ñ–∂ —Ü—ñ–ª—è–º–∏
            profit_component = total_return * 12.0
            risk_component = -max_drawdown * 10.0
            quality_component = win_rate * 2.0 if total_trades > 0 else 0
            
            # –î–æ–∑–≤–æ–ª—è—î–º–æ –º–æ–¥–µ–ª—ñ –≤–∏–±—Ä–∞—Ç–∏ –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç
            if total_return > 0.05:  # –ü—Ä–∏ —Ö–æ—Ä–æ—à–æ–º—É –ø—Ä–∏–±—É—Ç–∫—É - –¥–æ–∑–≤–æ–ª—è—î–º–æ –±—ñ–ª—å—à—É –ø—Ä–æ—Å–∞–¥–∫—É
                risk_component *= 0.5
            elif max_drawdown < 0.03:  # –ü—Ä–∏ –Ω–∏–∑—å–∫—ñ–π –ø—Ä–æ—Å–∞–¥—Ü—ñ - –∑–∞–æ—Ö–æ—á—É—î–º–æ –±—ñ–ª—å—à–∏–π —Ä–∏–∑–∏–∫
                profit_component *= 1.5
                
            reward = profit_component + risk_component + quality_component
        
        return reward * self.weight
    
    def _choose_strategy(self) -> str:
        """–í–∏–±–∏—Ä–∞—î –Ω–∞–π–∫—Ä–∞—â—É —Å—Ç—Ä–∞—Ç–µ–≥—ñ—é –Ω–∞ –æ—Å–Ω–æ–≤—ñ –Ω–µ–¥–∞–≤–Ω—å–æ—ó –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ."""
        if len(self.performance_history) < 10:
            return "balanced"
        
        recent_performance = self.performance_history[-10:]
        
        # –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø: –ë–µ–∑–ø–µ—á–Ω–∏–π —Ä–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ —Å–µ—Ä–µ–¥–Ω—ñ—Ö –∑–Ω–∞—á–µ–Ω—å –∑ –ø–µ—Ä–µ–≤—ñ—Ä–∫–æ—é –Ω–∞ –ø—É—Å—Ç—ñ –º–∞—Å–∏–≤–∏
        returns = [p['return'] for p in recent_performance]
        drawdowns = [p['drawdown'] for p in recent_performance]
        win_rates = [p['win_rate'] for p in recent_performance if p['win_rate'] > 0]
        
        # –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ —Å–µ—Ä–µ–¥–Ω—ñ –∑ –ø–µ—Ä–µ–≤—ñ—Ä–∫–æ—é –Ω–∞ –ø—É—Å—Ç—ñ —Å–ø–∏—Å–∫–∏
        avg_return = np.mean(returns) if len(returns) > 0 else 0.0
        avg_drawdown = np.mean(drawdowns) if len(drawdowns) > 0 else 0.0
        avg_win_rate = np.mean(win_rates) if len(win_rates) > 0 else 0.0
        
        # –õ–æ–≥—ñ–∫–∞ –≤–∏–±–æ—Ä—É —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó
        if avg_return < -0.02 and avg_drawdown > 0.1:
            # –ü–æ–≥–∞–Ω–∞ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å - —Ñ–æ–∫—É—Å –Ω–∞ —Ä–∏–∑–∏–∫–∏
            print(f"üõ°Ô∏è STRATEGY: Switching to RISK_FOCUSED (return={avg_return:.2%}, dd={avg_drawdown:.2%})")
            return "risk_focused"
        elif avg_return > 0.05 and avg_drawdown < 0.08:
            # –•–æ—Ä–æ—à–∞ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å - —Ñ–æ–∫—É—Å –Ω–∞ –ø—Ä–∏–±—É—Ç–æ–∫
            print(f"üí∞ STRATEGY: Switching to PROFIT_FOCUSED (return={avg_return:.2%}, dd={avg_drawdown:.2%})")
            return "profit_focused"
        else:
            # –ó–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω–∏–π –ø—ñ–¥—Ö—ñ–¥
            print(f"‚öñÔ∏è STRATEGY: Staying BALANCED (return={avg_return:.2%}, dd={avg_drawdown:.2%})")
            return "balanced"
    
    def reset(self):
        self.performance_history = []
        self.current_strategy = "balanced"
        self.strategy_counter = 0


def create_optimized_reward_scheme() -> CompositeRewardScheme:
    """
    –ü–û–ö–†–ê–©–ï–ù–ê —Å—Ö–µ–º–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥ –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å–∏–≤–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è:
    - –û—Å–Ω–æ–≤–Ω–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑–∞ –ø—Ä–∏–±—É—Ç–∫–æ–≤—ñ—Å—Ç—å –∑ –ø—Ä–æ–≥—Ä–µ—Å–∏–≤–Ω–∏–º –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è–º
    - –ë–æ–Ω—É—Å–∏ –∑–∞ —Ç–æ—Ä–≥–æ–≤—É –∞–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å —Ç–∞ –µ–∫—Å–ø–ª–æ—Ä–∞—Ü—ñ—é
    - –ú'—è–∫—ñ —à—Ç—Ä–∞—Ñ–∏ –∑–∞ —Ä–∏–∑–∏–∫–∏ –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è –±–∞–ª–∞–Ω—Å—É
    - –í–∏–Ω–∞–≥–æ—Ä–æ–¥–∏ –∑–∞ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è —è–∫–æ—Å—Ç—ñ —Ç–æ—Ä–≥—ñ–≤–ª—ñ
    """
    schemes = [
        # –ì–û–õ–û–í–ù–ò–ô –ö–û–ú–ü–û–ù–ï–ù–¢: –ü—Ä–æ–≥—Ä–µ—Å–∏–≤–Ω–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑–∞ –ø—Ä–∏–±—É—Ç–∫–æ–≤—ñ—Å—Ç—å
        TotalReturnReward(weight=4.0),  # –ó–±—ñ–ª—å—à–µ–Ω–∞ –≤–∞–≥–∞ –¥–ª—è —Å–∏–ª—å–Ω–∏—Ö –ø–æ–∑–∏—Ç–∏–≤–Ω–∏—Ö —Å–∏–≥–Ω–∞–ª—ñ–≤
        
        # –ï–ö–°–ü–õ–û–†–ê–¶–Ü–Ø: –ó–∞–æ—Ö–æ—á–µ–Ω–Ω—è —Ç–æ—Ä–≥–æ–≤–æ—ó –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
        ExplorationReward(weight=2.0, target_trades_per_episode=25),  # –ê–∫—Ç–∏–≤–Ω–∞ —Ç–æ—Ä–≥—ñ–≤–ª—è
        
        # –Ø–ö–Ü–°–¢–¨: –í–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑–∞ —Ö–æ—Ä–æ—à—ñ —É–≥–æ–¥–∏
        TradeQualityReward(weight=1.5, min_trades=3),  # –ë–æ–Ω—É—Å –∑–∞ –≤–∏—Å–æ–∫–∏–π –≤–∏–Ω—Ä–µ–π—Ç
        
        # –ü–†–û–ì–†–ï–°: –í–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑–∞ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è –º—ñ–∂ –∫—Ä–æ–∫–∞–º–∏
        StepProfitReward(weight=1.0),  # –ó–∞–æ—Ö–æ—á–µ–Ω–Ω—è –ø–æ–∑–∏—Ç–∏–≤–Ω–∏—Ö –∑–º—ñ–Ω
        
        # –ö–û–ù–¢–†–û–õ–¨ –†–ò–ó–ò–ö–Ü–í: –ú'—è–∫–∏–π –∫–æ–Ω—Ç—Ä–æ–ª—å –ø—Ä–æ—Å–∞–¥–∫–∏ (–Ω–µ –±–ª–æ–∫—É—î –Ω–∞–≤—á–∞–Ω–Ω—è)
        DrawdownPenalty(weight=-1.0, max_drawdown_threshold=0.15),  # 15% –ø–æ—Ä—ñ–≥
        
        # –ë–ê–õ–ê–ù–°: –ú'—è–∫—ñ —à—Ç—Ä–∞—Ñ–∏ –∑–∞ –≤–µ–ª–∏–∫—ñ –≤—Ç—Ä–∞—Ç–∏ (–∑–∞–æ—Ö–æ—á—É—î –æ–±–µ—Ä–µ–∂–Ω—ñ—Å—Ç—å)
        LossTradesPenalty(weight=-1.5),  # –ü–æ–º—ñ—Ä–Ω—ñ —à—Ç—Ä–∞—Ñ–∏ –∑–∞ –∑–±–∏—Ç–∫–∏
    ]
    
    # –£–≤—ñ–º–∫–Ω—É—Ç–∏ –¥–∏–Ω–∞–º—ñ—á–Ω–µ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ –Ω–∞–≤—á–∞–Ω–Ω—è
    composite = CompositeRewardScheme(schemes, enable_dynamic_scaling=True)
    return composite


def create_bear_market_optimized_reward_scheme() -> CompositeRewardScheme:
    """
    –ê–ì–†–ï–°–ò–í–ù–û –û–ü–¢–ò–ú–Ü–ó–û–í–ê–ù–ê —Å—Ö–µ–º–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥ –¥–ª—è –º–∞–∫—Å–∏–º—ñ–∑–∞—Ü—ñ—ó –ø—Ä–∏–±—É—Ç–∫—É:
    - –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–Ü –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∏ –∑–∞ –ø—Ä–∏–±—É—Ç–æ–∫ –ø–æ—Ä—Ç—Ñ–µ–ª—è  
    - –ê–ì–†–ï–°–ò–í–ù–Ü —à—Ç—Ä–∞—Ñ–∏ –∑–∞ –∑–±–∏—Ç–∫–∏ —Ç–∞ –≤—Ç—Ä–∞—Ç–∏
    - –ñ–û–†–°–¢–ö–ò–ô –∫–æ–Ω—Ç—Ä–æ–ª—å –ø—Ä–æ—Å–∞–¥–∫–∏
    - –°–£–í–û–†–Ü –ø–æ–∫–∞—Ä–∞–Ω–Ω—è –∑–∞ –ø–æ–≥–∞–Ω—É —Ç–æ—Ä–≥—ñ–≤–ª—é
    """
    schemes = [
        # –ì–û–õ–û–í–ù–ò–ô –ö–û–ú–ü–û–ù–ï–ù–¢: –ê–≥—Ä–µ—Å–∏–≤–Ω–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑–∞ –ø—Ä–∏–±—É—Ç–æ–∫ –ø–æ—Ä—Ç—Ñ–µ–ª—è
        ProfitReward(weight=4.0, normalize=True),  # –ó–±—ñ–ª—å—à–µ–Ω–æ –¥–æ 4x –¥–ª—è –º–∞–∫—Å–∏–º—ñ–∑–∞—Ü—ñ—ó –ø—Ä–∏–±—É—Ç–∫—É
        
        # –ê–ì–†–ï–°–ò–í–ù–ò–ô –∫–æ–Ω—Ç—Ä–æ–ª—å –ø—Ä–æ—Å–∞–¥–∫–∏ –∑ –Ω–∏–∑—å–∫–∏–º –ø–æ—Ä–æ–≥–æ–º
        DrawdownPenalty(weight=-2.0, max_drawdown_threshold=0.15),  # –ó–ù–ò–ñ–ï–ù–û –ø–æ—Ä—ñ–≥ –¥–æ 15%, –∑–±—ñ–ª—å—à–µ–Ω–æ —à—Ç—Ä–∞—Ñ
        
        # –í–ò–ù–ê–ì–û–†–û–î–ê –∑–∞ —è–∫—ñ—Å—Ç—å —É–≥–æ–¥ (–≤–∏—Å–æ–∫–∏–π –≤–∏–Ω—Ä–µ–π—Ç) 
        TradeQualityReward(weight=1.0, min_trades=1),  # –ó–±—ñ–ª—å—à–µ–Ω–æ –≤–∞–≥—É —è–∫–æ—Å—Ç—ñ
        
        # –ê–ì–†–ï–°–ò–í–ù–Ü —à—Ç—Ä–∞—Ñ–∏ –∑–∞ –∑–±–∏—Ç–∫–æ–≤—É —Ç–æ—Ä–≥—ñ–≤–ª—é
        AggressiveLossPenalty(weight=-3.0),  # –ù–û–í–ò–ô –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –¥–ª—è –∂–æ—Ä—Å—Ç–∫–æ–≥–æ –ø–æ–∫–∞—Ä–∞–Ω–Ω—è –∑–±–∏—Ç–∫—ñ–≤
        
        # –í–ò–ü–†–ê–í–õ–ï–ù–ò–ô –ë–û–ù–£–° –∑–∞ –ø—Ä–∏–±—É—Ç–∫–æ–≤—É —Ç–æ—Ä–≥—ñ–≤–ª—é –∑ –ê–ì–†–ï–°–ò–í–ù–ò–ú–ò —à—Ç—Ä–∞—Ñ–∞–º–∏ –∑–∞ –≤—Ç—Ä–∞—Ç–∏
        BearMarketActivityReward(weight=2.0),  # –ó–±—ñ–ª—å—à–µ–Ω–æ –≤–∞–≥—É –∑ –∞–≥—Ä–µ—Å–∏–≤–Ω–∏–º–∏ —à—Ç—Ä–∞—Ñ–∞–º–∏
        
        # –í–ò–ü–†–ê–í–õ–ï–ù–ò–ô —Ç–∞–π–º–∏–Ω–≥ –∑ –ñ–û–†–°–¢–ö–ò–ú–ò —à—Ç—Ä–∞—Ñ–∞–º–∏ –∑–∞ –≤—Ç—Ä–∞—Ç–∏
        MarketTimingReward(weight=1.5),  # –ó–±—ñ–ª—å—à–µ–Ω–æ –≤–∞–≥—É –∑ –∞–≥—Ä–µ—Å–∏–≤–Ω–∏–º–∏ —à—Ç—Ä–∞—Ñ–∞–º–∏
    ]
    
    # –ë–µ–∑ –¥–∏–Ω–∞–º—ñ—á–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ—ó —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ –Ω–∞–≤—á–∞–Ω–Ω—è
    composite = CompositeRewardScheme(schemes, enable_dynamic_scaling=False)
    
    return composite


class AggressiveLossPenalty(BaseRewardScheme):
    """–ê–ì–†–ï–°–ò–í–ù–Ü —à—Ç—Ä–∞—Ñ–∏ –∑–∞ –∑–±–∏—Ç–∫–æ–≤—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ —Ç–æ—Ä–≥—ñ–≤–ª—ñ."""
    
    def __init__(self, weight: float = -3.0, loss_threshold: float = -0.01):
        super().__init__(weight)
        self.loss_threshold = loss_threshold  # -1% —è–∫ –ø–æ—Ä—ñ–≥ –¥–ª—è —à—Ç—Ä–∞—Ñ—ñ–≤
        self.last_portfolio_value = None
        self.consecutive_losses = 0
        
    def calculate(self, env_state: Dict) -> float:
        portfolio_value = env_state.get('portfolio_value', 10000)
        total_return = env_state.get('total_return', 0.0)
        total_trades = env_state.get('total_trades', 0)
        win_rate = env_state.get('win_rate', 0.0)
        
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è
        if self.last_portfolio_value is None:
            self.last_portfolio_value = portfolio_value
            return 0.0
            
        # –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ –∑–º—ñ–Ω—É –ø–æ—Ä—Ç—Ñ–µ–ª—è –∑ –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ –∫—Ä–æ–∫—É
        step_change = 0.0
        if self.last_portfolio_value > 0:
            step_change = (portfolio_value - self.last_portfolio_value) / self.last_portfolio_value
            
        penalty = 0.0
        
        # 1. –ê–ì–†–ï–°–ò–í–ù–Ü —à—Ç—Ä–∞—Ñ–∏ –∑–∞ –∫—Ä–æ–∫–æ–≤—ñ –≤—Ç—Ä–∞—Ç–∏
        if step_change < self.loss_threshold:  # –í—Ç—Ä–∞—Ç–∏ –±—ñ–ª—å—à–µ 1% –∑–∞ –∫—Ä–æ–∫
            step_penalty = abs(step_change) * 50.0  # 1% –≤—Ç—Ä–∞—Ç = -0.5 —à—Ç—Ä–∞—Ñ—É
            penalty += min(step_penalty, 5.0)  # –ú–∞–∫—Å–∏–º—É–º -5.0 –∑–∞ –∫—Ä–æ–∫
            self.consecutive_losses += 1
        else:
            self.consecutive_losses = 0
            
        # 2. –ï–°–ö–ê–õ–ê–¶–Ü–ô–ù–Ü —à—Ç—Ä–∞—Ñ–∏ –∑–∞ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ –≤—Ç—Ä–∞—Ç–∏
        if self.consecutive_losses > 5:
            escalation = self.consecutive_losses * 0.2  # –ó–±—ñ–ª—å—à—É—î–º–æ —à—Ç—Ä–∞—Ñ –∑–∞ –∫–æ–∂–Ω—É –ø–æ—Å–ª—ñ–¥–æ–≤–Ω—É –≤—Ç—Ä–∞—Ç—É
            penalty += min(escalation, 3.0)  # –ú–∞–∫—Å–∏–º—É–º +3.0 –¥–æ —à—Ç—Ä–∞—Ñ—É
            
        # 3. –ñ–û–†–°–¢–ö–Ü —à—Ç—Ä–∞—Ñ–∏ –∑–∞ –∑–∞–≥–∞–ª—å–Ω—É –∑–±–∏—Ç–∫–æ–≤—ñ—Å—Ç—å
        if total_return < -0.05 and total_trades > 10:  # –ó–±–∏—Ç–∫–∏ –±—ñ–ª—å—à–µ 5% –ø—Ä–∏ –∞–∫—Ç–∏–≤–Ω—ñ–π —Ç–æ—Ä–≥—ñ–≤–ª—ñ
            total_penalty = abs(total_return) * 30.0  # 1% –∑–±–∏—Ç–∫—ñ–≤ = -0.3 —à—Ç—Ä–∞—Ñ—É
            penalty += min(total_penalty, 8.0)  # –ú–∞–∫—Å–∏–º—É–º -8.0
            
        # 4. –î–û–î–ê–¢–ö–û–í–Ü —à—Ç—Ä–∞—Ñ–∏ –∑–∞ –Ω–∏–∑—å–∫–∏–π –≤–∏–Ω—Ä–µ–π—Ç –ø—Ä–∏ –∑–±–∏—Ç–∫–∞—Ö
        if total_return < -0.02 and win_rate < 0.4 and total_trades > 5:
            winrate_penalty = (0.4 - win_rate) * 10.0  # –®—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≥–∞–Ω–∏–π –≤–∏–Ω—Ä–µ–π—Ç
            penalty += min(winrate_penalty, 2.0)  # –ú–∞–∫—Å–∏–º—É–º -2.0
            
        # –û–Ω–æ–≤–ª—é—î–º–æ —Å—Ç–∞–Ω
        self.last_portfolio_value = portfolio_value
        
        # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ –Ω–µ–≥–∞—Ç–∏–≤–Ω—É –≤–∏–Ω–∞–≥–æ—Ä–æ–¥—É (—à—Ç—Ä–∞—Ñ)
        final_penalty = -penalty * abs(self.weight) if penalty > 0 else 0.0
        
        # –õ–æ–≥—É–≤–∞–Ω–Ω—è –∑–Ω–∞—á–Ω–∏—Ö —à—Ç—Ä–∞—Ñ—ñ–≤ –¥–ª—è –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥—É
        if final_penalty < -2.0:
            print(f"üí• AGGRESSIVE LOSS PENALTY: {final_penalty:.2f} (return: {total_return:.2%}, step_change: {step_change:.2%})")
            
        return final_penalty
        
    def reset(self):
        self.last_portfolio_value = None
        self.consecutive_losses = 0


class BearMarketActivityReward(BaseRewardScheme):
    """–í–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑–∞ –ø—Ä–∏–±—É—Ç–∫–æ–≤—É —Ç–æ—Ä–≥—ñ–≤–ª—é –ø—ñ–¥ —á–∞—Å –ø–∞–¥–∞—é—á–æ–≥–æ —Ä–∏–Ω–∫—É - –í–ò–ü–†–ê–í–õ–ï–ù–ê –õ–û–ì–Ü–ö–ê."""
    
    def __init__(self, weight: float = 1.0, market_decline_threshold: float = -0.05):
        super().__init__(weight)
        self.market_decline_threshold = market_decline_threshold  # -5% —Å–ø–∞–¥ –∑–∞ –ø–µ—Ä—ñ–æ–¥ –¥–ª—è –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –≤–µ–¥–º–µ–∂–æ–≥–æ —Ä–∏–Ω–∫—É
        self.price_history = []
        self.last_portfolio_value = None
        
    def calculate(self, env_state: Dict) -> float:
        current_price = env_state.get('current_price', 0)
        total_trades = env_state.get('total_trades', 0)
        portfolio_value = env_state.get('portfolio_value', 0)
        
        if current_price <= 0:
            return 0.0
            
        # –í–µ–¥–µ–º–æ —ñ—Å—Ç–æ—Ä—ñ—é —Ü—ñ–Ω –¥–ª—è –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Ä–∏–Ω–∫–æ–≤–æ–≥–æ —Ç—Ä–µ–Ω–¥—É
        self.price_history.append(current_price)
        if len(self.price_history) > 20:  # –û—Å—Ç–∞–Ω–Ω—ñ 20 –∫—Ä–æ–∫—ñ–≤
            self.price_history = self.price_history[-20:]
            
        if len(self.price_history) < 10:
            # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î–º–æ –ø–æ—á–∞—Ç–∫–æ–≤–µ –∑–Ω–∞—á–µ–Ω–Ω—è –ø–æ—Ä—Ç—Ñ–µ–ª—è
            if self.last_portfolio_value is None:
                self.last_portfolio_value = portfolio_value
            return 0.0
            
        # –í–∏–∑–Ω–∞—á–∞—î–º–æ —á–∏ —Ä–∏–Ω–æ–∫ –ø–∞–¥–∞—î (–≤–µ–¥–º–µ–∂–∏–π —Ç—Ä–µ–Ω–¥)
        price_change = (self.price_history[-1] - self.price_history[0]) / self.price_history[0]
        is_bear_market = price_change < self.market_decline_threshold
        
        # –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ –∑–º—ñ–Ω—É –ø–æ—Ä—Ç—Ñ–µ–ª—è –∑ –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ –∫—Ä–æ–∫—É
        portfolio_change = 0.0
        if self.last_portfolio_value is not None and self.last_portfolio_value > 0:
            portfolio_change = portfolio_value - self.last_portfolio_value
        
        reward = 0.0
        
        if is_bear_market:
            # –ê–ì–†–ï–°–ò–í–ù–û –ó–ë–Ü–õ–¨–®–ï–ù–Ü –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∏ –∑–∞ –ø—Ä–∏–±—É—Ç–æ–∫ –ø–æ—Ä—Ç—Ñ–µ–ª—è
            if portfolio_change > 5:  # –ó–ù–ò–ñ–ï–ù–û –ø–æ—Ä—ñ–≥ –¥–æ $5 –¥–ª—è –∑–∞–æ—Ö–æ—á–µ–Ω–Ω—è
                # –í–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –ø—Ä–æ–ø–æ—Ä—Ü—ñ–π–Ω–∞ –†–ï–ê–õ–¨–ù–û–ú–£ –ø—Ä–∏–±—É—Ç–∫—É
                profit_percentage = portfolio_change / self.last_portfolio_value
                # –ó–ë–Ü–õ–¨–®–ï–ù–û –º–Ω–æ–∂–Ω–∏–∫–∏ –¥–ª—è –º–∞–∫—Å–∏–º—ñ–∑–∞—Ü—ñ—ó –ø—Ä–∏–±—É—Ç–∫—É
                profit_bonus = profit_percentage * 25.0  # 1% –ø—Ä–∏–±—É—Ç–∫—É = +0.25 –±–æ–Ω—É—Å—É
                reward += min(profit_bonus, 5.0)  # –ó–ë–Ü–õ–¨–®–ï–ù–û –º–∞–∫—Å–∏–º—É–º –¥–æ +5.0
                    
            # –ö–†–ò–¢–ò–ß–ù–û –ó–ë–Ü–õ–¨–®–ï–ù–Ü —à—Ç—Ä–∞—Ñ–∏ –∑–∞ –≤—Ç—Ä–∞—Ç–∏ –ø–æ—Ä—Ç—Ñ–µ–ª—è  
            elif portfolio_change < -5:  # –ó–ù–ò–ñ–ï–ù–û –ø–æ—Ä—ñ–≥ –¥–æ $5 –¥–ª—è –∂–æ—Ä—Å—Ç–∫–æ—Å—Ç—ñ
                loss_percentage = abs(portfolio_change) / self.last_portfolio_value
                # –ê–ì–†–ï–°–ò–í–ù–û –ó–ë–Ü–õ–¨–®–ï–ù–û —à—Ç—Ä–∞—Ñ–∏ –∑–∞ –≤—Ç—Ä–∞—Ç–∏
                loss_penalty = -loss_percentage * 60.0  # 1% –≤—Ç—Ä–∞—Ç = -0.6 —à—Ç—Ä–∞—Ñ—É (–±—É–ª–æ -0.15)
                reward += max(loss_penalty, -10.0)  # –ó–ë–Ü–õ–¨–®–ï–ù–û –º–∞–∫—Å–∏–º—É–º —à—Ç—Ä–∞—Ñ—É –¥–æ -10.0
            
            # –í–ò–ú–ö–ù–ï–ù–û: –ù–ï–ü–†–ê–í–ò–õ–¨–ù–ê –ª–æ–≥—ñ–∫–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥ –∑–∞ –ø–∞–¥—ñ–Ω–Ω—è —Ü—ñ–Ω
            # –ù–ï –î–û–î–ê–Ñ–ú–û –±–æ–Ω—É—Å–∏ –∑–∞ —Å–∞–º–æ –ø–∞–¥—ñ–Ω–Ω—è —Ä–∏–Ω–∫—É!
            # –ê–≥–µ–Ω—Ç –º–∞—î –æ—Ç—Ä–∏–º—É–≤–∞—Ç–∏ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∏ —Ç—ñ–ª—å–∫–∏ –∑–∞ –ø—Ä–∏–±—É—Ç–æ–∫ –ø–æ—Ä—Ç—Ñ–µ–ª—è!
            
        # –û–Ω–æ–≤–ª—é—î–º–æ —ñ—Å—Ç–æ—Ä—ñ—é –ø–æ—Ä—Ç—Ñ–µ–ª—è
        self.last_portfolio_value = portfolio_value
        
        return reward * self.weight
        
    def reset(self):
        self.price_history = []


class MarketTimingReward(BaseRewardScheme):
    """–í–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–∏–π —Ç–∞–π–º–∏–Ω–≥ –æ–ø–µ—Ä–∞—Ü—ñ–π - –í–ò–ü–†–ê–í–õ–ï–ù–ê –õ–û–ì–Ü–ö–ê."""
    
    def __init__(self, weight: float = 0.8):
        super().__init__(weight)
        self.last_action = None
        self.last_price = None
        self.last_portfolio_value = None
        
    def calculate(self, env_state: Dict) -> float:
        current_price = env_state.get('current_price', 0)
        current_action = env_state.get('last_action', 0)  # 0=—É—Ç—Ä–∏–º—É–≤–∞—Ç–∏, 1=–∫—É–ø—É–≤–∞—Ç–∏, 2=–ø—Ä–æ–¥–∞–≤–∞—Ç–∏
        portfolio_value = env_state.get('portfolio_value', 10000)
        
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –ø—Ä–∏ –ø–µ—Ä—à–æ–º—É –≤–∏–∫–ª–∏–∫—É
        if current_price <= 0 or self.last_price is None:
            self.last_price = current_price
            self.last_action = current_action
            self.last_portfolio_value = portfolio_value
            return 0.0
            
        # –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ –∑–º—ñ–Ω—É —Ü—ñ–Ω–∏ —Ç–∞ –ø–æ—Ä—Ç—Ñ–µ–ª—è
        price_change = (current_price - self.last_price) / self.last_price
        portfolio_change = 0.0
        if self.last_portfolio_value is not None and self.last_portfolio_value > 0:
            portfolio_change = portfolio_value - self.last_portfolio_value
        
        reward = 0.0
        
        # –ê–ì–†–ï–°–ò–í–ù–û –ó–ë–Ü–õ–¨–®–ï–ù–ê –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑–∞ –∑–±—ñ–ª—å—à–µ–Ω–Ω—è –ø–æ—Ä—Ç—Ñ–µ–ª—è
        if portfolio_change > 3:  # –ó–ù–ò–ñ–ï–ù–û –ø–æ—Ä—ñ–≥ –¥–æ $3 –¥–ª—è –∑–∞–æ—Ö–æ—á–µ–Ω–Ω—è
            # –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ –≤—ñ–¥—Å–æ—Ç–æ–∫ –ø—Ä–∏–±—É—Ç–∫—É
            profit_percentage = portfolio_change / self.last_portfolio_value
            
            # –ó–ë–Ü–õ–¨–®–ï–ù–û –±–∞–∑–æ–≤–∏–π –±–æ–Ω—É—Å –∑–∞ –ø—Ä–∏–±—É—Ç–æ–∫
            base_profit_bonus = profit_percentage * 20.0  # 1% –ø—Ä–∏–±—É—Ç–∫—É = +0.2 –±–æ–Ω—É—Å—É (–±—É–ª–æ +0.05)
            reward += min(base_profit_bonus, 4.0)  # –ó–ë–Ü–õ–¨–®–ï–ù–û –º–∞–∫—Å–∏–º—É–º –¥–æ +4.0
            
        # –ö–†–ò–¢–ò–ß–ù–û –ó–ë–Ü–õ–¨–®–ï–ù–Ü —à—Ç—Ä–∞—Ñ–∏ –∑–∞ –≤—Ç—Ä–∞—Ç–∏
        elif portfolio_change < -2:  # –ó–ù–ò–ñ–ï–ù–û –ø–æ—Ä—ñ–≥ –¥–æ $2 –¥–ª—è –∂–æ—Ä—Å—Ç–∫–æ—Å—Ç—ñ
            loss_percentage = abs(portfolio_change) / self.last_portfolio_value
            # –ê–ì–†–ï–°–ò–í–ù–û –ó–ë–Ü–õ–¨–®–ï–ù–û —à—Ç—Ä–∞—Ñ–∏ –∑–∞ –≤—Ç—Ä–∞—Ç–∏
            loss_penalty = -loss_percentage * 80.0  # 1% –≤—Ç—Ä–∞—Ç = -0.8 —à—Ç—Ä–∞—Ñ—É (–±—É–ª–æ -0.2)
            reward += max(loss_penalty, -8.0)  # –ó–ë–Ü–õ–¨–®–ï–ù–û –º–∞–∫—Å–∏–º—É–º —à—Ç—Ä–∞—Ñ—É –¥–æ -8.0
        
        # –í–ò–ú–ö–ù–ï–ù–û: –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞ –ª–æ–≥—ñ–∫–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥ –∑–∞ –ø–∞–¥—ñ–Ω–Ω—è —Ü—ñ–Ω
        # –ê–≥–µ–Ω—Ç –ù–ï –º–∞—î –æ—Ç—Ä–∏–º—É–≤–∞—Ç–∏ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∏ –∑–∞ —Å–∞–º–µ –ø–∞–¥—ñ–Ω–Ω—è —Ä–∏–Ω–∫—É!
        # –í–∏–Ω–∞–≥–æ—Ä–æ–¥–∏ –º–∞—é—Ç—å –±–∞–∑—É–≤–∞—Ç–∏—Å—è –¢–Ü–õ–¨–ö–ò –Ω–∞ –ø—Ä–∏–±—É—Ç–∫–æ–≤–æ—Å—Ç—ñ –ø–æ—Ä—Ç—Ñ–µ–ª—è!
        
        # –û–Ω–æ–≤–ª—é—î–º–æ —Å—Ç–∞–Ω –¥–ª—è –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ –∫—Ä–æ–∫—É
        self.last_price = current_price
        self.last_action = current_action
        self.last_portfolio_value = portfolio_value
        
        return reward * self.weight
        
    def reset(self):
        """–°–∫–∏–¥–∞–Ω–Ω—è —Å—Ç–∞–Ω—É —Å—Ö–µ–º–∏ –ø—Ä–∏ –ø–æ—á–∞—Ç–∫—É –Ω–æ–≤–æ–≥–æ –µ–ø—ñ–∑–æ–¥—É."""
        self.last_action = None
        self.last_price = None
        self.last_portfolio_value = None


def create_static_reward_scheme(initial_balance: float = 10000.0) -> StaticReward:
    """–°—Ç–≤–æ—Ä–∏—Ç–∏ –ø—Ä–æ—Å—Ç—É —Å—Ç–∞—Ç–∏—á–Ω—É —Å—Ö–µ–º—É –≤–∏–Ω–∞–≥–æ—Ä–æ–¥ –∑ —Ñ—ñ–∫—Å–æ–≤–∞–Ω–∏–º –ø–æ—á–∞—Ç–∫–æ–≤–∏–º –±–∞–ª–∞–Ω—Å–æ–º."""
    return StaticReward(weight=1.0, static_initial_balance=initial_balance)


def create_market_optimized_reward_scheme() -> CompositeRewardScheme:
    """
    –ó–ë–ê–õ–ê–ù–°–û–í–ê–ù–ê —Å—Ö–µ–º–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥ –¥–ª—è –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è:
    - –ß—ñ—Ç–∫—ñ –ø–æ–∑–∏—Ç–∏–≤–Ω—ñ —Å–∏–≥–Ω–∞–ª–∏ –∑–∞ –ø—Ä–∏–±—É—Ç–æ–∫
    - –ü–æ–º—ñ—Ä–Ω—ñ —à—Ç—Ä–∞—Ñ–∏ –∑–∞ –∑–±–∏—Ç–∫–∏ 
    - –ó–∞–æ—Ö–æ—á–µ–Ω–Ω—è –µ–∫—Å–ø–ª–æ—Ä–∞—Ü—ñ—ó —Ç–∞ –Ω–∞–≤—á–∞–Ω–Ω—è
    """
    schemes = [
        # –ì–û–õ–û–í–ù–ò–ô –ö–û–ú–ü–û–ù–ï–ù–¢: –ó–∞–≥–∞–ª—å–Ω–∞ –ø—Ä–∏–±—É—Ç–∫–æ–≤—ñ—Å—Ç—å (–≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–∞ –≤–∞–≥–∞)
        TotalReturnReward(weight=5.0),  # –ó–ë–Ü–õ–¨–®–ï–ù–û –¥–ª—è —Å–∏–ª—å–Ω–∏—Ö –ø–æ–∑–∏—Ç–∏–≤–Ω–∏—Ö —Å–∏–≥–Ω–∞–ª—ñ–≤
        
        # –ü–û–ú–Ü–†–ù–Ü —à—Ç—Ä–∞—Ñ–∏ –∑–∞ –∑–±–∏—Ç–∫–æ–≤—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
        LossTradesPenalty(weight=-2.0),  # –ó–ú–ï–ù–®–ï–ù–û –¥–ª—è –∫—Ä–∞—â–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è
        
        # –ú'–Ø–ö–Ü –®–¢–†–ê–§–ò –∑–∞ –Ω–∏–∑—å–∫–∏–π –≤–∏–Ω—Ä–µ–π—Ç –ø—Ä–∏ –∞–∫—Ç–∏–≤–Ω—ñ–π —Ç–æ—Ä–≥—ñ–≤–ª—ñ  
        WinRatePenalty(weight=-1.0),  # –ó–ú–ï–ù–®–ï–ù–û –¥–ª—è –∑–∞–æ—Ö–æ—á–µ–Ω–Ω—è –µ–∫—Å–ø–ª–æ—Ä–∞—Ü—ñ—ó
        
        # –¢–Ü–õ–¨–ö–ò –∑–∞ –∫—Ä–∏—Ç–∏—á–Ω—É –ø—Ä–æ—Å–∞–¥–∫—É
        DrawdownPenalty(weight=-0.5, max_drawdown_threshold=0.20),  # –ó–ë–Ü–õ–¨–®–ï–ù–û –ø–æ—Ä—ñ–≥ –¥–æ 20%
        
        # –°–ò–õ–¨–ù–ê –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑–∞ —è–∫—ñ—Å—Ç—å —É–≥–æ–¥
        TradeQualityReward(weight=2.0, min_trades=1),  # –ó–ë–Ü–õ–¨–®–ï–ù–û –¥–ª—è –∑–∞–æ—Ö–æ—á–µ–Ω–Ω—è
        
        # –ü–û–ó–ò–¢–ò–í–ù–ê –≤–∞–≥–∞ –¥–ª—è –∑–∞–æ—Ö–æ—á–µ–Ω–Ω—è –ø—Ä–∏–±—É—Ç–∫–æ–≤–æ—Å—Ç—ñ
        StepProfitReward(weight=1.0),  # –ü–û–í–ï–†–ù–ï–ù–û –ø–æ–∑–∏—Ç–∏–≤–Ω—É –≤–∞–≥—É
        
        # –ù–û–í–ò–ô: –í–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑–∞ –µ–∫—Å–ø–ª–æ—Ä–∞—Ü—ñ—é (—Ç–æ—Ä–≥–æ–≤—É –∞–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å)
        ExplorationReward(weight=0.5),  # –ó–∞–æ—Ö–æ—á—É—î–º–æ —Ç–æ—Ä–≥–æ–≤—É –∞–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å
    ]
    
    composite = CompositeRewardScheme(schemes, enable_dynamic_scaling=False)
    return composite


class CumulativeGrowthReward(BaseRewardScheme):
    """–í–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑–∞ cumulative –∑—Ä–æ—Å—Ç–∞–Ω–Ω—è –ø–æ—Ä—Ç—Ñ–µ–ª—è —á–µ—Ä–µ–∑ –µ–ø—ñ–∑–æ–¥–∏."""
    
    def __init__(self, weight: float = 4.0):
        super().__init__(weight)
        self.initial_balance = None
        
    def calculate(self, env_state: Dict) -> float:
        if self.initial_balance is None:
            self.initial_balance = env_state.get('initial_balance', 10000.0)
        
        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ cumulative return —è–∫—â–æ –¥–æ—Å—Ç—É–ø–Ω–∏–π, —ñ–Ω–∞–∫—à–µ episodic
        cumulative_return = env_state.get('cumulative_return', env_state.get('total_return', 0.0))
        portfolio_value = env_state.get('portfolio_value', 10000.0)
        
        # üí∞ –ó–ë–ê–õ–ê–ù–°–û–í–ê–ù–Ü –í–ò–ù–ê–ì–û–†–û–î–ò –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è
        if cumulative_return > 0.2:  # >20% –ø—Ä–∏–±—É—Ç–æ–∫ = –í–ï–õ–ò–ö–ê –í–ò–ù–ê–ì–û–†–û–î–ê
            reward = (cumulative_return * 10.0 + 3.0) * self.weight  # 20% = +5.0 –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞
        elif cumulative_return > 0.1:  # >10% –ø—Ä–∏–±—É—Ç–æ–∫ = –•–û–†–û–®–ê –í–ò–ù–ê–ì–û–†–û–î–ê
            reward = (cumulative_return * 8.0 + 1.0) * self.weight   # 10% = +1.8 –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞
        elif cumulative_return > 0.05:  # >5% –ø—Ä–∏–±—É—Ç–æ–∫ = –ü–û–ú–Ü–†–ù–ê –í–ò–ù–ê–ì–û–†–û–î–ê
            reward = cumulative_return * 6.0 * self.weight           # 5% = +0.3 –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞
        elif cumulative_return > 0:  # –ë—É–¥—å-—è–∫–∏–π –ø—Ä–∏–±—É—Ç–æ–∫ = –ü–û–ó–ò–¢–ò–í–ù–ê –í–ò–ù–ê–ì–û–†–û–î–ê
            reward = cumulative_return * 4.0 * self.weight           # 1% = +0.04 –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞
        else:  # üîª –ü–û–ú–Ü–†–ù–Ü –®–¢–†–ê–§–ò –ó–ê –ó–ë–ò–¢–ö–ò
            if cumulative_return < -0.2:  # >20% –∑–±–∏—Ç–∫—ñ–≤ = –ö–†–ò–¢–ò–ß–ù–ò–ô –®–¢–†–ê–§
                reward = cumulative_return * 8.0 * self.weight       # -20% = -1.6 —à—Ç—Ä–∞—Ñ
            elif cumulative_return < -0.1:  # >10% –∑–±–∏—Ç–∫—ñ–≤ = –í–ò–°–û–ö–ò–ô –®–¢–†–ê–§
                reward = cumulative_return * 6.0 * self.weight       # -10% = -0.6 —à—Ç—Ä–∞—Ñ
            else:  # <10% –∑–±–∏—Ç–∫—ñ–≤ = –ü–û–ú–Ü–†–ù–ò–ô –®–¢–†–ê–§
                reward = cumulative_return * 4.0 * self.weight       # -5% = -0.2 —à—Ç—Ä–∞—Ñ
            
        # –õ–æ–≥—É–≤–∞–Ω–Ω—è –∫–æ–∂–Ω—ñ 100 –∫—Ä–æ–∫—ñ–≤
        step = env_state.get('step', 0)
        if step % 100 == 0:
            print(f"üí∞ CUMULATIVE GROWTH: return={cumulative_return:+.1%} ‚Üí reward={reward:.2f} (portfolio=${portfolio_value:.0f})")
            
        return np.clip(reward, -10.0, 15.0)  # –ó–ë–ê–õ–ê–ù–°–û–í–ê–ù–ò–ô –¥—ñ–∞–ø–∞–∑–æ–Ω –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è
    
    def reset(self):
        # –ù–ï —Å–∫–∏–¥–∞—î–º–æ initial_balance –¥–ª—è cumulative tracking
        pass


class CumulativeDrawdownPenalty(BaseRewardScheme):
    """–®—Ç—Ä–∞—Ñ –∑–∞ cumulative –ø—Ä–æ—Å–∞–¥–∫—É –ø–æ—Ä—Ç—Ñ–µ–ª—è."""
    
    def __init__(self, weight: float = -4.0, max_cumulative_drawdown: float = 0.20):
        super().__init__(weight)
        self.max_cumulative_drawdown = max_cumulative_drawdown
        
    def calculate(self, env_state: Dict) -> float:
        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ cumulative drawdown —è–∫—â–æ –¥–æ—Å—Ç—É–ø–Ω–∏–π
        cumulative_drawdown = env_state.get('cumulative_drawdown', env_state.get('max_drawdown', 0.0))
        
        if cumulative_drawdown > self.max_cumulative_drawdown:
            # –ï–∫—Å–ø–æ–Ω–µ–Ω—Ü—ñ–π–Ω–∏–π —à—Ç—Ä–∞—Ñ –∑–∞ –ø–µ—Ä–µ–≤–∏—â–µ–Ω–Ω—è cumulative –ø—Ä–æ—Å–∞–¥–∫–∏
            excess_drawdown = cumulative_drawdown - self.max_cumulative_drawdown
            penalty = np.exp(excess_drawdown * 15) - 1  # –ê–≥—Ä–µ—Å–∏–≤–Ω–∏–π —à—Ç—Ä–∞—Ñ
            return -penalty * abs(self.weight)
        elif cumulative_drawdown > self.max_cumulative_drawdown * 0.7:  # 14% –ø—Ä–∏ –ª—ñ–º—ñ—Ç—ñ 20%
            # –ü–æ–ø–µ—Ä–µ–¥–∂—É–≤–∞–ª—å–Ω–∏–π —à—Ç—Ä–∞—Ñ
            warning_penalty = (cumulative_drawdown - self.max_cumulative_drawdown * 0.7) * 10
            return -warning_penalty * abs(self.weight)
        
        return 0.0


class CapitalPreservationReward(BaseRewardScheme):
    """–í–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ç–∞ –ø—Ä–∏–º–Ω–æ–∂–µ–Ω–Ω—è –∫–∞–ø—ñ—Ç–∞–ª—É."""
    
    def __init__(self, weight: float = 2.0):
        super().__init__(weight)
        self.last_portfolio_value = None
        
    def calculate(self, env_state: Dict) -> float:
        portfolio_value = env_state.get('portfolio_value', 10000.0)
        initial_balance = env_state.get('initial_balance', 10000.0)
        
        # –í–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∫–∞–ø—ñ—Ç–∞–ª—É –≤–∏—â–µ –ø–æ—á–∞—Ç–∫–æ–≤–æ–≥–æ —Ä—ñ–≤–Ω—è
        if portfolio_value > initial_balance:
            preservation_ratio = portfolio_value / initial_balance
            if preservation_ratio > 1.5:  # >150% –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è
                reward = 3.0 * self.weight
            elif preservation_ratio > 1.2:  # >120% –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è
                reward = 2.0 * self.weight
            elif preservation_ratio > 1.1:  # >110% –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è
                reward = 1.0 * self.weight
            else:  # >100% –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è
                reward = 0.5 * self.weight
        elif portfolio_value > initial_balance * 0.9:  # >90% –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è
            reward = 0.1 * self.weight
        else:  # <90% –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è - —à—Ç—Ä–∞—Ñ
            loss_ratio = (initial_balance - portfolio_value) / initial_balance
            reward = -loss_ratio * 5.0 * abs(self.weight)
            
        return np.clip(reward, -5.0, 10.0)


class PortfolioVolatilityPenalty(BaseRewardScheme):
    """–®—Ç—Ä–∞—Ñ –∑–∞ –≤–æ–ª–∞—Ç—ñ–ª—å–Ω—ñ—Å—Ç—å cumulative –ø–æ—Ä—Ç—Ñ–µ–ª—è."""
    
    def __init__(self, weight: float = -1.0, window: int = 10):
        super().__init__(weight)
        self.window = window
        
    def calculate(self, env_state: Dict) -> float:
        cumulative_history = env_state.get('cumulative_portfolio_history', [])
        portfolio_value = env_state.get('portfolio_value', 10000.0)
        
        if len(cumulative_history) < self.window:
            return 0.0
        
        # –ê–Ω–∞–ª—ñ–∑—É—î–º–æ –≤–æ–ª–∞—Ç—ñ–ª—å–Ω—ñ—Å—Ç—å –æ—Å—Ç–∞–Ω–Ω—ñ—Ö –∑–Ω–∞—á–µ–Ω—å –ø–æ—Ä—Ç—Ñ–µ–ª—è
        recent_values = cumulative_history[-self.window:] + [portfolio_value]
        returns = np.diff(recent_values) / recent_values[:-1]
        volatility = np.std(returns) if len(returns) > 1 else 0.0
        
        # –®—Ç—Ä–∞—Ñ –∑–∞ –≤–∏—Å–æ–∫—É –≤–æ–ª–∞—Ç—ñ–ª—å–Ω—ñ—Å—Ç—å
        if volatility > 0.1:  # >10% –≤–æ–ª–∞—Ç—ñ–ª—å–Ω—ñ—Å—Ç—å –º—ñ–∂ –µ–ø—ñ–∑–æ–¥–∞–º–∏
            penalty = volatility * 20.0 * abs(self.weight)
            return -min(penalty, 5.0)
        
        return 0.0


class ConsistentGrowthReward(BaseRewardScheme):
    """–í–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑–∞ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–µ –∑—Ä–æ—Å—Ç–∞–Ω–Ω—è –ø–æ—Ä—Ç—Ñ–µ–ª—è."""
    
    def __init__(self, weight: float = 1.5, window: int = 5):
        super().__init__(weight)
        self.window = window
        
    def calculate(self, env_state: Dict) -> float:
        cumulative_history = env_state.get('cumulative_portfolio_history', [])
        portfolio_value = env_state.get('portfolio_value', 10000.0)
        
        if len(cumulative_history) < self.window:
            return 0.0
        
        # –ê–Ω–∞–ª—ñ–∑—É—î–º–æ —Ç—Ä–µ–Ω–¥ –∑—Ä–æ—Å—Ç–∞–Ω–Ω—è
        recent_values = cumulative_history[-self.window:] + [portfolio_value]
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —î –∑–∞–≥–∞–ª—å–Ω–∏–π upward trend
        positive_changes = 0
        total_changes = len(recent_values) - 1
        
        for i in range(1, len(recent_values)):
            if recent_values[i] > recent_values[i-1]:
                positive_changes += 1
        
        consistency_ratio = positive_changes / total_changes if total_changes > 0 else 0
        
        # –í–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑–∞ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ñ—Å—Ç—å
        if consistency_ratio >= 0.8:  # 80%+ –ø–æ–∑–∏—Ç–∏–≤–Ω–∏—Ö –∑–º—ñ–Ω
            return 2.0 * self.weight
        elif consistency_ratio >= 0.6:  # 60%+ –ø–æ–∑–∏—Ç–∏–≤–Ω–∏—Ö –∑–º—ñ–Ω
            return 1.0 * self.weight
        elif consistency_ratio >= 0.4:  # 40%+ –ø–æ–∑–∏—Ç–∏–≤–Ω–∏—Ö –∑–º—ñ–Ω
            return 0.5 * self.weight
        
        return 0.0


def create_cumulative_growth_reward_scheme() -> CompositeRewardScheme:
    """
    –ù–û–í–ê –°–•–ï–ú–ê –¥–ª—è –ø–æ—Ä—Ç—Ñ–µ–ª—å–Ω–æ—ó –∫–æ–Ω—Ç–∏–Ω—É–∞–ª—å–Ω–æ—Å—Ç—ñ:
    - –§–æ–∫—É—Å –Ω–∞ CUMULATIVE –∑—Ä–æ—Å—Ç–∞–Ω–Ω—è –ø–æ—Ä—Ç—Ñ–µ–ª—è —á–µ—Ä–µ–∑ –µ–ø—ñ–∑–æ–¥–∏
    - –í–∏–Ω–∞–≥–æ—Ä–æ–¥–∏ –∑–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ç–∞ –ø—Ä–∏–º–Ω–æ–∂–µ–Ω–Ω—è –∫–∞–ø—ñ—Ç–∞–ª—É
    - –ñ–æ—Ä—Å—Ç–∫–∏–π –∫–æ–Ω—Ç—Ä–æ–ª—å cumulative –ø—Ä–æ—Å–∞–¥–∫–∏
    - –ó–∞–æ—Ö–æ—á–µ–Ω–Ω—è –¥–æ–≤–≥–æ—Å—Ç—Ä–æ–∫–æ–≤–æ–≥–æ –º–∏—Å–ª–µ–Ω–Ω—è
    """
    schemes = [
        # –°–ü–†–û–©–ï–ù–ê —Å—Ö–µ–º–∞ –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è
        CumulativeGrowthReward(weight=2.0),  # –û—Å–Ω–æ–≤–Ω–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑–∞ –∑—Ä–æ—Å—Ç–∞–Ω–Ω—è
        
        # –ü–æ–º—ñ—Ä–Ω–∏–π –∫–æ–Ω—Ç—Ä–æ–ª—å –ø—Ä–æ—Å–∞–¥–∫–∏
        CumulativeDrawdownPenalty(weight=-1.0, max_cumulative_drawdown=0.25),  # 25% –º–∞–∫—Å–∏–º—É–º
        
        # –í–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑–∞ —Ç–æ—Ä–≥–æ–≤—É –∞–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å
        ExplorationReward(weight=1.0, target_trades_per_episode=20),  # –ó–∞–æ—Ö–æ—á–µ–Ω–Ω—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
    ]
    
    composite = CompositeRewardScheme(schemes, enable_dynamic_scaling=True)
    return composite


def create_risk_adjusted_reward_scheme() -> CompositeRewardScheme:
    """
    –†–ò–ó–ò–ö-–ó–ë–ê–õ–ê–ù–°–û–í–ê–ù–ê —Å—Ö–µ–º–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥ –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è:
    - –°–∏–ª—å–Ω–∏–π –∞–∫—Ü–µ–Ω—Ç –Ω–∞ –∫–æ–Ω—Ç—Ä–æ–ª—å —Ä–∏–∑–∏–∫—ñ–≤ —Ç–∞ –ø—Ä–æ—Å–∞–¥–∫–∏
    - –ü–æ–º—ñ—Ä–Ω—ñ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∏ –∑–∞ –ø—Ä–∏–±—É—Ç–æ–∫ –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º —Ä–∏–∑–∏–∫—É
    - –ó–∞–æ—Ö–æ—á–µ–Ω–Ω—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—ñ —Ç–∞ —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ
    - –ó–∞–ø–æ–±—ñ–≥–∞–Ω–Ω—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω—ñ–π –ø–æ–≤–µ–¥—ñ–Ω—Ü—ñ
    """
    schemes = [
        # –ì–û–õ–û–í–ù–ò–ô –ö–û–ú–ü–û–ù–ï–ù–¢: –ó–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑–∞ –ø—Ä–∏–±—É—Ç–æ–∫
        TotalReturnReward(weight=3.0),  # –ó–ë–Ü–õ–¨–®–ï–ù–û –¥–ª—è –∫—Ä–∞—â–∏—Ö –ø–æ–∑–∏—Ç–∏–≤–Ω–∏—Ö —Å–∏–≥–Ω–∞–ª—ñ–≤
        
        # –ñ–û–†–°–¢–ö–ò–ô –∫–æ–Ω—Ç—Ä–æ–ª—å –ø—Ä–æ—Å–∞–¥–∫–∏ –¥–ª—è –¥–æ—Å—è–≥–Ω–µ–Ω–Ω—è —Ü—ñ–ª—å–æ–≤–∏—Ö 20%
        DrawdownPenalty(weight=-3.0, max_drawdown_threshold=0.08),  # –ó–ë–Ü–õ–¨–®–ï–ù–û —à—Ç—Ä–∞—Ñ, –ó–ú–ï–ù–®–ï–ù–û –ø–æ—Ä—ñ–≥ –¥–æ 8%
        
        # –í–ò–ù–ê–ì–û–†–û–î–ê –∑–∞ –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç –®–∞—Ä–ø–∞ (—Ä–∏–∑–∏–∫-—Å–∫–æ—Ä–µ–≥–æ–≤–∞–Ω–∞ –¥–æ—Ö–æ–¥–Ω—ñ—Å—Ç—å)
        SharpeRatioReward(weight=1.0, window=50),  # –ó–ú–ï–ù–®–ï–ù–û –≤–∞–≥—É, –ó–ë–Ü–õ–¨–®–ï–ù–û –≤—ñ–∫–Ω–æ –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ
        
        # –ú'–Ø–ö–ê –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑–∞ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ñ—Å—Ç—å
        ConsistencyReward(weight=0.5, window=30),  # –ó–ú–ï–ù–®–ï–ù–û –¥–ª—è –º–µ–Ω—à–∏—Ö –≤–∏–º–æ–≥ –ø—ñ–¥ —á–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è
        
        # –õ–ï–ì–ö–ò–ô —à—Ç—Ä–∞—Ñ –∑–∞ –≤–æ–ª–∞—Ç—ñ–ª—å–Ω—ñ—Å—Ç—å (—Ç—ñ–ª—å–∫–∏ –ø—Ä–∏ –µ–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω–Ω—è—Ö)
        VolatilityPenalty(weight=-0.3, window=20),  # –ó–ú–ï–ù–®–ï–ù–û –¥–ª—è –∑–∞–æ—Ö–æ—á–µ–Ω–Ω—è –µ–∫—Å–ø–ª–æ—Ä–∞—Ü—ñ—ó
        
        # –í–ò–ù–ê–ì–û–†–û–î–ê –∑–∞ —è–∫—ñ—Å—Ç—å —É–≥–æ–¥ –∑ –º'—è–∫–∏–º–∏ –≤–∏–º–æ–≥–∞–º–∏
        TradeQualityReward(weight=1.0, min_trades=1),  # –ó–ú–ï–ù–®–ï–ù–û –º—ñ–Ω—ñ–º—É–º –¥–æ 1 —É–≥–æ–¥–∏ –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è
        
        # –î–£–ñ–ï –ú'–Ø–ö–Ü —à—Ç—Ä–∞—Ñ–∏ –∑–∞ –∑–±–∏—Ç–∫–∏ (–∑–∞–æ—Ö–æ—á—É—î–º–æ –µ–∫—Å–ø–ª–æ—Ä–∞—Ü—ñ—é)
        LossTradesPenalty(weight=-0.5),  # –°–ò–õ–¨–ù–û –ó–ú–ï–ù–®–ï–ù–û –¥–ª—è –∫—Ä–∞—â–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è
        
        # –í–ò–ú–ö–ù–ï–ù–û: –ú'—è–∫–∏–π —à—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≥—ñ—Ä—à–µ–Ω–Ω—è –ø–æ–∫–∞–∑–Ω–∏–∫—ñ–≤ (–∑–∞–Ω–∞–¥—Ç–æ –∞–≥—Ä–µ—Å–∏–≤–Ω–∏–π –ø—ñ–¥ —á–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è)
        # PerformanceDeclineReward(weight=-0.2, decline_threshold=0.20),  # –¢–ò–ú–ß–ê–°–û–í–û –í–ò–ú–ö–ù–ï–ù–û
        
        # –ê–ì–†–ï–°–ò–í–ù–ê –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑–∞ —Ç–æ—Ä–≥–æ–≤—É –∞–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å —Ç–∞ –µ–∫—Å–ø–ª–æ—Ä–∞—Ü—ñ—é 
        ExplorationReward(weight=3.0, target_trades_per_episode=20),  # –ó–ë–Ü–õ–¨–®–ï–ù–û –¥–ª—è —Ñ–æ—Ä—Å–æ–≤–∞–Ω–æ—ó –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
    ]
    
    composite = CompositeRewardScheme(schemes, enable_dynamic_scaling=True)  # –£–í–Ü–ú–ö–ù–ï–ù–û –¥–ª—è —Å—Ç–∞–±—ñ–ª—ñ–∑–∞—Ü—ñ—ó
    return composite