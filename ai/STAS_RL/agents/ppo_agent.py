import os
import numpy as np
import torch
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv
from stable_baselines3.common.callbacks import BaseCallback
from .base_agent import BaseAgent


class ExplorationMaintenanceCallback(BaseCallback):
    """Callback –¥–ª—è –ø—ñ–¥—Ç—Ä–∏–º–∫–∏ –º—ñ–Ω—ñ–º–∞–ª—å–Ω–æ–≥–æ —Ä—ñ–≤–Ω—è –µ–∫—Å–ø–ª–æ—Ä–∞—Ü—ñ—ó –ø—ñ–¥ —á–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è."""
    
    def __init__(self, min_std=1.0, check_frequency=500):
        super().__init__()
        self.min_std = min_std
        self.check_frequency = check_frequency
        self.step_count = 0
    
    def _on_step(self) -> bool:
        self.step_count += 1
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ç–∞ –∫–æ—Ä–µ–≥—É—î–º–æ –µ–∫—Å–ø–ª–æ—Ä–∞—Ü—ñ—é –∫–æ–∂–Ω—ñ check_frequency –∫—Ä–æ–∫—ñ–≤
        if self.step_count % self.check_frequency == 0:
            if hasattr(self.model.policy, 'log_std'):
                current_std = torch.exp(self.model.policy.log_std).mean().item()
                
                if current_std < self.min_std:
                    # –ê–ì–†–ï–°–ò–í–ù–û –ø—ñ–¥–≤–∏—â—É—î–º–æ std —è–∫—â–æ –≤—ñ–Ω –∑–∞–Ω–∞–¥—Ç–æ –Ω–∏–∑—å–∫–∏–π
                    target_log_std = np.log(self.min_std)
                    with torch.no_grad():
                        self.model.policy.log_std.fill_(target_log_std)
                    print(f"üîß FORCED EXPLORATION: std {current_std:.3f} -> {self.min_std:.3f}")
                else:
                    print(f"‚úÖ Exploration OK: std={current_std:.3f}")
        
        return True

class PPOAgent(BaseAgent):
    def __init__(self, config):
        super().__init__(config)
        self.model = None
        self.vec_env = None
        self.device = self._get_device()
        
    def _get_device(self):
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (GPU –∏–ª–∏ CPU)."""
        if torch.cuda.is_available():
            device = "cuda"
            gpu_name = torch.cuda.get_device_name(0)
            print(f"üöÄ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è GPU: {gpu_name}")
            print(f"üíæ –î–æ—Å—Ç—É–ø–Ω–∞—è –≤–∏–¥–µ–æ–ø–∞–º—è—Ç—å: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        else:
            device = "cpu"
            print("üîß GPU –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")
        return device
        
    def _create_parallel_envs(self, env, n_envs=4):
        """–°—Ç–≤–æ—Ä–∏—Ç–∏ –ø–∞—Ä–∞–ª–µ–ª—å–Ω—ñ —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞ –¥–ª—è –ø—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è."""
        import platform
        
        # –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø: –ù–∞ Windows –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ DummyVecEnv —á–µ—Ä–µ–∑ –ø—Ä–æ–±–ª–µ–º–∏ –∑ multiprocessing
        if platform.system() == "Windows":
            print(f"üîß Windows –≤–∏—è–≤–ª–µ–Ω–æ - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ DummyVecEnv –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ")
            return DummyVecEnv([lambda: env])
        
        try:
            # –°–ø—Ä–æ–±—É—î–º–æ —Å—Ç–≤–æ—Ä–∏—Ç–∏ –ø–∞—Ä–∞–ª–µ–ª—å–Ω—ñ —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞ —Ç—ñ–ª—å–∫–∏ –Ω–∞ Unix —Å–∏—Å—Ç–µ–º–∞—Ö
            env_fns = []
            for i in range(n_envs):
                env_fns.append(lambda: env)
            
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ SubprocVecEnv –¥–ª—è –ø–∞—Ä–∞–ª–µ–ª—ñ–∑–∞—Ü—ñ—ó
            vec_env = SubprocVecEnv(env_fns)
            print(f"‚úÖ –°—Ç–≤–æ—Ä–µ–Ω–æ {n_envs} –ø–∞—Ä–∞–ª–µ–ª—å–Ω–∏—Ö —Å–µ—Ä–µ–¥–æ–≤–∏—â –¥–ª—è –ø—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è")
            return vec_env
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ –≤–¥–∞–ª–æ—Å—è —Å—Ç–≤–æ—Ä–∏—Ç–∏ –ø–∞—Ä–∞–ª–µ–ª—å–Ω—ñ —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞: {e}")
            print("üîÑ –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–µ —Å–µ—Ä–µ–¥–æ–≤–∏—â–µ")
            return DummyVecEnv([lambda: env])

    def create_model(self, env, model_config=None):
        """–°–æ–∑–¥–∞—Ç—å –º–æ–¥–µ–ª—å PPO."""
        # –°—Ç–≤–æ—Ä—é—î–º–æ –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–µ —Å–µ—Ä–µ–¥–æ–≤–∏—â–µ (–ø–∞—Ä–∞–ª–µ–ª—å–Ω–µ —è–∫—â–æ –º–æ–∂–ª–∏–≤–æ)
        self.vec_env = self._create_parallel_envs(env, n_envs=4)
        
        # üö® –ö–†–ò–¢–ò–ß–ù–û: –ï–ö–°–¢–†–ï–ù–Ü –ü–ê–†–ê–ú–ï–¢–†–ò –î–õ–Ø –§–û–†–°–û–í–ê–ù–û–á –ï–ö–°–ü–õ–û–†–ê–¶–Ü–á
        default_config = {
            'learning_rate': 3e-4,  
            'n_steps': 2048,  
            'batch_size': 64,  
            'n_epochs': 10,  
            'gamma': 0.99,  
            'gae_lambda': 0.95,  
            'clip_range': 0.2,  
            # üö® –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–ê –ï–ù–¢–†–û–ü–Ü–Ø –¥–ª—è —Ñ–æ—Ä—Å–æ–≤–∞–Ω–æ—ó –µ–∫—Å–ø–ª–æ—Ä–∞—Ü—ñ—ó
            'ent_coef': 1.0,  # –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û –ó–ë–Ü–õ–¨–®–ï–ù–û –¥–ª—è –µ–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ—ó –µ–∫—Å–ø–ª–æ—Ä–∞—Ü—ñ—ó
            'vf_coef': 0.5,  
            'max_grad_norm': 0.5,  
            'verbose': 1,
            # üö® –ö–†–ò–¢–ò–ß–ù–û: –ó–ë–Ü–õ–¨–®–£–Ñ–ú–û target_kl - –∑–∞–Ω–∞–¥—Ç–æ –Ω–∏–∑—å–∫–µ –∑–Ω–∞—á–µ–Ω–Ω—è –±–ª–æ–∫—É—î –Ω–∞–≤—á–∞–Ω–Ω—è
            'target_kl': 0.05,  # –ó–ë–Ü–õ–¨–®–ï–ù–û –∑ 0.01 –¥–æ 0.05 - –±–∞—á–∏–º–æ "Early stopping due to max kl"
            'normalize_advantage': True,
            # üö® –†–ê–î–ò–ö–ê–õ–¨–ù–Ü –ó–ú–Ü–ù–ò policy_kwargs
            'policy_kwargs': {
                'net_arch': [32, 32],  # –ö–ê–†–î–ò–ù–ê–õ–¨–ù–û –ó–ú–ï–ù–®–ï–ù–û - –ø—Ä–æ—Å—Ç–∞ –º–µ—Ä–µ–∂–∞ –¥–ª—è –∫—Ä–∞—â–æ—ó –µ–∫—Å–ø–ª–æ—Ä–∞—Ü—ñ—ó
                'activation_fn': torch.nn.ReLU,  # –ü–û–í–ï–†–¢–ê–Ñ–ú–û ReLU
                'normalize_images': False,
                'ortho_init': True,  # –í–ú–ò–ö–ê–Ñ–ú–û –ù–ê–ó–ê–î - –º–æ–∂–µ –¥–æ–ø–æ–º–æ–≥—Ç–∏ –∑ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—î—é
                # üö® –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–ê log_std_init –¥–ª—è –µ–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ—ó –µ–∫—Å–ø–ª–æ—Ä–∞—Ü—ñ—ó
                'log_std_init': 1.0,  # –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û –ó–ë–Ü–õ–¨–®–ï–ù–û: 1.0 = std=2.7 (–µ–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–∞ –µ–∫—Å–ø–ª–æ—Ä–∞—Ü—ñ—è)
                'optimizer_class': torch.optim.Adam,
                'optimizer_kwargs': {'eps': 1e-5}
            }
        }
        
        if model_config:
            # –ö–†–ò–¢–ò–ß–ù–û: –ü—Ä–∞–≤–∏–ª—å–Ω–æ –æ–±'—î–¥–Ω—É—î–º–æ policy_kwargs –∑–∞–º—ñ—Å—Ç—å –ø–æ–≤–Ω–æ—ó –∑–∞–º—ñ–Ω–∏
            if 'policy_kwargs' in model_config and 'policy_kwargs' in default_config:
                # –û–±'—î–¥–Ω—É—î–º–æ policy_kwargs, –Ω–∞–¥–∞—é—á–∏ –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç model_config
                merged_policy_kwargs = default_config['policy_kwargs'].copy()
                merged_policy_kwargs.update(model_config['policy_kwargs'])
                
                # –¢–∏–º—á–∞—Å–æ–≤–æ –∑–±–µ—Ä—ñ–≥–∞—î–º–æ –æ–±'—î–¥–Ω–∞–Ω—ñ policy_kwargs
                temp_policy_kwargs = merged_policy_kwargs
                
                # –û–Ω–æ–≤–ª—é—î–º–æ config –±–µ–∑ policy_kwargs
                model_config_without_policy = {k: v for k, v in model_config.items() if k != 'policy_kwargs'}
                default_config.update(model_config_without_policy)
                
                # –í—Å—Ç–∞–Ω–æ–≤–ª—é—î–º–æ –æ–±'—î–¥–Ω–∞–Ω—ñ policy_kwargs
                default_config['policy_kwargs'] = temp_policy_kwargs
                
                print(f"üîß –û–±'—î–¥–Ω–∞–Ω–æ policy_kwargs:")
                print(f"   log_std_init: {default_config['policy_kwargs'].get('log_std_init', 'NOT SET')}")
                print(f"   net_arch: {default_config['policy_kwargs'].get('net_arch', 'NOT SET')}")
                print(f"   ortho_init: {default_config['policy_kwargs'].get('ortho_init', 'NOT SET')}")
            else:
                default_config.update(model_config)
        
        print(f"üîß –°—Ç–≤–æ—Ä—é—î–º–æ PPO –º–æ–¥–µ–ª—å –∑ –ö–†–ò–¢–ò–ß–ù–ò–ú–ò –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –µ–∫—Å–ø–ª–æ—Ä–∞—Ü—ñ—ó:")
        print(f"   ent_coef: {default_config['ent_coef']}")
        print(f"   log_std_init: {default_config['policy_kwargs']['log_std_init']}")
        print(f"   clip_range: {default_config['clip_range']}")
        print(f"   target_kl: {default_config['target_kl']}")
        
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å PPO
        self.model = PPO(
            "MlpPolicy",
            self.vec_env,
            device=self.device,
            **default_config
        )
        
        # –ö–†–ò–¢–ò–ß–ù–û: –ü—ñ—Å–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ —Ñ–æ—Ä—Å—É—î–º–æ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—é log_std
        print(f"üîß –§–æ—Ä—Å–æ–≤–∞–Ω–∞ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è log_std –ø—ñ—Å–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ...")
        self._force_exploration_init()
        
        # –ù–û–í–ò–ô: –í—Å—Ç–∞–Ω–æ–≤–ª—é—î–º–æ callback –¥–ª—è –ø—ñ–¥—Ç—Ä–∏–º–∫–∏ –µ–∫—Å–ø–ª–æ—Ä–∞—Ü—ñ—ó –ø—ñ–¥ —á–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è
        self._setup_exploration_maintenance()
        
        return self.model
    
    def _force_exploration_init(self):
        """–§–æ—Ä—Å–æ–≤–∞–Ω–∞ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –µ–∫—Å–ø–ª–æ—Ä–∞—Ü—ñ—ó."""
        if hasattr(self.model.policy, 'log_std'):
            # –ë–µ–∑–ø–æ—Å–µ—Ä–µ–¥–Ω—å–æ –≤—Å—Ç–∞–Ω–æ–≤–ª—é—î–º–æ log_std –¥–ª—è –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û–á –µ–∫—Å–ø–ª–æ—Ä–∞—Ü—ñ—ó
            with torch.no_grad():
                self.model.policy.log_std.fill_(0.5)  # std = exp(0.5) ‚âà 1.65
                print(f"‚úÖ log_std —Ñ–æ—Ä—Å–æ–≤–∞–Ω–æ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –Ω–∞ 0.5 (std ‚âà 1.65)")
        elif hasattr(self.model.policy, 'action_net') and hasattr(self.model.policy.action_net, 'log_std'):
            with torch.no_grad():
                self.model.policy.action_net.log_std.fill_(0.5)
                print(f"‚úÖ action_net.log_std —Ñ–æ—Ä—Å–æ–≤–∞–Ω–æ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –Ω–∞ 0.5")
        else:
            print(f"‚ö†Ô∏è –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–Ω–∞–π—Ç–∏ log_std –ø–∞—Ä–∞–º–µ—Ç—Ä –≤ policy")
    
    def _setup_exploration_maintenance(self):
        """–í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è –º–µ—Ö–∞–Ω—ñ–∑–º—É –ø—ñ–¥—Ç—Ä–∏–º–∫–∏ –µ–∫—Å–ø–ª–æ—Ä–∞—Ü—ñ—ó –ø—ñ–¥ —á–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è."""
        # –°—Ç–≤–æ—Ä—é—î–º–æ callback –¥–ª—è –ø—ñ–¥—Ç—Ä–∏–º–∫–∏ –º—ñ–Ω—ñ–º–∞–ª—å–Ω–æ–≥–æ —Ä—ñ–≤–Ω—è –µ–∫—Å–ø–ª–æ—Ä–∞—Ü—ñ—ó
        self.exploration_callback = ExplorationMaintenanceCallback()
    
    def train(self, total_timesteps=100000, callback=None):
        """–û–±—É—á–∏—Ç—å –∞–≥–µ–Ω—Ç–∞ –∑ —Ñ–æ—Ä—Å–æ–≤–∞–Ω–æ—é –ø—ñ–¥—Ç—Ä–∏–º–∫–æ—é –µ–∫—Å–ø–ª–æ—Ä–∞—Ü—ñ—ó."""
        if not self.model:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ —Å–æ–∑–¥–∞–Ω–∞. –í—ã–∑–æ–≤–∏—Ç–µ create_model() —Å–Ω–∞—á–∞–ª–∞.")
        
        # –ö–æ–º–±—ñ–Ω—É—î–º–æ callbacks: –æ—Å–Ω–æ–≤–Ω–∏–π + exploration maintenance
        from stable_baselines3.common.callbacks import CallbackList
        callbacks = []
        if callback:
            callbacks.append(callback)
        if hasattr(self, 'exploration_callback'):
            callbacks.append(self.exploration_callback)
        
        # –ü—Ä–∞–≤–∏–ª—å–Ω–æ –∫–æ–º–±—ñ–Ω—É—î–º–æ callbacks –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é CallbackList
        if len(callbacks) > 1:
            final_callback = CallbackList(callbacks)
        elif len(callbacks) == 1:
            final_callback = callbacks[0]
        else:
            final_callback = None
        
        self.model.learn(
            total_timesteps=total_timesteps,
            callback=final_callback
        )
        
        return self.model
    
    def act(self, state):
        """–í—ã–±—Ä–∞—Ç—å –¥–µ–π—Å—Ç–≤–∏–µ —Å —Ñ–æ—Ä—Å–æ–≤–∞–Ω–æ—é –µ–∫—Å–ø–ª–æ—Ä–∞—Ü—ñ—î—é."""
        if not self.model:
            return np.array([0.0])
        
        # –ö–†–ò–¢–ò–ß–ù–û: –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –ù–ï –¥–µ—Ç–µ—Ä–º—ñ–Ω–æ–≤–∞–Ω–∏–π —Ä–µ–∂–∏–º –¥–ª—è –µ–∫—Å–ø–ª–æ—Ä–∞—Ü—ñ—ó
        action, _ = self.model.predict(state, deterministic=False)
        
        # –î–û–î–ê–¢–ö–û–í–ê —Ñ–æ—Ä—Å–æ–≤–∞–Ω–∞ –µ–∫—Å–ø–ª–æ—Ä–∞—Ü—ñ—è —á–µ—Ä–µ–∑ —à—É–º
        exploration_noise = np.random.normal(0, 0.1, size=action.shape)
        action = action + exploration_noise
        action = np.clip(action, -1.0, 1.0)
        
        return action
    
    def save(self, path):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å."""
        if self.model:
            self.model.save(path)
    
    def load(self, path, env=None):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å."""
        if env:
            self.vec_env = DummyVecEnv([lambda: env])
        
        self.model = PPO.load(path, env=self.vec_env)
        return self.model 