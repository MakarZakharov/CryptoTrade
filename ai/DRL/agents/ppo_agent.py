import os
import numpy as np
import torch
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.callbacks import BaseCallback
from .base_agent import BaseAgent

class PPOAgent(BaseAgent):
    def __init__(self, config):
        super().__init__(config)
        self.model = None
        self.vec_env = None
        self.device = self._get_device()
        
    def _get_device(self):
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (GPU –∏–ª–∏ CPU)."""
        if torch.cuda.is_available():
            device = "cuda"
            gpu_name = torch.cuda.get_device_name(0)
            print(f"üöÄ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è GPU: {gpu_name}")
            print(f"üíæ –î–æ—Å—Ç—É–ø–Ω–∞—è –≤–∏–¥–µ–æ–ø–∞–º—è—Ç—å: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        else:
            device = "cpu"
            print("üîß GPU –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")
        return device
        
    def create_model(self, env, model_config=None):
        """–°–æ–∑–¥–∞—Ç—å –º–æ–¥–µ–ª—å PPO."""
        # –û–±–æ—Ä–∞—á–∏–≤–∞–µ–º —Å—Ä–µ–¥—É
        self.vec_env = DummyVecEnv([lambda: env])
        
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ø—Ä–∏–±—ã–ª—å–Ω–æ–π —Ç–æ—Ä–≥–æ–≤–ª–∏ –Ω–∞ 15–º–∏–Ω
        default_config = {
            'learning_rate': 1e-4,  # –ë–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
            'n_steps': 1024,  # –ú–µ–Ω—å—à–µ —à–∞–≥–æ–≤ –¥–ª—è —á–∞—Å—Ç—ã—Ö –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π
            'batch_size': 128,  # –ë–æ–ª—å—à–µ —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            'n_epochs': 4,  # –ú–µ–Ω—å—à–µ —ç–ø–æ—Ö –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
            'gamma': 0.995,  # –í—ã—à–µ –¥–ª—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –±—É–¥—É—â–∏—Ö –Ω–∞–≥—Ä–∞–¥
            'gae_lambda': 0.98,  # –í—ã—à–µ –¥–ª—è –ª—É—á—à–µ–π –æ—Ü–µ–Ω–∫–∏ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤
            'clip_range': 0.15,  # –ë–æ–ª–µ–µ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–∞—è –ø–æ–ª–∏—Ç–∏–∫–∞
            'ent_coef': 0.01,  # –ù–µ–±–æ–ª—å—à–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            'vf_coef': 0.25,  # –ú–µ–Ω—å—à–∏–π –≤–µ—Å —Ñ—É–Ω–∫—Ü–∏–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏
            'max_grad_norm': 0.3,  # –ë–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–π –∫–ª–∏–ø–ø–∏–Ω–≥ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
            'verbose': 1,
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            'use_sde': False,  # –û—Ç–∫–ª—é—á–∞–µ–º —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ
            'sde_sample_freq': -1,
            'target_kl': 0.01,  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è –ø–æ–ª–∏—Ç–∏–∫–∏
            'normalize_advantage': True  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤
        }
        
        if model_config:
            default_config.update(model_config)
        
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å PPO
        self.model = PPO(
            "MlpPolicy",
            self.vec_env,
            device=self.device,
            **default_config
        )
        
        return self.model
    
    def train(self, total_timesteps=100000, callback=None):
        """–û–±—É—á–∏—Ç—å –∞–≥–µ–Ω—Ç–∞."""
        if not self.model:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ —Å–æ–∑–¥–∞–Ω–∞. –í—ã–∑–æ–≤–∏—Ç–µ create_model() —Å–Ω–∞—á–∞–ª–∞.")
        
        self.model.learn(
            total_timesteps=total_timesteps,
            callback=callback
        )
        
        return self.model
    
    def act(self, state):
        """–í—ã–±—Ä–∞—Ç—å –¥–µ–π—Å—Ç–≤–∏–µ."""
        if not self.model:
            return np.array([0.0])
        
        action, _ = self.model.predict(state, deterministic=True)
        return action
    
    def save(self, path):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å."""
        if self.model:
            self.model.save(path)
    
    def load(self, path, env=None):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å."""
        if env:
            self.vec_env = DummyVecEnv([lambda: env])
        
        self.model = PPO.load(path, env=self.vec_env)
        return self.model 