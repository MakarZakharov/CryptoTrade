"""–ü—Ä–∏–º–µ—Ä –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è DRL –∞–≥–µ–Ω—Ç–æ–≤."""

import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from CryptoTrade.ai.DRL.config import DRLConfig, TradingConfig
from CryptoTrade.ai.DRL.utils import HyperparameterTuner, DRLLogger


def optimize_ppo_agent():
    """–ü—Ä–∏–º–µ—Ä –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ PPO –∞–≥–µ–Ω—Ç–∞."""
    
    print("=" * 60)
    print("–û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –ì–ò–ü–ï–†–ü–ê–†–ê–ú–ï–¢–†–û–í PPO –ê–ì–ï–ù–¢–ê")
    print("=" * 60)
    
    # –ë–∞–∑–æ–≤—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    base_trading_config = TradingConfig(
        symbol="BTCUSDT",
        timeframe="1d",
        initial_balance=10000.0,
        reward_scheme="profit_based",
        action_type="continuous",
        lookback_window=20,
        max_episode_steps=300  # –ö–æ—Ä–æ—Ç–∫–∏–µ —ç–ø–∏–∑–æ–¥—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    )
    
    base_drl_config = DRLConfig(
        agent_type="PPO",
        total_timesteps=20000,  # –ù–µ–±–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
        verbose=0,
        tensorboard_log=False,  # –û—Ç–∫–ª—é—á–∞–µ–º –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        eval_freq=10000,
        save_freq=20000
    )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç—é–Ω–µ—Ä–∞
    logger = DRLLogger("ppo_optimization")
    tuner = HyperparameterTuner(
        base_drl_config=base_drl_config,
        base_trading_config=base_trading_config,
        study_name="ppo_trading_optimization",
        direction="maximize"
    )
    
    # –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –¥–∏–∞–ø–∞–∑–æ–Ω—ã –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    custom_ranges = {
        "reward_scaling": {
            "type": "float",
            "low": 0.1,
            "high": 10.0,
            "log": True
        },
        "lookback_window": {
            "type": "categorical",
            "choices": [10, 20, 30, 50]
        }
    }
    
    try:
        # –ó–∞–ø—É—Å–∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        results = tuner.optimize(
            agent_type="PPO",
            n_trials=10,  # –ù–µ–±–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
            training_timesteps=20000,
            evaluation_episodes=3,
            optimization_metric="mean_reward",
            parameter_ranges=custom_ranges,
            timeout=1800,  # 30 –º–∏–Ω—É—Ç –º–∞–∫—Å–∏–º—É–º
            n_jobs=1  # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
        )
        
        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print("\n" + "=" * 60)
        print("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò")
        print("=" * 60)
        
        best_trial = results["best_trial"]
        print(f"–õ—É—á—à–∏–π trial: #{best_trial['number']}")
        print(f"–õ—É—á—à–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {best_trial['value']:.4f}")
        
        print("\n–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:")
        for param, value in best_trial["params"].items():
            print(f"  {param}: {value}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≤—Å–µ–º trials
        stats = results["all_trials_stats"]
        print(f"\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≤—Å–µ–º trials:")
        print(f"  –°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {stats['mean_value']:.4f}")
        print(f"  –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {stats['std_value']:.4f}")
        print(f"  –õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {stats['max_value']:.4f}")
        
        # –í–∞–∂–Ω–æ—Å—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        if results["parameter_importance"]:
            print(f"\n–í–∞–∂–Ω–æ—Å—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:")
            importance_sorted = sorted(
                results["parameter_importance"].items(),
                key=lambda x: x[1],
                reverse=True
            )
            for param, importance in importance_sorted[:5]:  # –¢–æ–ø 5
                print(f"  {param}: {importance:.4f}")
        
        # –û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        print(f"\n" + "=" * 60)
        print("–û–ë–£–ß–ï–ù–ò–ï –§–ò–ù–ê–õ–¨–ù–û–ô –ú–û–î–ï–õ–ò")
        print("=" * 60)
        
        final_agent = tuner.train_best_model(
            agent_type="PPO",
            training_timesteps=50000,  # –ë–æ–ª—å—à–µ —à–∞–≥–æ–≤ –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
            experiment_name="optimized_ppo_final"
        )
        
        print("‚úÖ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏ –æ–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –∑–∞–≤–µ—Ä—à–µ–Ω—ã!")
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
        report = tuner.generate_optimization_report()
        print(f"\n{report}")
        
        return tuner, results
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {e}")
        import traceback
        traceback.print_exc()
        return None, None


def compare_agents_optimization():
    """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∞–≥–µ–Ω—Ç–æ–≤."""
    
    print("=" * 60)
    print("–°–†–ê–í–ù–ï–ù–ò–ï –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò –†–ê–ó–ù–´–• –ê–ì–ï–ù–¢–û–í")
    print("=" * 60)
    
    # –û–±—â–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    trading_config = TradingConfig(
        symbol="BTCUSDT",
        timeframe="1d",
        initial_balance=10000.0,
        max_episode_steps=200
    )
    
    base_drl_config = DRLConfig(
        total_timesteps=15000,
        verbose=0
    )
    
    agents_to_compare = ["PPO", "SAC", "A2C"]
    results_comparison = {}
    
    logger = DRLLogger("agents_comparison")
    
    for agent_type in agents_to_compare:
        try:
            print(f"\n--- –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è {agent_type} –∞–≥–µ–Ω—Ç–∞ ---")
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª—è continuous/discrete –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∞–≥–µ–Ω—Ç–∞
            if agent_type in ["PPO", "SAC", "A2C"]:
                trading_config.action_type = "continuous"
            else:
                trading_config.action_type = "discrete"
            
            # –°–æ–∑–¥–∞–Ω–∏–µ —Ç—é–Ω–µ—Ä–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∞–≥–µ–Ω—Ç–∞
            tuner = HyperparameterTuner(
                base_drl_config=base_drl_config,
                base_trading_config=trading_config,
                study_name=f"{agent_type.lower()}_comparison",
                direction="maximize"
            )
            
            # –ë—ã—Å—Ç—Ä–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
            results = tuner.optimize(
                agent_type=agent_type,
                n_trials=5,  # –ú–∞–ª–æ trials –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
                training_timesteps=15000,
                evaluation_episodes=3,
                optimization_metric="mean_reward",
                timeout=600  # 10 –º–∏–Ω—É—Ç –Ω–∞ –∞–≥–µ–Ω—Ç–∞
            )
            
            results_comparison[agent_type] = {
                "best_value": results["best_trial"]["value"],
                "best_params": results["best_trial"]["params"],
                "mean_value": results["all_trials_stats"]["mean_value"],
                "std_value": results["all_trials_stats"]["std_value"]
            }
            
            print(f"‚úÖ {agent_type}: –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç = {results['best_trial']['value']:.4f}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å {agent_type}: {e}")
            results_comparison[agent_type] = {"error": str(e)}
    
    # –í—ã–≤–æ–¥ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    print(f"\n" + "=" * 60)
    print("–°–†–ê–í–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
    print("=" * 60)
    
    successful_results = {k: v for k, v in results_comparison.items() if "error" not in v}
    
    if successful_results:
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ª—É—á—à–µ–º—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É
        sorted_results = sorted(
            successful_results.items(),
            key=lambda x: x[1]["best_value"],
            reverse=True
        )
        
        print("–†–µ–π—Ç–∏–Ω–≥ –∞–≥–µ–Ω—Ç–æ–≤ –ø–æ –ª—É—á—à–µ–º—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É:")
        for i, (agent, result) in enumerate(sorted_results, 1):
            print(f"{i}. {agent}: {result['best_value']:.4f} "
                  f"(—Å—Ä–µ–¥–Ω–µ–µ: {result['mean_value']:.4f}¬±{result['std_value']:.4f})")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è
        best_agent = sorted_results[0][0]
        print(f"\nüèÜ –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π –∞–≥–µ–Ω—Ç: {best_agent}")
    
    else:
        print("–ù–µ —É–¥–∞–ª–æ—Å—å —É—Å–ø–µ—à–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞")
    
    return results_comparison


def advanced_optimization_example():
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø—Ä–∏–º–µ—Ä –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏."""
    
    print("=" * 60)
    print("–ü–†–û–î–í–ò–ù–£–¢–ê–Ø –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –° –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–¨–°–ö–ò–ú–ò –ü–ê–†–ê–ú–ï–¢–†–ê–ú–ò")
    print("=" * 60)
    
    # –¢–æ—Ä–≥–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    trading_config = TradingConfig(
        symbol="BTCUSDT",
        timeframe="1d",
        initial_balance=10000.0,
        reward_scheme="risk_adjusted",  # –ë–æ–ª–µ–µ —Å–ª–æ–∂–Ω–∞—è —Å—Ö–µ–º–∞ –Ω–∞–≥—Ä–∞–¥
        action_type="continuous",
        lookback_window=30,
        max_episode_steps=400
    )
    
    drl_config = DRLConfig(
        agent_type="SAC",
        total_timesteps=25000,
        verbose=0
    )
    
    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –¥–∏–∞–ø–∞–∑–æ–Ω—ã –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    advanced_ranges = {
        # –¢–æ—Ä–≥–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        "commission_rate": {
            "type": "float",
            "low": 0.001,
            "high": 0.01
        },
        "max_risk_per_trade": {
            "type": "float",
            "low": 0.01,
            "high": 0.05
        },
        "lookback_window": {
            "type": "categorical",
            "choices": [20, 30, 50, 80]
        },
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞–≥—Ä–∞–¥
        "reward_scaling": {
            "type": "float",
            "low": 0.1,
            "high": 5.0,
            "log": True
        },
        
        # –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        "use_lstm": {
            "type": "categorical", 
            "choices": [True, False]
        },
        
        # SAC —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        "train_freq": {
            "type": "categorical",
            "choices": [1, 4, 8, 16]
        },
        "gradient_steps": {
            "type": "categorical",
            "choices": [1, 2, 4]
        }
    }
    
    logger = DRLLogger("advanced_optimization")
    
    tuner = HyperparameterTuner(
        base_drl_config=drl_config,
        base_trading_config=trading_config,
        study_name="advanced_sac_optimization",
        direction="maximize"
    )
    
    try:
        # –ó–∞–ø—É—Å–∫ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        results = tuner.optimize(
            agent_type="SAC",
            n_trials=15,
            training_timesteps=25000,
            evaluation_episodes=5,
            optimization_metric="total_return",  # –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º –ø–æ –æ–±—â–µ–π –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏
            parameter_ranges=advanced_ranges,
            timeout=2400,  # 40 –º–∏–Ω—É—Ç
            n_jobs=1
        )
        
        print("\nüéØ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        
        # –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        best_trial = results["best_trial"]
        print(f"\n–õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {best_trial['value']:.4f}")
        
        print(f"\n–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:")
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        ml_params = {}
        trading_params = {}
        architecture_params = {}
        
        for param, value in best_trial["params"].items():
            if param in ["learning_rate", "batch_size", "gamma", "tau", "alpha"]:
                ml_params[param] = value
            elif param in ["commission_rate", "max_risk_per_trade", "lookback_window"]:
                trading_params[param] = value
            elif param in ["net_arch_size", "net_arch_layers", "activation_fn", "use_lstm"]:
                architecture_params[param] = value
            else:
                ml_params[param] = value
        
        if ml_params:
            print("\n  ML –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:")
            for param, value in ml_params.items():
                print(f"    {param}: {value}")
        
        if trading_params:
            print("\n  –¢–æ—Ä–≥–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:")
            for param, value in trading_params.items():
                print(f"    {param}: {value}")
        
        if architecture_params:
            print("\n  –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:")
            for param, value in architecture_params.items():
                print(f"    {param}: {value}")
        
        # –í–∞–∂–Ω–æ—Å—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        if results["parameter_importance"]:
            print(f"\nüìä –í–∞–∂–Ω–æ—Å—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (—Ç–æ–ø-10):")
            importance_sorted = sorted(
                results["parameter_importance"].items(),
                key=lambda x: x[1],
                reverse=True
            )
            for param, importance in importance_sorted[:10]:
                print(f"  {param}: {importance:.3f}")
        
        return tuner, results
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {e}")
        import traceback
        traceback.print_exc()
        return None, None


if __name__ == "__main__":
    print("–í—ã–±–µ—Ä–∏—Ç–µ —Ç–∏–ø –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:")
    print("1. –ë–∞–∑–æ–≤–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è PPO –∞–≥–µ–Ω—Ç–∞")
    print("2. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∞–∑–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤")
    print("3. –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏")
    print("4. –ó–∞–ø—É—Å—Ç–∏—Ç—å –≤—Å–µ –ø—Ä–∏–º–µ—Ä—ã")
    
    choice = input("\n–í–≤–µ–¥–∏—Ç–µ –Ω–æ–º–µ—Ä (1-4): ").strip()
    
    if choice == "1":
        optimize_ppo_agent()
    elif choice == "2":
        compare_agents_optimization()
    elif choice == "3":
        advanced_optimization_example()
    elif choice == "4":
        print("–ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏...\n")
        optimize_ppo_agent()
        print("\n" + "="*80 + "\n")
        compare_agents_optimization()
        print("\n" + "="*80 + "\n")
        advanced_optimization_example()
    else:
        print("–ó–∞–ø—É—Å–∫ –±–∞–∑–æ–≤–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é...")
        optimize_ppo_agent()