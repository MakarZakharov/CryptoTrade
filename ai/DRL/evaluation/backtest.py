"""–ú–æ–¥—É–ª—å –±–µ–∫—Ç–µ—Å—Ç–∏–Ω–≥–∞ –¥–ª—è DRL –∞–≥–µ–Ω—Ç–æ–≤."""

import json
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, Any, List, Optional, Union
from datetime import datetime

from ..agents.base_agent import BaseAgent
from ..environments import TradingEnv
from ..config import DRLConfig, TradingConfig
from ..utils import DRLLogger, TradingMetrics


class DRLBacktester:
    """
    –°–∏—Å—Ç–µ–º–∞ –±–µ–∫—Ç–µ—Å—Ç–∏–Ω–≥–∞ –¥–ª—è DRL –∞–≥–µ–Ω—Ç–æ–≤.
    
    –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤
    –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–æ–π.
    """
    
    def __init__(
        self,
        agent: BaseAgent,
        config: TradingConfig,
        logger: Optional[DRLLogger] = None
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–µ–∫—Ç–µ—Å—Ç–µ—Ä–∞.
        
        Args:
            agent: –æ–±—É—á–µ–Ω–Ω—ã–π DRL –∞–≥–µ–Ω—Ç
            config: —Ç–æ—Ä–≥–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
            logger: –ª–æ–≥–≥–µ—Ä
        """
        self.agent = agent
        self.config = config
        self.logger = logger or DRLLogger("drl_backtester")
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–∫—Ç–µ—Å—Ç–∞
        self.results: Dict[str, Any] = {}
        self.trade_history: List[Dict[str, Any]] = []
        self.portfolio_history: List[Dict[str, Any]] = []
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        self.trading_metrics = TradingMetrics()
        
        self.logger.info("DRL Backtester –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def run_backtest(
        self,
        test_data: Optional[pd.DataFrame] = None,
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        deterministic: bool = True,
        save_results: bool = True
    ) -> Dict[str, Any]:
        """
        –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –±–µ–∫—Ç–µ—Å—Ç–∞.
        
        Args:
            test_data: —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            start_date: –Ω–∞—á–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞
            end_date: –∫–æ–Ω–µ—á–Ω–∞—è –¥–∞—Ç–∞
            deterministic: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Å—Ç–∏—á–Ω—É—é –ø–æ–ª–∏—Ç–∏–∫—É
            save_results: —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–∫—Ç–µ—Å—Ç–∞
        """
        self.logger.info("–ó–∞–ø—É—Å–∫ –±–µ–∫—Ç–µ—Å—Ç–∞ DRL –∞–≥–µ–Ω—Ç–∞...")
        start_time = datetime.now()
        
        try:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å—Ä–µ–¥—ã
            test_env = self._prepare_test_environment(test_data, start_date, end_date)
            
            # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –±–µ–∫—Ç–µ—Å—Ç–∞
            episode_results = self._run_backtest_episode(test_env, deterministic)
            
            # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            analysis_results = self._analyze_results(episode_results)
            
            # –°–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞
            self.results = self._compile_final_results(
                episode_results, 
                analysis_results,
                start_time
            )
            
            if save_results:
                self._save_results()
            
            self.logger.info("–ë–µ–∫—Ç–µ—Å—Ç –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
            return self.results
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –±–µ–∫—Ç–µ—Å—Ç–∞: {e}")
            raise
    
    def _prepare_test_environment(
        self,
        test_data: Optional[pd.DataFrame],
        start_date: Optional[str],
        end_date: Optional[str]
    ) -> TradingEnv:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤–æ–π —Å—Ä–µ–¥—ã."""
        self.logger.info("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤–æ–π —Å—Ä–µ–¥—ã...")
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—É—é —Å—Ä–µ–¥—É
        test_env = TradingEnv(self.config, data=test_data, logger=self.logger)
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        test_env.set_data_split("test")
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –¥–∞—Ç–∞–º –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if start_date or end_date:
            test_env = self._filter_data_by_dates(test_env, start_date, end_date)
        
        data_info = test_env.get_data_info()
        self.logger.info(f"–¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ: {data_info['test_samples']} –æ–±—Ä–∞–∑—Ü–æ–≤")
        self.logger.info(f"–ü–µ—Ä–∏–æ–¥: {start_date or '–Ω–∞—á–∞–ª–æ'} - {end_date or '–∫–æ–Ω–µ—Ü'}")
        
        return test_env
    
    def _filter_data_by_dates(
        self, 
        env: TradingEnv, 
        start_date: Optional[str], 
        end_date: Optional[str]
    ) -> TradingEnv:
        """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ –¥–∞—Ç–∞–º."""
        # –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –º–æ–∂–µ—Ç –±—ã—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–∞
        # –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –¥–∞—Ç–∞–º
        return env
    
    def _run_backtest_episode(
        self, 
        test_env: TradingEnv, 
        deterministic: bool
    ) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ–¥–Ω–æ–≥–æ —ç–ø–∏–∑–æ–¥–∞ –±–µ–∫—Ç–µ—Å—Ç–∞."""
        self.logger.info("–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –±–µ–∫—Ç–µ—Å—Ç —ç–ø–∏–∑–æ–¥–∞...")
        
        # –°–±—Ä–æ—Å —Å—Ä–µ–¥—ã
        obs, info = test_env.reset()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è
        episode_data = {
            'observations': [],
            'actions': [],
            'rewards': [],
            'info': [],
            'portfolio_values': [],
            'prices': []
        }
        
        step = 0
        total_reward = 0
        done = False
        
        while not done:
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—è
            action = self.agent.predict(obs, deterministic=deterministic)
            
            # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —à–∞–≥–∞
            obs, reward, terminated, truncated, info = test_env.step(action)
            done = terminated or truncated
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
            episode_data['observations'].append(obs.copy())
            episode_data['actions'].append(action)
            episode_data['rewards'].append(reward)
            episode_data['info'].append(info.copy())
            episode_data['portfolio_values'].append(
                info.get('portfolio', {}).get('total_value', 0)
            )
            episode_data['prices'].append(info.get('price', 0))
            
            total_reward += reward
            step += 1
            
            if step % 1000 == 0:
                self.logger.debug(f"–®–∞–≥ {step}, –Ω–∞–≥—Ä–∞–¥–∞: {reward:.4f}")
        
        # –§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ —ç–ø–∏–∑–æ–¥–∞
        final_info = test_env.get_episode_summary()
        
        episode_results = {
            'episode_data': episode_data,
            'final_info': final_info,
            'total_steps': step,
            'total_reward': total_reward,
            'final_portfolio_value': episode_data['portfolio_values'][-1] if episode_data['portfolio_values'] else 0
        }
        
        self.logger.info(f"–≠–ø–∏–∑–æ–¥ –∑–∞–≤–µ—Ä—à–µ–Ω: {step} —à–∞–≥–æ–≤, –∏—Ç–æ–≥–æ–≤–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {total_reward:.4f}")
        
        return episode_results
    
    def _analyze_results(self, episode_results: Dict[str, Any]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–µ–∫—Ç–µ—Å—Ç–∞."""
        self.logger.info("–ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–µ–∫—Ç–µ—Å—Ç–∞...")
        
        episode_data = episode_results['episode_data']
        
        # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        portfolio_values = np.array(episode_data['portfolio_values'])
        prices = np.array(episode_data['prices'])
        rewards = np.array(episode_data['rewards'])
        
        # –î–æ—Ö–æ–¥–Ω–æ—Å—Ç—å
        returns = np.diff(portfolio_values) / portfolio_values[:-1]
        returns = returns[~np.isnan(returns)]  # –£–¥–∞–ª—è–µ–º NaN
        
        # –ë–∞–∑–æ–≤—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        initial_value = portfolio_values[0] if len(portfolio_values) > 0 else self.config.initial_balance
        final_value = portfolio_values[-1] if len(portfolio_values) > 0 else initial_value
        total_return = (final_value - initial_value) / initial_value
        
        # Buy & Hold —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
        initial_price = prices[0] if len(prices) > 0 else 1
        final_price = prices[-1] if len(prices) > 0 else initial_price
        buy_hold_return = (final_price - initial_price) / initial_price
        
        # –†–∏—Å–∫-–º–µ—Ç—Ä–∏–∫–∏
        volatility = np.std(returns) * np.sqrt(252) if len(returns) > 1 else 0
        sharpe_ratio = self.trading_metrics.sharpe_ratio(returns) if len(returns) > 1 else 0
        max_drawdown = self._calculate_max_drawdown(portfolio_values)
        
        # –¢–æ—Ä–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        trade_analysis = self._analyze_trades(episode_data['info'])
        
        analysis = {
            'performance_metrics': {
                'total_return': float(total_return),
                'total_return_pct': float(total_return * 100),
                'annualized_return': float(total_return * 252 / len(portfolio_values)) if len(portfolio_values) > 0 else 0,
                'buy_hold_return': float(buy_hold_return),
                'buy_hold_return_pct': float(buy_hold_return * 100),
                'alpha': float(total_return - buy_hold_return),
                'volatility': float(volatility),
                'sharpe_ratio': float(sharpe_ratio),
                'max_drawdown': float(max_drawdown),
                'max_drawdown_pct': float(max_drawdown * 100),
                'final_portfolio_value': float(final_value),
                'initial_portfolio_value': float(initial_value)
            },
            'trading_metrics': trade_analysis,
            'reward_metrics': {
                'total_reward': float(np.sum(rewards)),
                'mean_reward': float(np.mean(rewards)),
                'std_reward': float(np.std(rewards)),
                'min_reward': float(np.min(rewards)),
                'max_reward': float(np.max(rewards))
            }
        }
        
        return analysis
    
    def _calculate_max_drawdown(self, portfolio_values: np.ndarray) -> float:
        """–†–∞—Å—á–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ—Å–∞–¥–∫–∏."""
        if len(portfolio_values) == 0:
            return 0.0
        
        # –ö—É–º—É–ª—è—Ç–∏–≤–Ω—ã–π –º–∞–∫—Å–∏–º—É–º
        cummax = np.maximum.accumulate(portfolio_values)
        
        # –ü—Ä–æ—Å–∞–¥–∫–∞
        drawdown = (cummax - portfolio_values) / cummax
        
        return float(np.max(drawdown))
    
    def _analyze_trades(self, info_history: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Ç–æ—Ä–≥–æ–≤—ã—Ö —Å–¥–µ–ª–æ–∫."""
        trades = []
        positions = []
        
        for info in info_history:
            if 'trade' in info:
                trades.append(info['trade'])
            
            portfolio = info.get('portfolio', {})
            if portfolio:
                positions.append({
                    'step': info.get('step', 0),
                    'position_size': portfolio.get('position_size', 0),
                    'total_value': portfolio.get('total_value', 0)
                })
        
        if not trades:
            return {
                'total_trades': 0,
                'winning_trades': 0,
                'losing_trades': 0,
                'win_rate': 0.0,
                'profit_factor': 0.0,
                'avg_trade_return': 0.0,
                'avg_winning_trade': 0.0,
                'avg_losing_trade': 0.0
            }
        
        # –ê–Ω–∞–ª–∏–∑ —Å–¥–µ–ª–æ–∫
        trade_returns = []
        winning_trades = []
        losing_trades = []
        
        for trade in trades:
            if isinstance(trade, dict) and 'pnl' in trade:
                pnl = trade['pnl']
                if pnl != 0:
                    trade_returns.append(pnl)
                    if pnl > 0:
                        winning_trades.append(pnl)
                    else:
                        losing_trades.append(pnl)
        
        win_rate = len(winning_trades) / len(trade_returns) if trade_returns else 0
        profit_factor = (sum(winning_trades) / abs(sum(losing_trades))) if losing_trades else float('inf')
        
        return {
            'total_trades': len(trades),
            'profitable_trades': len(winning_trades),
            'losing_trades': len(losing_trades),
            'win_rate': float(win_rate),
            'profit_factor': float(profit_factor),
            'avg_trade_return': float(np.mean(trade_returns)) if trade_returns else 0,
            'avg_winning_trade': float(np.mean(winning_trades)) if winning_trades else 0,
            'avg_losing_trade': float(np.mean(losing_trades)) if losing_trades else 0,
            'largest_win': float(np.max(winning_trades)) if winning_trades else 0,
            'largest_loss': float(np.min(losing_trades)) if losing_trades else 0
        }
    
    def _compile_final_results(
        self, 
        episode_results: Dict[str, Any],
        analysis_results: Dict[str, Any],
        start_time: datetime
    ) -> Dict[str, Any]:
        """–°–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞."""
        end_time = datetime.now()
        execution_time = (end_time - start_time).total_seconds()
        
        final_results = {
            'metadata': {
                'agent_type': self.agent.__class__.__name__,
                'symbol': self.config.symbol,
                'timeframe': self.config.timeframe,
                'initial_balance': self.config.initial_balance,
                'start_time': start_time.isoformat(),
                'end_time': end_time.isoformat(),
                'execution_time_seconds': execution_time,
                'total_steps': episode_results['total_steps']
            },
            'performance': analysis_results['performance_metrics'],
            'trading': analysis_results['trading_metrics'],
            'rewards': analysis_results['reward_metrics'],
            'summary': {
                'success': True,
                'total_return_pct': analysis_results['performance_metrics']['total_return_pct'],
                'vs_buy_hold': analysis_results['performance_metrics']['alpha'] * 100,
                'sharpe_ratio': analysis_results['performance_metrics']['sharpe_ratio'],
                'max_drawdown_pct': analysis_results['performance_metrics']['max_drawdown_pct'],
                'win_rate': analysis_results['trading_metrics']['win_rate'] * 100,
                'total_trades': analysis_results['trading_metrics']['total_trades']
            }
        }
        
        return final_results
    
    def _save_results(self, filename: Optional[str] = None):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–µ–∫—Ç–µ—Å—Ç–∞."""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"backtest_results_{self.config.symbol}_{timestamp}.json"
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        results_dir = Path("CryptoTrade/ai/DRL/evaluation/results")
        results_dir.mkdir(parents=True, exist_ok=True)
        
        filepath = results_dir / filename
        
        with open(filepath, 'w') as f:
            json.dump(self.results, f, indent=2, default=str)
        
        self.logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–∫—Ç–µ—Å—Ç–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {filepath}")
    
    def print_results(self):
        """–í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–µ–∫—Ç–µ—Å—Ç–∞ –≤ –∫–æ–Ω—Å–æ–ª—å."""
        if not self.results:
            self.logger.warning("–ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è")
            return
        
        print("\n" + "="*70)
        print("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ë–ï–ö–¢–ï–°–¢–ê DRL –ê–ì–ï–ù–¢–ê")
        print("="*70)
        
        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        meta = self.results['metadata']
        print(f"–ê–≥–µ–Ω—Ç: {meta['agent_type']}")
        print(f"–°–∏–º–≤–æ–ª: {meta['symbol']} ({meta['timeframe']})")
        print(f"–ù–∞—á–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å: ${meta['initial_balance']:,.2f}")
        print(f"–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {meta['execution_time_seconds']:.2f} —Å–µ–∫")
        print(f"–í—Å–µ–≥–æ —à–∞–≥–æ–≤: {meta['total_steps']:,}")
        
        # –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        print(f"\nüìä –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–¨:")
        perf = self.results['performance']
        print(f"   –ò—Ç–æ–≥–æ–≤–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å: {perf['total_return_pct']:.2f}%")
        print(f"   Buy & Hold –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å: {perf['buy_hold_return_pct']:.2f}%")
        print(f"   –ê–ª—å—Ñ–∞ (–ø—Ä–µ–≤—ã—à–µ–Ω–∏–µ): {perf['alpha']*100:.2f}%")
        print(f"   –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –®–∞—Ä–ø–∞: {perf['sharpe_ratio']:.2f}")
        print(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–æ—Å–∞–¥–∫–∞: {perf['max_drawdown_pct']:.2f}%")
        print(f"   –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å: {perf['volatility']:.2f}")
        
        # –¢–æ—Ä–≥–æ–≤–ª—è
        print(f"\nüíº –¢–û–†–ì–û–í–ê–Ø –ê–ö–¢–ò–í–ù–û–°–¢–¨:")
        trading = self.results['trading']
        print(f"   –í—Å–µ–≥–æ —Å–¥–µ–ª–æ–∫: {trading['total_trades']}")
        print(f"   –ü—Ä–∏–±—ã–ª—å–Ω—ã–µ —Å–¥–µ–ª–∫–∏: {trading['profitable_trades']}")
        print(f"   –£–±—ã—Ç–æ—á–Ω—ã–µ —Å–¥–µ–ª–∫–∏: {trading['losing_trades']}")
        print(f"   –í–∏–Ω—Ä–µ–π—Ç: {trading['win_rate']*100:.1f}%")
        print(f"   Profit Factor: {trading['profit_factor']:.2f}")
        print(f"   –°—Ä–µ–¥–Ω—è—è –ø—Ä–∏–±—ã–ª—å —Å–¥–µ–ª–∫–∏: ${trading['avg_winning_trade']:.2f}")
        print(f"   –°—Ä–µ–¥–Ω–∏–π —É–±—ã—Ç–æ–∫ —Å–¥–µ–ª–∫–∏: ${trading['avg_losing_trade']:.2f}")
        
        # –ù–∞–≥—Ä–∞–¥—ã
        print(f"\nüéØ –ú–ï–¢–†–ò–ö–ò –ù–ê–ì–†–ê–î:")
        rewards = self.results['rewards']
        print(f"   –û–±—â–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {rewards['total_reward']:.4f}")
        print(f"   –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: {rewards['mean_reward']:.4f}")
        print(f"   –õ—É—á—à–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {rewards['max_reward']:.4f}")
        print(f"   –•—É–¥—à–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {rewards['min_reward']:.4f}")
        
        print("="*70)
        
        # –ö—Ä–∞—Ç–∫–∞—è —Å–≤–æ–¥–∫–∞
        summary = self.results['summary']
        print(f"\nüèÜ –ò–¢–û–ì–û: {summary['total_return_pct']:.2f}% –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏ –∑–∞ {meta['total_steps']} —à–∞–≥–æ–≤")
        print(f"    –ü—Ä–µ–≤—ã—à–µ–Ω–∏–µ –Ω–∞–¥ Buy&Hold: {summary['vs_buy_hold']:.2f}%")
        print(f"    –†–∏—Å–∫-—Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å (Sharpe): {summary['sharpe_ratio']:.2f}")
    
    def get_results(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–µ–∫—Ç–µ—Å—Ç–∞."""
        return self.results.copy()
    
    def compare_with_baseline(self, baseline_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –±–∞–∑–æ–≤–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π.
        
        Args:
            baseline_results: —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–∞–∑–æ–≤–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
            
        Returns:
            –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        """
        if not self.results:
            raise ValueError("–°–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ –±–µ–∫—Ç–µ—Å—Ç")
        
        comparison = {
            'drl_return': self.results['performance']['total_return_pct'],
            'baseline_return': baseline_results.get('total_return_pct', 0),
            'outperformance': self.results['performance']['total_return_pct'] - baseline_results.get('total_return_pct', 0),
            'drl_sharpe': self.results['performance']['sharpe_ratio'],
            'baseline_sharpe': baseline_results.get('sharpe_ratio', 0),
            'drl_max_dd': self.results['performance']['max_drawdown_pct'],
            'baseline_max_dd': baseline_results.get('max_drawdown_pct', 0)
        }
        
        return comparison


def run_quick_backtest(
    agent: BaseAgent,
    config: TradingConfig,
    test_data: Optional[pd.DataFrame] = None,
    deterministic: bool = True
) -> Dict[str, Any]:
    """
    –ë—ã—Å—Ç—Ä—ã–π –∑–∞–ø—É—Å–∫ –±–µ–∫—Ç–µ—Å—Ç–∞.
    
    Args:
        agent: –æ–±—É—á–µ–Ω–Ω—ã–π –∞–≥–µ–Ω—Ç
        config: —Ç–æ—Ä–≥–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        test_data: —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        deterministic: –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Å—Ç–∏—á–Ω–∞—è –ø–æ–ª–∏—Ç–∏–∫–∞
        
    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–∫—Ç–µ—Å—Ç–∞
    """
    backtester = DRLBacktester(agent, config)
    return backtester.run_backtest(
        test_data=test_data,
        deterministic=deterministic,
        save_results=False
    )