"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ DRL-–∞–≥–µ–Ω—Ç–∞.
"""

import os
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from typing import Dict, List, Optional, Tuple
from datetime import datetime

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º –ø—Ä–æ–µ–∫—Ç–∞
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..', '..'))
sys.path.insert(0, project_root)

from CryptoTrade.ai.DRL.config.trading_config import TradingConfig
from CryptoTrade.ai.DRL.environment.trading_env import TradingEnv
from CryptoTrade.ai.DRL.agents.dqn_agent import DQNAgent
from CryptoTrade.ai.DRL.agents.ppo_agent import PPOAgent
from CryptoTrade.ai.DRL.environment.reward_schemes import TradingMetrics


class DRLEvaluator:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –æ–±—É—á–µ–Ω–Ω—ã—Ö DRL –∞–≥–µ–Ω—Ç–æ–≤."""
    
    def __init__(self, model_path: str, config: TradingConfig, agent_type: str = "PPO"):
        self.model_path = model_path
        self.config = config
        self.agent_type = agent_type
        self.agent = None
        self.results = {}
        
    def load_agent(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –æ–±—É—á–µ–Ω–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞."""
        env = TradingEnv(self.config)
        
        if self.agent_type.upper() == "DQN":
            self.agent = DQNAgent(self.config)
        elif self.agent_type.upper() == "PPO":
            self.agent = PPOAgent(self.config)
        else:
            raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø –∞–≥–µ–Ω—Ç–∞: {self.agent_type}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—Ö–æ–¥–∏–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—É—Ç—å
        found_model_path = self._find_model_path()
        if not found_model_path:
            raise FileNotFoundError(f"–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {self.model_path}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        self.agent.load(found_model_path, env)
        model_path = found_model_path
        print(f"‚úÖ –ê–≥–µ–Ω—Ç {self.agent_type} –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ {model_path}")
        return self.agent
    
    def get_available_models(self):
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π."""
        import glob
        
        models_dir = os.path.join("CryptoTrade", "ai", "DRL", "models")
        available_models = []
        
        if not os.path.exists(models_dir):
            print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –º–æ–¥–µ–ª–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {models_dir}")
            return available_models
        
        print(f"üîç –ü–æ–∏—Å–∫ –º–æ–¥–µ–ª–µ–π –≤: {models_dir}")
        
        # –ò—â–µ–º –≤—Å–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –º–æ–¥–µ–ª—è–º–∏
        for item in os.listdir(models_dir):
            model_dir = os.path.join(models_dir, item)
            if os.path.isdir(model_dir):
                model_info = {
                    'name': item,
                    'path': model_dir,
                    'models': [],
                    'checkpoints': []
                }
                
                # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏
                for model_name in ['best_model.zip', 'final_model.zip']:
                    model_path = os.path.join(model_dir, model_name)
                    if os.path.exists(model_path):
                        model_info['models'].append({
                            'type': model_name.replace('.zip', ''),
                            'path': model_path,
                            'size': os.path.getsize(model_path),
                            'modified': os.path.getmtime(model_path)
                        })
                
                # –ò—â–µ–º checkpoints
                checkpoint_dir = os.path.join(model_dir, 'checkpoints')
                if os.path.exists(checkpoint_dir):
                    checkpoint_files = glob.glob(f"{checkpoint_dir}/*.zip")
                    for checkpoint_file in sorted(checkpoint_files):
                        model_info['checkpoints'].append({
                            'type': 'checkpoint',
                            'name': os.path.basename(checkpoint_file),
                            'path': checkpoint_file,
                            'size': os.path.getsize(checkpoint_file),
                            'modified': os.path.getmtime(checkpoint_file)
                        })
                
                if model_info['models'] or model_info['checkpoints']:
                    available_models.append(model_info)
        
        return available_models
    
    def validate_model_compatibility(self, model_path: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ —Å —Ç–µ–∫—É—â–µ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π."""
        try:
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é —Å—Ä–µ–¥—É –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            temp_env = TradingEnv(self.config)
            
            # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å
            if self.agent_type.upper() == "DQN":
                temp_agent = DQNAgent(self.config)
            elif self.agent_type.upper() == "PPO":
                temp_agent = PPOAgent(self.config)
            else:
                return False
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≥—Ä—É–∑–∫—É –±–µ–∑ —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
            from stable_baselines3 import PPO, DQN
            
            if self.agent_type.upper() == "PPO":
                model = PPO.load(model_path, env=None)
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å observation space
                expected_shape = temp_env.observation_space.shape
                if hasattr(model, 'observation_space'):
                    actual_shape = model.observation_space.shape
                    return expected_shape == actual_shape
            elif self.agent_type.upper() == "DQN":
                model = DQN.load(model_path, env=None)
                expected_shape = temp_env.observation_space.shape
                if hasattr(model, 'observation_space'):
                    actual_shape = model.observation_space.shape
                    return expected_shape == actual_shape
            
            return True
            
        except Exception as e:
            print(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º–∞: {e}")
            return False
    
    def interactive_model_selection(self):
        """–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏."""
        available_models = self.get_available_models()
        
        if not available_models:
            print("‚ùå –ú–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
            print("üí° –°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å —Å –ø–æ–º–æ—â—å—é mvp_train.py")
            return None
        
        print(f"\nüìã –ù–∞–π–¥–µ–Ω–æ {len(available_models)} –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π —Å –º–æ–¥–µ–ª—è–º–∏:")
        print("=" * 80)
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏
        model_options = []
        option_counter = 1
        
        for model_info in available_models:
            print(f"\nüìÅ {model_info['name']}:")
            
            # –û—Å–Ω–æ–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: best_model > final_model)
            sorted_models = sorted(model_info['models'], 
                                 key=lambda x: 0 if x['type'] == 'best_model' else 1)
            
            for model in sorted_models:
                size_mb = model['size'] / (1024 * 1024)
                modified_time = pd.to_datetime(model['modified'], unit='s').strftime('%Y-%m-%d %H:%M')
                
                print(f"  {option_counter}. {model['type']}.zip ({size_mb:.1f} MB, {modified_time})")
                model_options.append({
                    'index': option_counter,
                    'path': model['path'],
                    'type': model['type'],
                    'dir_name': model_info['name']
                })
                option_counter += 1
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 3 checkpoint'–∞
            if model_info['checkpoints']:
                recent_checkpoints = sorted(model_info['checkpoints'], 
                                          key=lambda x: x['modified'], reverse=True)[:3]
                
                print(f"  üìä –ü–æ—Å–ª–µ–¥–Ω–∏–µ checkpoints:")
                for checkpoint in recent_checkpoints:
                    size_mb = checkpoint['size'] / (1024 * 1024)
                    modified_time = pd.to_datetime(checkpoint['modified'], unit='s').strftime('%Y-%m-%d %H:%M')
                    
                    print(f"    {option_counter}. {checkpoint['name']} ({size_mb:.1f} MB, {modified_time})")
                    model_options.append({
                        'index': option_counter,
                        'path': checkpoint['path'],
                        'type': 'checkpoint',
                        'dir_name': model_info['name']
                    })
                    option_counter += 1
        
        print("\n" + "=" * 80)
        
        # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –≤—ã–±–æ—Ä
        while True:
            try:
                choice = input(f"\n–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ (1-{len(model_options)}) –∏–ª–∏ 'q' –¥–ª—è –≤—ã—Ö–æ–¥–∞: ").strip()
                
                if choice.lower() == 'q':
                    print("‚ùå –û—Ü–µ–Ω–∫–∞ –æ—Ç–º–µ–Ω–µ–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
                    return None
                
                choice_idx = int(choice)
                if 1 <= choice_idx <= len(model_options):
                    selected_model = model_options[choice_idx - 1]
                    model_path = selected_model['path']
                    
                    print(f"\n‚úÖ –í—ã–±—Ä–∞–Ω–∞ –º–æ–¥–µ–ª—å: {selected_model['type']} –∏–∑ {selected_model['dir_name']}")
                    print(f"üìÅ –ü—É—Ç—å: {model_path}")
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
                    print("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏...")
                    if self.validate_model_compatibility(model_path):
                        print("‚úÖ –ú–æ–¥–µ–ª—å —Å–æ–≤–º–µ—Å—Ç–∏–º–∞ —Å —Ç–µ–∫—É—â–µ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π")
                        return model_path
                    else:
                        print("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º–∞ —Å —Ç–µ–∫—É—â–µ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π —Å—Ä–µ–¥—ã!")
                        print("üí° –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
                        print("  - –ò–∑–º–µ–Ω–∏–ª–∞—Å—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö")
                        print("  - –î—Ä—É–≥–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤")
                        print("  - –ù–µ—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å—Ä–µ–¥—ã")
                        
                        retry = input("–ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –¥—Ä—É–≥—É—é –º–æ–¥–µ–ª—å? (y/n): ").strip().lower()
                        if retry not in ['y', 'yes', '–¥–∞']:
                            print("üî¥ –û—Ü–µ–Ω–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ - –Ω–µ—Ç —Å–æ–≤–º–µ—Å—Ç–∏–º—ã—Ö –º–æ–¥–µ–ª–µ–π")
                            return None
                        continue
                    
                else:
                    print(f"‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä! –í–≤–µ–¥–∏—Ç–µ —á–∏—Å–ª–æ –æ—Ç 1 –¥–æ {len(model_options)}")
                    
            except ValueError:
                print("‚ùå –í–≤–µ–¥–∏—Ç–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ —á–∏—Å–ª–æ!")
            except KeyboardInterrupt:
                print("\n‚ùå –û—Ü–µ–Ω–∫–∞ –æ—Ç–º–µ–Ω–µ–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
                return None
    
    def _find_model_path(self):
        """–ù–∞–π—Ç–∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—É—Ç—å –∫ –º–æ–¥–µ–ª–∏."""
        # –ï—Å–ª–∏ –ø—É—Ç—å –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
        if os.path.isabs(self.model_path) and os.path.exists(self.model_path):
            print(f"üîç –ò—Å–ø–æ–ª—å–∑—É—é –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å: {self.model_path}")
            return self.model_path
        
        # –ï—Å–ª–∏ –ø—É—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π, –∏—â–µ–º –≤ CryptoTrade/ai/DRL/models/
        models_dir = os.path.join("CryptoTrade", "ai", "DRL", "models")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä—è–º—ã–µ –ø—É—Ç–∏
        possible_paths = [
            self.model_path,
            f"{self.model_path}.zip",
            os.path.join(models_dir, self.model_path),
            os.path.join(models_dir, f"{self.model_path}.zip"),
            os.path.join(models_dir, self.model_path, "best_model.zip"),
            os.path.join(models_dir, self.model_path, "final_model.zip"),
        ]
        
        for path in possible_paths:
            if os.path.exists(path):
                print(f"üîç –ù–∞–π–¥–µ–Ω–∞ –º–æ–¥–µ–ª—å: {path}")
                return path
        
        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –∑–∞–ø—É—Å–∫–∞–µ–º –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –≤—ã–±–æ—Ä
        print(f"ü§ñ –ú–æ–¥–µ–ª—å '{self.model_path}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏")
        print("üéØ –ó–∞–ø—É—Å–∫ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏...")
        return self.interactive_model_selection()
    
    def evaluate_episodes(self, env: TradingEnv, num_episodes: int = 10, 
                         deterministic: bool = True) -> Dict:
        """–û—Ü–µ–Ω–∏—Ç—å –∞–≥–µ–Ω—Ç–∞ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —ç–ø–∏–∑–æ–¥–∞—Ö."""
        if not self.agent:
            self.load_agent()
        
        episode_results = []
        all_actions = []
        
        print(f"üîÑ –ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏ –Ω–∞ {num_episodes} —ç–ø–∏–∑–æ–¥–∞—Ö...")
        
        for episode in range(num_episodes):
            obs, _ = env.reset()  # Gymnasium API returns (obs, info)
            episode_reward = 0
            episode_actions = []
            episode_steps = 0
            
            while True:
                action = self.agent.act(obs)
                all_actions.append(action[0])
                episode_actions.append(action[0])
                
                obs, reward, terminated, truncated, info = env.step(action)
                done = terminated or truncated
                episode_reward += reward
                episode_steps += 1
                
                if done:
                    break
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–ø–∏–∑–æ–¥–∞
            episode_result = {
                'episode': episode,
                'total_reward': episode_reward,
                'total_return': info.get('total_return', 0),
                'max_drawdown': info.get('max_drawdown', 0),
                'win_rate': info.get('win_rate', 0),
                'total_trades': info.get('total_trades', 0),
                'final_portfolio': info.get('portfolio_value', 0),
                'steps': episode_steps
            }
            episode_results.append(episode_result)
            
            print(f"  –≠–ø–∏–∑–æ–¥ {episode+1}: –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å={episode_result['total_return']:.2%}, "
                  f"–ø—Ä–æ—Å–∞–¥–∫–∞={episode_result['max_drawdown']:.2%}, "
                  f"—Å–¥–µ–ª–æ–∫={episode_result['total_trades']}")
        
        # –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        results = {
            'episodes': episode_results,
            'mean_reward': np.mean([ep['total_reward'] for ep in episode_results]),
            'mean_return': np.mean([ep['total_return'] for ep in episode_results]),
            'mean_drawdown': np.mean([ep['max_drawdown'] for ep in episode_results]),
            'mean_win_rate': np.mean([ep['win_rate'] for ep in episode_results]),
            'mean_trades': np.mean([ep['total_trades'] for ep in episode_results]),
            'std_return': np.std([ep['total_return'] for ep in episode_results]),
            'sharpe_ratio': self._calculate_sharpe_ratio(episode_results),
            'win_rate_episodes': sum(1 for ep in episode_results if ep['total_return'] > 0) / num_episodes,
            'all_actions': all_actions
        }
        
        self.results = results
        return results
    
    def _calculate_sharpe_ratio(self, episode_results: List[Dict]) -> float:
        """–†–∞—Å—Å—á–∏—Ç–∞—Ç—å –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –®–∞—Ä–ø–∞ –ø–æ —ç–ø–∏–∑–æ–¥–∞–º."""
        returns = [ep['total_return'] for ep in episode_results]
        if len(returns) < 2:
            return 0.0
        
        mean_return = np.mean(returns)
        std_return = np.std(returns)
        
        if std_return == 0:
            return 0.0
        
        return mean_return / std_return
    
    def create_detailed_report(self, save_path: Optional[str] = None) -> Dict:
        """–°–æ–∑–¥–∞—Ç—å –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –æ–± –æ—Ü–µ–Ω–∫–µ."""
        if not self.results:
            raise ValueError("–°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ evaluate_episodes()")
        
        report = {
            'model_info': {
                'model_path': self.model_path,
                'agent_type': self.agent_type,
                'symbol': self.config.symbol,
                'timeframe': self.config.timeframe,
                'reward_scheme': self.config.reward_scheme,
                'evaluation_date': datetime.now().isoformat()
            },
            'performance_metrics': {
                'mean_return': self.results['mean_return'],
                'std_return': self.results['std_return'],
                'sharpe_ratio': self.results['sharpe_ratio'],
                'mean_drawdown': self.results['mean_drawdown'],
                'mean_win_rate': self.results['mean_win_rate'],
                'win_rate_episodes': self.results['win_rate_episodes'],
                'mean_trades_per_episode': self.results['mean_trades']
            },
            'action_analysis': {
                'mean_action': np.mean(self.results['all_actions']),
                'std_action': np.std(self.results['all_actions']),
                'action_range': [np.min(self.results['all_actions']), np.max(self.results['all_actions'])],
                'buy_actions_pct': sum(1 for a in self.results['all_actions'] if a > 0.1) / len(self.results['all_actions']),
                'sell_actions_pct': sum(1 for a in self.results['all_actions'] if a < -0.1) / len(self.results['all_actions']),
                'hold_actions_pct': sum(1 for a in self.results['all_actions'] if abs(a) <= 0.1) / len(self.results['all_actions'])
            }
        }
        
        if save_path:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
            import json
            with open(f"{save_path}/evaluation_report.json", 'w') as f:
                json.dump(report, f, indent=2)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–ø–∏–∑–æ–¥–æ–≤
            episodes_df = pd.DataFrame(self.results['episodes'])
            episodes_df.to_csv(f"{save_path}/episode_results.csv", index=False)
            
            print(f"üìä –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {save_path}")
        
        return report
    
    def plot_results(self, save_path: Optional[str] = None):
        """–°–æ–∑–¥–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤."""
        if not self.results:
            raise ValueError("–°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ evaluate_episodes()")
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # –ì—Ä–∞—Ñ–∏–∫ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏ –ø–æ —ç–ø–∏–∑–æ–¥–∞–º
        episodes = [ep['episode'] for ep in self.results['episodes']]
        returns = [ep['total_return'] * 100 for ep in self.results['episodes']]
        
        axes[0, 0].plot(episodes, returns, 'b-o')
        axes[0, 0].axhline(y=0, color='r', linestyle='--', alpha=0.7)
        axes[0, 0].set_title('–î–æ—Ö–æ–¥–Ω–æ—Å—Ç—å –ø–æ —ç–ø–∏–∑–æ–¥–∞–º (%)')
        axes[0, 0].set_xlabel('–≠–ø–∏–∑–æ–¥')
        axes[0, 0].set_ylabel('–î–æ—Ö–æ–¥–Ω–æ—Å—Ç—å (%)')
        axes[0, 0].grid(True, alpha=0.3)
        
        # –ì—Ä–∞—Ñ–∏–∫ –ø—Ä–æ—Å–∞–¥–æ–∫
        drawdowns = [ep['max_drawdown'] * 100 for ep in self.results['episodes']]
        axes[0, 1].plot(episodes, drawdowns, 'r-o')
        axes[0, 1].set_title('–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–æ—Å–∞–¥–∫–∞ –ø–æ —ç–ø–∏–∑–æ–¥–∞–º (%)')
        axes[0, 1].set_xlabel('–≠–ø–∏–∑–æ–¥')
        axes[0, 1].set_ylabel('–ü—Ä–æ—Å–∞–¥–∫–∞ (%)')
        axes[0, 1].grid(True, alpha=0.3)
        
        # –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ –¥–µ–π—Å—Ç–≤–∏–π
        actions = self.results['all_actions']
        axes[1, 0].hist(actions, bins=50, alpha=0.7, edgecolor='black')
        axes[1, 0].axvline(x=0, color='r', linestyle='--', alpha=0.7)
        axes[1, 0].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏–π –∞–≥–µ–Ω—Ç–∞')
        axes[1, 0].set_xlabel('–î–µ–π—Å—Ç–≤–∏–µ (–æ—Ç -1 –¥–æ 1)')
        axes[1, 0].set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
        axes[1, 0].grid(True, alpha=0.3)
        
        # –ì—Ä–∞—Ñ–∏–∫ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–¥–µ–ª–æ–∫
        trades = [ep['total_trades'] for ep in self.results['episodes']]
        axes[1, 1].plot(episodes, trades, 'g-o')
        axes[1, 1].set_title('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–¥–µ–ª–æ–∫ –ø–æ —ç–ø–∏–∑–æ–¥–∞–º')
        axes[1, 1].set_xlabel('–≠–ø–∏–∑–æ–¥')
        axes[1, 1].set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–¥–µ–ª–æ–∫')
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(f"{save_path}/evaluation_plots.png", dpi=300, bbox_inches='tight')
            print(f"üìà –ì—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {save_path}/evaluation_plots.png")
        
        plt.show()
    
    def compare_with_baseline(self, baseline_strategy: str = "buy_hold") -> Dict:
        """–°—Ä–∞–≤–Ω–∏—Ç—å —Å –±–∞–∑–æ–≤–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π."""
        if not self.results:
            raise ValueError("–°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ evaluate_episodes()")
        
        # –°–æ–∑–¥–∞–µ–º —Å—Ä–µ–¥—É –¥–ª—è –±–∞–∑–æ–≤–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
        env = TradingEnv(self.config)
        obs = env.reset()
        
        if baseline_strategy == "buy_hold":
            # –°—Ç—Ä–∞—Ç–µ–≥–∏—è buy and hold
            action = np.array([1.0])  # –ü–æ–∫—É–ø–∞–µ–º –Ω–∞ –≤–µ—Å—å –∫–∞–ø–∏—Ç–∞–ª –≤ –Ω–∞—á–∞–ª–µ
            obs, reward, done, info = env.step(action)
            
            while not done:
                action = np.array([0.0])  # –î–µ—Ä–∂–∏–º
                obs, reward, done, info = env.step(action)
            
            baseline_return = info.get('total_return', 0)
            baseline_drawdown = info.get('max_drawdown', 0)
            
        else:
            baseline_return = 0
            baseline_drawdown = 0
        
        comparison = {
            'agent_return': self.results['mean_return'],
            'baseline_return': baseline_return,
            'outperformance': self.results['mean_return'] - baseline_return,
            'agent_drawdown': self.results['mean_drawdown'],
            'baseline_drawdown': baseline_drawdown,
            'risk_adjusted_performance': (self.results['mean_return'] - baseline_return) / max(self.results['mean_drawdown'], 0.01)
        }
        
        print(f"üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å {baseline_strategy}:")
        print(f"  –ê–≥–µ–Ω—Ç: {comparison['agent_return']:.2%}")
        print(f"  –ë–∞–∑–æ–≤–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è: {comparison['baseline_return']:.2%}")
        print(f"  –ü—Ä–µ–≤—ã—à–µ–Ω–∏–µ: {comparison['outperformance']:.2%}")
        
        return comparison


def quick_evaluate(model_path: str, symbol: str = "BTCUSDT", timeframe: str = "1d",
                  agent_type: str = "PPO", episodes: int = 10):
    """–ë—ã—Å—Ç—Ä–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏."""
    config = TradingConfig(
        symbol=symbol,
        timeframe=timeframe,
        reward_scheme='optimized'
    )
    
    evaluator = DRLEvaluator(model_path, config, agent_type)
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—É—é —Å—Ä–µ–¥—É
    env = TradingEnv(config)
    
    # –û—Ü–µ–Ω–∫–∞
    results = evaluator.evaluate_episodes(env, episodes)
    
    # –°–æ–∑–¥–∞–µ–º –æ—Ç—á–µ—Ç
    report = evaluator.create_detailed_report()
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –≥—Ä–∞—Ñ–∏–∫–∏
    evaluator.plot_results()
    
    return evaluator, results, report


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='–û—Ü–µ–Ω–∫–∞ DRL –∞–≥–µ–Ω—Ç–∞')
    parser.add_argument('model_path', help='–ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏')
    parser.add_argument('--symbol', default='BTCUSDT', help='–¢–æ—Ä–≥–æ–≤–∞—è –ø–∞—Ä–∞')
    parser.add_argument('--timeframe', default='1d', help='–¢–∞–π–º—Ñ—Ä–µ–π–º')
    parser.add_argument('--agent', default='PPO', choices=['PPO', 'DQN'], help='–¢–∏–ø –∞–≥–µ–Ω—Ç–∞')
    parser.add_argument('--episodes', type=int, default=10, help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–∏–∑–æ–¥–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏')
    
    args = parser.parse_args()
    
    quick_evaluate(
        model_path=args.model_path,
        symbol=args.symbol,
        timeframe=args.timeframe,
        agent_type=args.agent,
        episodes=args.episodes
    ) 