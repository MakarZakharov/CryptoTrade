"""
Callbacks –¥–ª—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è DRL –∞–≥–µ–Ω—Ç–æ–≤.
"""

import os
import numpy as np
import pandas as pd
from typing import Dict, Any
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.logger import Figure


class TradingCallback(BaseCallback):
    """Callback –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Ç–æ—Ä–≥–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫."""
    
    def __init__(self, log_dir: str, experiment_name: str, verbose: int = 1):
        super().__init__(verbose)
        self.log_dir = log_dir
        self.experiment_name = experiment_name
        self.episode_rewards = []
        self.episode_returns = []
        self.episode_drawdowns = []
        self.episode_win_rates = []
        self.episode_trades = []
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –ª–æ–≥–æ–≤
        os.makedirs(log_dir, exist_ok=True)
        
    def _on_step(self) -> bool:
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ."""
        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —Å—Ä–µ–¥—ã
        if len(self.locals.get('infos', [])) > 0:
            info = self.locals['infos'][0]
            
            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –≤ tensorboard
            if 'portfolio_value' in info:
                self.logger.record('trading/portfolio_value', info['portfolio_value'])
            if 'total_return' in info:
                self.logger.record('trading/total_return', info['total_return'])
            if 'max_drawdown' in info:
                self.logger.record('trading/max_drawdown', info['max_drawdown'])
            if 'win_rate' in info:
                self.logger.record('trading/win_rate', info['win_rate'])
            if 'total_trades' in info:
                self.logger.record('trading/total_trades', info['total_trades'])
        
        return True
    
    def _on_episode_end(self) -> None:
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –≤ –∫–æ–Ω—Ü–µ —ç–ø–∏–∑–æ–¥–∞."""
        if len(self.locals.get('infos', [])) > 0:
            info = self.locals['infos'][0]
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ —ç–ø–∏–∑–æ–¥–∞
            self.episode_returns.append(info.get('total_return', 0))
            self.episode_drawdowns.append(info.get('max_drawdown', 0))
            self.episode_win_rates.append(info.get('win_rate', 0))
            self.episode_trades.append(info.get('total_trades', 0))
            
            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            if len(self.episode_returns) > 0:
                self.logger.record('episode/mean_return', np.mean(self.episode_returns[-100:]))
                self.logger.record('episode/mean_drawdown', np.mean(self.episode_drawdowns[-100:]))
                self.logger.record('episode/mean_win_rate', np.mean(self.episode_win_rates[-100:]))
                self.logger.record('episode/mean_trades', np.mean(self.episode_trades[-100:]))
    
    def _on_training_end(self) -> None:
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –≤ –∫–æ–Ω—Ü–µ –æ–±—É—á–µ–Ω–∏—è."""
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Ç–æ–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        metrics = {
            'final_mean_return': np.mean(self.episode_returns[-50:]) if self.episode_returns else 0,
            'final_mean_drawdown': np.mean(self.episode_drawdowns[-50:]) if self.episode_drawdowns else 0,
            'final_mean_win_rate': np.mean(self.episode_win_rates[-50:]) if self.episode_win_rates else 0,
            'final_mean_trades': np.mean(self.episode_trades[-50:]) if self.episode_trades else 0,
            'total_episodes': len(self.episode_returns)
        }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV
        metrics_df = pd.DataFrame([metrics])
        metrics_df.to_csv(f"{self.log_dir}/final_metrics.csv", index=False)
        
        print(f"üìä –ò—Ç–æ–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏:")
        print(f"  –°—Ä–µ–¥–Ω—è—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å: {metrics['final_mean_return']:.2%}")
        print(f"  –°—Ä–µ–¥–Ω—è—è –ø—Ä–æ—Å–∞–¥–∫–∞: {metrics['final_mean_drawdown']:.2%}")
        print(f"  –°—Ä–µ–¥–Ω–∏–π win rate: {metrics['final_mean_win_rate']:.2%}")
        print(f"  –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–¥–µ–ª–æ–∫: {metrics['final_mean_trades']:.1f}")


class TensorboardCallback(BaseCallback):
    """Callback –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –≤ Tensorboard."""
    
    def __init__(self, log_dir: str, verbose: int = 1):
        super().__init__(verbose)
        self.log_dir = log_dir
        self.step_count = 0
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
        os.makedirs(log_dir, exist_ok=True)
    
    def _on_step(self) -> bool:
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ."""
        self.step_count += 1
        
        # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–µ 1000 —à–∞–≥–æ–≤
        if self.step_count % 1000 == 0:
            # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–≥—Ä–∞–¥—ã
            if 'rewards' in self.locals:
                rewards = self.locals['rewards']
                if len(rewards) > 0:
                    self.logger.record('reward/mean_reward', np.mean(rewards))
                    self.logger.record('reward/max_reward', np.max(rewards))
                    self.logger.record('reward/min_reward', np.min(rewards))
            
            # –õ–æ–≥–∏—Ä—É–µ–º –¥–µ–π—Å—Ç–≤–∏—è –∞–≥–µ–Ω—Ç–∞
            if 'actions' in self.locals:
                actions = self.locals['actions']
                if len(actions) > 0:
                    self.logger.record('action/mean_action', np.mean(actions))
                    self.logger.record('action/std_action', np.std(actions))
        
        return True


class EarlyStoppingCallback(BaseCallback):
    """Callback –¥–ª—è —Ä–∞–Ω–Ω–µ–≥–æ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –æ–±—É—á–µ–Ω–∏—è."""
    
    def __init__(self, patience: int = 50, min_improvement: float = 0.01, verbose: int = 1):
        super().__init__(verbose)
        self.patience = patience
        self.min_improvement = min_improvement
        self.best_mean_reward = -np.inf
        self.patience_counter = 0
    
    def _on_step(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å–ª–æ–≤–∏–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏."""
        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â—É—é —Å—Ä–µ–¥–Ω—é—é –Ω–∞–≥—Ä–∞–¥—É
        if len(self.model.ep_info_buffer) > 0:
            mean_reward = np.mean([ep_info['r'] for ep_info in self.model.ep_info_buffer])
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–ª—É—á—à–µ–Ω–∏–µ
            if mean_reward > self.best_mean_reward + self.min_improvement:
                self.best_mean_reward = mean_reward
                self.patience_counter = 0
                if self.verbose > 0:
                    print(f"üìà –ù–æ–≤—ã–π –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {mean_reward:.4f}")
            else:
                self.patience_counter += 1
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å–ª–æ–≤–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
            if self.patience_counter >= self.patience:
                if self.verbose > 0:
                    print(f"üõë –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞: –Ω–µ—Ç —É–ª—É—á—à–µ–Ω–∏–π {self.patience} —à–∞–≥–æ–≤")
                return False
        
        return True


class PerformanceMonitorCallback(BaseCallback):
    """Callback –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º—ã."""
    
    def __init__(self, log_freq: int = 10000, verbose: int = 1):
        super().__init__(verbose)
        self.log_freq = log_freq
        self.step_count = 0
        
        try:
            import psutil
            self.psutil_available = True
        except ImportError:
            self.psutil_available = False
            if verbose > 0:
                print("‚ö†Ô∏è psutil –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –æ—Ç–∫–ª—é—á–µ–Ω")
    
    def _on_step(self) -> bool:
        """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏."""
        self.step_count += 1
        
        if self.step_count % self.log_freq == 0 and self.psutil_available:
            import psutil
            
            # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–∞–º—è—Ç–∏
            memory_info = psutil.virtual_memory()
            self.logger.record('system/memory_usage_percent', memory_info.percent)
            self.logger.record('system/memory_available_gb', memory_info.available / 1024**3)
            
            # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ CPU
            cpu_percent = psutil.cpu_percent(interval=1)
            self.logger.record('system/cpu_usage_percent', cpu_percent)
            
            # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ GPU (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
            try:
                import torch
                if torch.cuda.is_available():
                    gpu_memory = torch.cuda.memory_allocated() / 1024**3
                    self.logger.record('system/gpu_memory_gb', gpu_memory)
            except ImportError:
                pass
        
        return True 