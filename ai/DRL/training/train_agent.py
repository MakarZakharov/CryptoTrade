#!/usr/bin/env python3
"""
–û—Å–Ω–æ–≤–Ω–æ–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è DRL –∞–≥–µ–Ω—Ç–æ–≤ —Ç–æ—Ä–≥–æ–≤–ª–∏ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–æ–π.
–†–µ–∞–ª–∏–∑—É–µ—Ç –ø–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –æ—Ç —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–æ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏.
"""

import os
import sys
import argparse
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, Any, Optional
import pandas as pd
import numpy as np

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º –ø—Ä–æ–µ–∫—Ç–∞
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from data_processing.data_collector import CryptoDataCollector, DataConfig
from data_processing.feature_engineering import FeatureEngineer, DataNormalizer
from environment.trading_env import create_trading_environment, TradingConfig
from agents.base_agent import AgentFactory, get_default_config
from evaluation.backtester import Backtester, BacktestConfig


class TrainingPipeline:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ–º –æ–±—É—á–µ–Ω–∏—è."""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = self._setup_logger()
        self.results = {}
        
    def _setup_logger(self) -> logging.Logger:
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è."""
        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –ª–æ–≥–æ–≤
        log_dir = 'logs'
        os.makedirs(log_dir, exist_ok=True)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞
        logger = logging.getLogger('TrainingPipeline')
        logger.setLevel(logging.INFO)
        
        # –§–∞–π–ª–æ–≤—ã–π handler
        log_file = os.path.join(log_dir, f'training_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')
        file_handler = logging.FileHandler(log_file)
        file_handler.setLevel(logging.INFO)
        
        # –ö–æ–Ω—Å–æ–ª—å–Ω—ã–π handler
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # –§–æ—Ä–º–∞—Ç—Ç–µ—Ä
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ handlers
        if not logger.handlers:
            logger.addHandler(file_handler)
            logger.addHandler(console_handler)
            
        return logger
    
    def run_full_pipeline(self) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ –æ–±—É—á–µ–Ω–∏—è."""
        self.logger.info("=" * 60)
        self.logger.info("–ó–ê–ü–£–°–ö –ü–û–õ–ù–û–ì–û –ü–ê–ô–ü–õ–ê–ô–ù–ê –û–ë–£–ß–ï–ù–ò–Ø DRL –ê–ì–ï–ù–¢–ê")
        self.logger.info("=" * 60)
        
        try:
            # –®–∞–≥ 1: –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö
            data = self._collect_data()
            if data.empty:
                raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –¥–∞–Ω–Ω—ã–µ")
            
            # –®–∞–≥ 2: –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            processed_data = self._preprocess_data(data)
            
            # –®–∞–≥ 3: –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
            train_data, val_data, test_data = self._split_data(processed_data)
            
            # –®–∞–≥ 4: –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ä–µ–¥—ã
            train_env = self._create_environment(train_data)
            
            # –®–∞–≥ 5: –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞
            agent = self._train_agent(train_env)
            
            # –®–∞–≥ 6: –û—Ü–µ–Ω–∫–∞ –∞–≥–µ–Ω—Ç–∞
            evaluation_results = self._evaluate_agent(agent, test_data)
            
            # –®–∞–≥ 7: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            self._save_results(agent, evaluation_results)
            
            self.logger.info("–ü–∞–π–ø–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
            return self.results
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –≤ –ø–∞–π–ø–ª–∞–π–Ω–µ –æ–±—É—á–µ–Ω–∏—è: {e}")
            raise
    
    def _collect_data(self) -> pd.DataFrame:
        """–°–±–æ—Ä –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö."""
        self.logger.info("–®–∞–≥ 1: –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö...")
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
        data_config = DataConfig(
            symbol=self.config['data']['symbol'],
            timeframe=self.config['data']['timeframe'],
            start_date=self.config['data']['start_date'],
            end_date=self.config['data'].get('end_date'),
            exchange=self.config['data']['exchange']
        )
        
        # –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö
        collector = CryptoDataCollector(data_config)
        data = collector.collect_ohlcv_data()
        
        if not data.empty:
            self.logger.info(f"–°–æ–±—Ä–∞–Ω–æ {len(data)} –∑–∞–ø–∏—Å–µ–π –¥–∞–Ω–Ω—ã—Ö")
            self.logger.info(f"–ü–µ—Ä–∏–æ–¥: {data.index.min()} - {data.index.max()}")
        else:
            self.logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –¥–∞–Ω–Ω—ã–µ")
        
        return data
    
    def _preprocess_data(self, data: pd.DataFrame) -> pd.DataFrame:
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤."""
        self.logger.info("–®–∞–≥ 2: –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        feature_engineer = FeatureEngineer()
        enhanced_data = feature_engineer.add_all_features(data)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        normalizer = DataNormalizer()
        normalized_data = normalizer.normalize_features(
            enhanced_data, 
            method=self.config['preprocessing'].get('normalization', 'minmax')
        )
        
        self.logger.info(f"–î–∞–Ω–Ω—ã–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã: {normalized_data.shape}")
        self.logger.info(f"–ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(normalized_data.columns)}")
        
        return normalized_data
    
    def _split_data(self, data: pd.DataFrame) -> tuple:
        """–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ train/val/test."""
        self.logger.info("–®–∞–≥ 3: –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...")
        
        train_ratio = self.config['data']['train_ratio']
        val_ratio = self.config['data']['val_ratio']
        
        n_total = len(data)
        n_train = int(n_total * train_ratio)
        n_val = int(n_total * val_ratio)
        
        train_data = data[:n_train]
        val_data = data[n_train:n_train + n_val]
        test_data = data[n_train + n_val:]
        
        self.logger.info(f"Train: {len(train_data)} –∑–∞–ø–∏—Å–µ–π")
        self.logger.info(f"Validation: {len(val_data)} –∑–∞–ø–∏—Å–µ–π")
        self.logger.info(f"Test: {len(test_data)} –∑–∞–ø–∏—Å–µ–π")
        
        return train_data, val_data, test_data
    
    def _create_environment(self, data: pd.DataFrame):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ—Ä–≥–æ–≤–æ–π —Å—Ä–µ–¥—ã."""
        self.logger.info("–®–∞–≥ 4: –°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ—Ä–≥–æ–≤–æ–π —Å—Ä–µ–¥—ã...")
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å—Ä–µ–¥—ã
        trading_config = TradingConfig(
            initial_balance=self.config['environment']['initial_balance'],
            transaction_fee=self.config['environment']['transaction_fee'],
            slippage=self.config['environment']['slippage'],
            lookback_window=self.config['environment']['lookback_window']
        )
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ä–µ–¥—ã
        env = create_trading_environment(
            data, 
            trading_config, 
            self.config['environment']['reward_function']
        )
        
        self.logger.info("–¢–æ—Ä–≥–æ–≤–∞—è —Å—Ä–µ–¥–∞ —Å–æ–∑–¥–∞–Ω–∞")
        self.logger.info(f"–ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π: {env.observation_space}")
        self.logger.info(f"–ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –¥–µ–π—Å—Ç–≤–∏–π: {env.action_space}")
        
        return env
    
    def _train_agent(self, env):
        """–û–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞."""
        self.logger.info("–®–∞–≥ 5: –û–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞...")
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç–∞
        agent_type = self.config['agent']['type']
        agent_config = get_default_config(agent_type)
        agent_config.update(self.config['agent'].get('params', {}))
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞
        agent = AgentFactory.create_agent(agent_type, env, agent_config)
        
        # –û–±—É—á–µ–Ω–∏–µ
        training_params = self.config['training']
        self.logger.info(f"–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è {agent_type} –Ω–∞ {training_params['total_timesteps']} —à–∞–≥–æ–≤")
        
        agent.train(
            total_timesteps=training_params['total_timesteps'],
            eval_freq=training_params.get('eval_freq', 10000),
            n_eval_episodes=training_params.get('n_eval_episodes', 5)
        )
        
        self.logger.info("–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
        return agent
    
    def _evaluate_agent(self, agent, test_data: pd.DataFrame) -> Dict[str, Any]:
        """–û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–∞."""
        self.logger.info("–®–∞–≥ 6: –û—Ü–µ–Ω–∫–∞ –∞–≥–µ–Ω—Ç–∞...")
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –±—ç–∫—Ç–µ—Å—Ç–∏–Ω–≥–∞
        backtest_config = BacktestConfig(
            initial_capital=self.config['evaluation']['initial_capital'],
            commission=self.config['evaluation']['commission'],
            benchmark=self.config['evaluation']['benchmark']
        )
        
        # –ó–∞–ø—É—Å–∫ –±—ç–∫—Ç–µ—Å—Ç–∏–Ω–≥–∞
        backtester = Backtester(backtest_config)
        results = backtester.run_backtest(agent, test_data)
        
        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        backtester.print_results()
        
        self.logger.info("–û—Ü–µ–Ω–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        return results
    
    def _save_results(self, agent, evaluation_results: Dict[str, Any]):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤."""
        self.logger.info("–®–∞–≥ 7: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –º–æ–¥–µ–ª–µ–π
        models_dir = 'models'
        os.makedirs(models_dir, exist_ok=True)
        
        # –ò–º—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
        experiment_name = f"{self.config['agent']['type']}_{self.config['data']['symbol']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        model_path = os.path.join(models_dir, experiment_name)
        agent.save(model_path)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        results_data = {
            'experiment_name': experiment_name,
            'config': self.config,
            'evaluation_results': evaluation_results,
            'agent_stats': agent.get_training_stats(),
            'timestamp': datetime.now().isoformat()
        }
        
        results_file = model_path + '_results.json'
        with open(results_file, 'w') as f:
            json.dump(results_data, f, indent=2, default=str)
        
        self.results = results_data
        
        self.logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
        self.logger.info(f"  –ú–æ–¥–µ–ª—å: {model_path}")
        self.logger.info(f"  –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {results_file}")


def create_default_config() -> Dict[str, Any]:
    """–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é."""
    return {
        'data': {
            'symbol': 'BTC/USDT',
            'timeframe': '1h',
            'start_date': '2023-01-01',
            'end_date': None,
            'exchange': 'binance',
            'train_ratio': 0.7,
            'val_ratio': 0.15
        },
        'preprocessing': {
            'normalization': 'minmax'
        },
        'environment': {
            'initial_balance': 10000.0,
            'transaction_fee': 0.001,
            'slippage': 0.0005,
            'lookback_window': 50,
            'reward_function': 'profit_based'
        },
        'agent': {
            'type': 'PPO',
            'params': {}
        },
        'training': {
            'total_timesteps': 100000,
            'eval_freq': 10000,
            'n_eval_episodes': 5
        },
        'evaluation': {
            'initial_capital': 10000.0,
            'commission': 0.001,
            'benchmark': 'buy_and_hold'
        }
    }


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    parser = argparse.ArgumentParser(description='–û–±—É—á–µ–Ω–∏–µ DRL –∞–≥–µ–Ω—Ç–∞ –¥–ª—è —Ç–æ—Ä–≥–æ–≤–ª–∏ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–æ–π')
    
    parser.add_argument('--config', type=str, help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ JSON')
    parser.add_argument('--symbol', type=str, default='BTC/USDT', help='–¢–æ—Ä–≥–æ–≤–∞—è –ø–∞—Ä–∞')
    parser.add_argument('--timeframe', type=str, default='1h', help='–¢–∞–π–º—Ñ—Ä–µ–π–º')
    parser.add_argument('--algorithm', type=str, default='PPO', choices=['PPO', 'A2C', 'DDPG', 'DQN'], help='–ê–ª–≥–æ—Ä–∏—Ç–º DRL')
    parser.add_argument('--timesteps', type=int, default=100000, help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è')
    parser.add_argument('--start-date', type=str, default='2023-01-01', help='–ù–∞—á–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞ (YYYY-MM-DD)')
    parser.add_argument('--exchange', type=str, default='binance', help='–ë–∏—Ä–∂–∞')
    
    args = parser.parse_args()
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    if args.config:
        with open(args.config, 'r') as f:
            config = json.load(f)
    else:
        config = create_default_config()
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
        config['data']['symbol'] = args.symbol
        config['data']['timeframe'] = args.timeframe
        config['data']['start_date'] = args.start_date
        config['data']['exchange'] = args.exchange
        config['agent']['type'] = args.algorithm
        config['training']['total_timesteps'] = args.timesteps
    
    print("üöÄ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è DRL –∞–≥–µ–Ω—Ç–∞ –¥–ª—è —Ç–æ—Ä–≥–æ–≤–ª–∏ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–æ–π")
    print(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:")
    print(f"  –°–∏–º–≤–æ–ª: {config['data']['symbol']}")
    print(f"  –¢–∞–π–º—Ñ—Ä–µ–π–º: {config['data']['timeframe']}")
    print(f"  –ê–ª–≥–æ—Ä–∏—Ç–º: {config['agent']['type']}")
    print(f"  –®–∞–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è: {config['training']['total_timesteps']:,}")
    print("=" * 60)
    
    # –ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞
    pipeline = TrainingPipeline(config)
    
    try:
        results = pipeline.run_full_pipeline()
        
        print("\nüéâ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û –£–°–ü–ï–®–ù–û!")
        print(f"–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç: {results['experiment_name']}")
        
        # –ö—Ä–∞—Ç–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        eval_results = results['evaluation_results']
        print(f"–û–±—â–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å: {eval_results['total_return']:.2%}")
        print(f"–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –®–∞—Ä–ø–∞: {eval_results['sharpe_ratio']:.2f}")
        print(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–æ—Å–∞–¥–∫–∞: {eval_results['max_drawdown']:.2%}")
        
    except Exception as e:
        print(f"\n‚ùå –û–®–ò–ë–ö–ê: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())