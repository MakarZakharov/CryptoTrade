#!/usr/bin/env python3
"""
Professional DRL Trading Agent Evaluation Script
Comprehensive backtesting and performance analysis

Features:
- Out-of-sample testing on 2023-2024 data
- Comprehensive financial metrics (Sharpe, Drawdown, etc.)
- Benchmark comparisons (Buy&Hold, Random, MA Crossover)
- Professional visualizations and reports
- Reproducible evaluation with seeding
"""

import os
import sys
import yaml
import argparse
import numpy as np
import pandas as pd
from pathlib import Path
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Add project root to path
project_root = Path(__file__).parent.parent.parent.parent.parent  # Go up to 'trading' directory
sys.path.insert(0, str(project_root))
print(f"üîç Project root set to: {project_root}")
print(f"üîç Current directory: {Path.cwd()}")

# DRL imports
try:
    from stable_baselines3 import PPO
    from stable_baselines3.common.utils import set_random_seed
    print("‚úÖ Stable-Baselines3 imported successfully")
except ImportError as e:
    print(f"‚ùå Error importing Stable-Baselines3: {e}")
    sys.exit(1)

# Environment imports
try:
    # Try absolute import first
    from CryptoTrade.ai.DRL.environment.environment import create_trading_environment
    print("‚úÖ Trading environment imported successfully")
except ImportError:
    try:
        # Try relative import from parent directory
        sys.path.append(str(project_root / "CryptoTrade" / "ai" / "DRL"))
        from environment.environment import create_trading_environment
        print("‚úÖ Trading environment imported successfully (relative path)")
    except ImportError as e:
        print(f"‚ùå Error importing trading environment: {e}")
        print(f"üîç Project root: {project_root}")
        print(f"üîç Current working directory: {Path.cwd()}")
        sys.exit(1)

# Local imports
from utils.metrics import ProfessionalMetricsCalculator, BenchmarkCalculator, MetricsVisualizer
try:
    import matplotlib.pyplot as plt
    import seaborn as sns
    PLOTTING_AVAILABLE = True
    # Set plotting style
    plt.style.use('seaborn-v0_8')
    sns.set_palette("husl")
except ImportError:
    PLOTTING_AVAILABLE = False
    plt = None
    sns = None
    print("‚ö†Ô∏è Matplotlib/Seaborn not available - visualizations will be limited")


class ProfessionalEvaluator:
    """
    Professional evaluation system for DRL trading agents
    
    Implements comprehensive backtesting with:
    - Out-of-sample testing (2023-2024 data)
    - Multiple performance metrics
    - Benchmark strategy comparisons
    - Professional visualization and reporting
    - Statistical significance testing
    """
    
    def __init__(self, config_path: str = "configs/ppo_config.yaml"):
        """Initialize professional evaluator"""
        self.config_path = Path(config_path)
        self.config = self._load_config()
        self.setup_directories()
        
        # Set random seed for reproducibility
        self.seed = self.config['training']['seed']
        set_random_seed(self.seed)
        np.random.seed(self.seed)
        
        # Initialize metrics calculator
        self.metrics_calculator = ProfessionalMetricsCalculator()
        
        print(f"üî¨ Professional DRL Evaluator Initialized")
        print(f"üìÅ Config: {self.config_path}")
        print(f"üå± Seed: {self.seed}")
        
    def _load_config(self) -> dict:
        """Load configuration from YAML file"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            print(f"‚úÖ Configuration loaded from {self.config_path}")
            return config
        except Exception as e:
            print(f"‚ùå Error loading config: {e}")
            sys.exit(1)
    
    def setup_directories(self):
        """Create necessary directories for results"""
        base_dir = Path(".")
        
        dirs_to_create = [
            base_dir / self.config['paths']['results_dir'],
            base_dir / self.config['paths']['results_dir'] / "plots",
            base_dir / self.config['paths']['results_dir'] / "reports",
            base_dir / self.config['paths']['results_dir'] / "data"
        ]
        
        for dir_path in dirs_to_create:
            dir_path.mkdir(parents=True, exist_ok=True)
            
        print(f"‚úÖ Results directories created")
    
    def create_test_environment(self):
        """Create test environment with out-of-sample data"""
        print("üèóÔ∏è Creating test environment...")
        
        # Test environment configuration (2023-2024 data)
        test_config = self.config['environment'].copy()
        test_config.update({
            'data_split': 'test',
            'start_date': self.config['data']['test_start'],
            'end_date': self.config['data']['test_end'],
            'max_steps': 10000  # Extended episodes for comprehensive testing
        })
        
        try:
            self.test_env = create_trading_environment(test_config)
            
            print(f"‚úÖ Test environment created successfully")
            print(f"üìÖ Test period: {self.config['data']['test_start']} to {self.config['data']['test_end']}")
            print(f"üìä Observation space: {self.test_env.observation_space}")
            print(f"üéÆ Action space: {self.test_env.action_space}")
            
            return self.test_env
            
        except Exception as e:
            print(f"‚ùå Error creating test environment: {e}")
            raise
    
    def load_model(self, model_path: str):
        """Load trained PPO model"""
        print(f"ü§ñ Loading model from: {model_path}")
        
        try:
            self.model = PPO.load(model_path)
            print(f"‚úÖ Model loaded successfully")
            return self.model
        except Exception as e:
            print(f"‚ùå Error loading model: {e}")
            raise
    
    def run_backtest(self, n_episodes: int = 10, deterministic: bool = True) -> dict:
        """
        Run comprehensive backtest on out-of-sample data
        
        Args:
            n_episodes: Number of episodes to run
            deterministic: Use deterministic policy
            
        Returns:
            Dictionary with detailed backtest results
        """
        print(f"\nüìà RUNNING COMPREHENSIVE BACKTEST")
        print(f"üéØ Episodes: {n_episodes}")
        print(f"üé≤ Deterministic: {deterministic}")
        print("=" * 50)
        
        # Store results
        episode_results = []
        all_portfolio_values = []
        all_actions = []
        all_trades = []
        
        for episode in range(n_episodes):
            print(f"üìä Running episode {episode + 1}/{n_episodes}...")
            
            obs, info = self.test_env.reset()
            episode_portfolio_values = [info.get('portfolio_value', 100000)]
            episode_actions = []
            episode_trades = []
            episode_reward = 0
            step_count = 0
            
            while True:
                # Get action from model
                action, _ = self.model.predict(obs, deterministic=deterministic)
                obs, reward, terminated, truncated, info = self.test_env.step(action)
                
                # Store data
                episode_portfolio_values.append(info.get('portfolio_value', episode_portfolio_values[-1]))
                episode_actions.append(action)
                episode_reward += reward
                step_count += 1
                
                # Check for completion
                if terminated or truncated:
                    break
            
            # Collect episode trades if available
            if hasattr(self.test_env, 'trades_history'):
                episode_trades = self.test_env.trades_history.copy()
                all_trades.extend(episode_trades)
            
            # Store episode results
            episode_result = {
                'episode': episode,
                'portfolio_values': episode_portfolio_values,
                'actions': episode_actions,
                'trades': episode_trades,
                'final_portfolio_value': episode_portfolio_values[-1],
                'total_return': (episode_portfolio_values[-1] - episode_portfolio_values[0]) / episode_portfolio_values[0],
                'episode_reward': episode_reward,
                'steps': step_count
            }
            
            episode_results.append(episode_result)
            all_portfolio_values.extend(episode_portfolio_values)
            all_actions.extend(episode_actions)
            
            print(f"   ‚úÖ Episode {episode + 1} completed: "
                  f"Return={episode_result['total_return']:.2%}, "
                  f"Final Value=${episode_result['final_portfolio_value']:,.2f}")
        
        print(f"\n‚úÖ Backtest completed: {n_episodes} episodes")
        
        return {
            'episode_results': episode_results,
            'all_portfolio_values': all_portfolio_values,
            'all_actions': all_actions,
            'all_trades': all_trades,
            'n_episodes': n_episodes,
            'initial_capital': 100000.0  # From config
        }
    
    def calculate_comprehensive_metrics(self, backtest_results: dict) -> dict:
        """Calculate comprehensive performance metrics"""
        print("üìä Calculating comprehensive performance metrics...")
        
        portfolio_values = backtest_results['all_portfolio_values']
        trades = backtest_results['all_trades']
        initial_capital = backtest_results['initial_capital']
        
        # Calculate main metrics
        metrics = self.metrics_calculator.calculate_comprehensive_metrics(
            portfolio_values=portfolio_values,
            trades=trades,
            initial_capital=initial_capital
        )
        
        # Additional episode-level metrics
        episode_returns = [ep['total_return'] for ep in backtest_results['episode_results']]
        episode_rewards = [ep['episode_reward'] for ep in backtest_results['episode_results']]
        
        additional_metrics = {
            'episode_metrics': {
                'mean_episode_return': np.mean(episode_returns),
                'std_episode_return': np.std(episode_returns),
                'min_episode_return': np.min(episode_returns),
                'max_episode_return': np.max(episode_returns),
                'mean_episode_reward': np.mean(episode_rewards),
                'std_episode_reward': np.std(episode_rewards),
                'consistent_episodes': sum(1 for ret in episode_returns if ret > 0),
                'consistency_ratio': sum(1 for ret in episode_returns if ret > 0) / len(episode_returns)
            }
        }
        
        print(f"‚úÖ Metrics calculated:")
        print(f"   üìà Total Return: {metrics.total_return:.2%}")
        print(f"   ‚ö° Sharpe Ratio: {metrics.sharpe_ratio:.3f}")
        print(f"   üìâ Max Drawdown: {metrics.max_drawdown:.2%}")
        print(f"   üéØ Win Rate: {metrics.win_rate:.2%}")
        print(f"   üíº Total Trades: {metrics.total_trades}")
        
        return {
            'main_metrics': metrics,
            'additional_metrics': additional_metrics
        }
    
    def run_benchmark_comparisons(self, backtest_results: dict) -> dict:
        """Run benchmark strategy comparisons"""
        print("üèÜ Running benchmark comparisons...")
        
        # Extract price data from portfolio values
        # Note: This is a simplification - in practice, you'd want actual price data
        portfolio_values = backtest_results['all_portfolio_values']
        initial_capital = backtest_results['initial_capital']
        
        # Generate synthetic price series for benchmarks
        # In production, you'd use actual market prices
        synthetic_prices = []
        for i, pv in enumerate(portfolio_values):
            # Convert portfolio performance to price-like series
            if i == 0:
                synthetic_prices.append(50000)  # BTC-like initial price
            else:
                price_change = (pv - portfolio_values[i-1]) / portfolio_values[i-1]
                new_price = synthetic_prices[-1] * (1 + price_change * 0.1)  # Dampened
                synthetic_prices.append(new_price)
        
        benchmarks = {}
        
        try:
            # Buy and Hold
            buy_hold_values = BenchmarkCalculator.calculate_buy_and_hold(
                synthetic_prices, initial_capital
            )
            buy_hold_metrics = self.metrics_calculator.calculate_comprehensive_metrics(
                buy_hold_values, initial_capital=initial_capital
            )
            benchmarks['buy_and_hold'] = {
                'values': buy_hold_values,
                'metrics': buy_hold_metrics
            }
            print(f"   ‚úÖ Buy & Hold: {buy_hold_metrics.total_return:.2%} return")
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è Buy & Hold benchmark failed: {e}")
        
        try:
            # Random Trading
            random_values = BenchmarkCalculator.calculate_random_trading(
                synthetic_prices, initial_capital, seed=self.seed
            )
            random_metrics = self.metrics_calculator.calculate_comprehensive_metrics(
                random_values, initial_capital=initial_capital
            )
            benchmarks['random_trading'] = {
                'values': random_values,
                'metrics': random_metrics
            }
            print(f"   ‚úÖ Random Trading: {random_metrics.total_return:.2%} return")
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è Random Trading benchmark failed: {e}")
        
        try:
            # Simple MA Crossover
            ma_values = BenchmarkCalculator.calculate_simple_ma_crossover(
                synthetic_prices, initial_capital
            )
            ma_metrics = self.metrics_calculator.calculate_comprehensive_metrics(
                ma_values, initial_capital=initial_capital
            )
            benchmarks['ma_crossover'] = {
                'values': ma_values,
                'metrics': ma_metrics
            }
            print(f"   ‚úÖ MA Crossover: {ma_metrics.total_return:.2%} return")
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è MA Crossover benchmark failed: {e}")
        
        return benchmarks
    
    def create_comprehensive_visualizations(self, 
                                          backtest_results: dict, 
                                          metrics: dict, 
                                          benchmarks: dict) -> list:
        """Create comprehensive visualization suite"""
        print("üé® Creating comprehensive visualizations...")
        
        generated_plots = []
        results_dir = Path(self.config['paths']['results_dir'])
        plots_dir = results_dir / "plots"
        
        try:
            # 1. Performance Dashboard
            main_metrics = metrics['main_metrics']
            portfolio_values = backtest_results['all_portfolio_values']
            
            # Get benchmark values for comparison
            benchmark_values = None
            if 'buy_and_hold' in benchmarks:
                benchmark_values = benchmarks['buy_and_hold']['values']
            
            dashboard_fig = MetricsVisualizer.create_performance_dashboard(
                metrics=main_metrics,
                portfolio_values=portfolio_values,
                benchmark_values=benchmark_values,
                title=f"DRL Agent Performance - {self.config['environment']['symbols'][0]}"
            )
            
            dashboard_path = plots_dir / "performance_dashboard.png"
            dashboard_fig.savefig(dashboard_path, dpi=300, bbox_inches='tight')
            plt.close(dashboard_fig)
            generated_plots.append(dashboard_path)
            print(f"   ‚úÖ Performance dashboard: {dashboard_path}")
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è Performance dashboard failed: {e}")
        
        try:
            # 2. Benchmark Comparison Plot
            fig, axes = plt.subplots(2, 2, figsize=(16, 12))
            
            # Equity curves comparison
            ax1 = axes[0, 0]
            steps = range(len(portfolio_values))
            ax1.plot(steps, portfolio_values, label='DRL Agent', linewidth=2, color='blue')
            
            colors = ['gray', 'red', 'green']
            for i, (name, benchmark) in enumerate(benchmarks.items()):
                if len(benchmark['values']) == len(portfolio_values):
                    label = name.replace('_', ' ').title()
                    ax1.plot(steps, benchmark['values'], 
                            label=label, linewidth=1, color=colors[i % len(colors)], alpha=0.7)
            
            ax1.set_title('Strategy Comparison - Equity Curves')
            ax1.set_xlabel('Time Steps')
            ax1.set_ylabel('Portfolio Value ($)')
            ax1.legend()
            ax1.grid(True, alpha=0.3)
            
            # Returns comparison
            ax2 = axes[0, 1]
            strategy_names = ['DRL Agent']
            returns = [main_metrics.total_return]
            colors_bar = ['blue']
            
            for name, benchmark in benchmarks.items():
                strategy_names.append(name.replace('_', ' ').title())
                returns.append(benchmark['metrics'].total_return)
                colors_bar.append('gray')
            
            bars = ax2.bar(strategy_names, [r * 100 for r in returns], color=colors_bar, alpha=0.7)
            ax2.set_title('Total Returns Comparison')
            ax2.set_ylabel('Return (%)')
            ax2.tick_params(axis='x', rotation=45)
            
            # Add value labels on bars
            for bar, ret in zip(bars, returns):
                height = bar.get_height()
                ax2.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                        f'{ret:.1%}', ha='center', va='bottom')
            
            # Sharpe Ratio comparison
            ax3 = axes[1, 0]
            sharpe_ratios = [main_metrics.sharpe_ratio]
            for name, benchmark in benchmarks.items():
                sharpe_ratios.append(benchmark['metrics'].sharpe_ratio)
            
            bars = ax3.bar(strategy_names, sharpe_ratios, color=colors_bar, alpha=0.7)
            ax3.set_title('Sharpe Ratio Comparison')
            ax3.set_ylabel('Sharpe Ratio')
            ax3.tick_params(axis='x', rotation=45)
            
            # Max Drawdown comparison
            ax4 = axes[1, 1] 
            drawdowns = [main_metrics.max_drawdown * 100]
            for name, benchmark in benchmarks.items():
                drawdowns.append(benchmark['metrics'].max_drawdown * 100)
            
            bars = ax4.bar(strategy_names, drawdowns, color=colors_bar, alpha=0.7)
            ax4.set_title('Maximum Drawdown Comparison')
            ax4.set_ylabel('Max Drawdown (%)')
            ax4.tick_params(axis='x', rotation=45)
            
            plt.tight_layout()
            
            comparison_path = plots_dir / "benchmark_comparison.png"
            fig.savefig(comparison_path, dpi=300, bbox_inches='tight')
            plt.close(fig)
            generated_plots.append(comparison_path)
            print(f"   ‚úÖ Benchmark comparison: {comparison_path}")
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è Benchmark comparison plot failed: {e}")
        
        try:
            # 3. Trade Analysis
            if backtest_results['all_trades']:
                trades_fig = MetricsVisualizer.create_trades_analysis(backtest_results['all_trades'])
                trades_path = plots_dir / "trades_analysis.png"
                trades_fig.savefig(trades_path, dpi=300, bbox_inches='tight')
                plt.close(trades_fig)
                generated_plots.append(trades_path)
                print(f"   ‚úÖ Trades analysis: {trades_path}")
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è Trades analysis failed: {e}")
        
        return generated_plots
    
    def generate_comprehensive_report(self, 
                                    backtest_results: dict, 
                                    metrics: dict, 
                                    benchmarks: dict,
                                    plots: list) -> str:
        """Generate comprehensive evaluation report"""
        print("üìù Generating comprehensive report...")
        
        results_dir = Path(self.config['paths']['results_dir'])
        reports_dir = results_dir / "reports"
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_path = reports_dir / f"evaluation_report_{timestamp}.md"
        
        main_metrics = metrics['main_metrics']
        additional_metrics = metrics['additional_metrics']
        
        # Generate report content
        report_content = f"""# Professional DRL Trading Agent Evaluation Report

**Generated:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}  
**Symbol:** {self.config['environment']['symbols'][0]}  
**Timeframe:** {self.config['environment']['timeframe']}  
**Test Period:** {self.config['data']['test_start']} to {self.config['data']['test_end']}  
**Episodes:** {backtest_results['n_episodes']}

---

## Executive Summary

The DRL trading agent achieved the following performance on out-of-sample data:

- **Total Return:** {main_metrics.total_return:.2%}
- **Annualized Return:** {main_metrics.annualized_return:.2%}
- **Sharpe Ratio:** {main_metrics.sharpe_ratio:.3f}
- **Maximum Drawdown:** {main_metrics.max_drawdown:.2%}
- **Win Rate:** {main_metrics.win_rate:.2%}

---

## Detailed Performance Metrics

### Return Metrics
- **Total Return:** {main_metrics.total_return:.2%}
- **Annualized Return:** {main_metrics.annualized_return:.2%}
- **Cumulative Return:** {main_metrics.cumulative_return:.2%}

### Risk Metrics
- **Volatility:** {main_metrics.volatility:.2%}
- **Maximum Drawdown:** {main_metrics.max_drawdown:.2%}
- **Max Drawdown Duration:** {main_metrics.max_drawdown_duration} periods
- **Value at Risk (95%):** {main_metrics.var_95:.4f}
- **Conditional VaR (95%):** {main_metrics.cvar_95:.4f}

### Risk-Adjusted Metrics
- **Sharpe Ratio:** {main_metrics.sharpe_ratio:.3f}
- **Sortino Ratio:** {main_metrics.sortino_ratio:.3f}
- **Calmar Ratio:** {main_metrics.calmar_ratio:.3f}
- **Information Ratio:** {main_metrics.information_ratio:.3f}

### Trading Metrics
- **Win Rate:** {main_metrics.win_rate:.2%}
- **Profit Factor:** {main_metrics.profit_factor:.2f}
- **Average Win:** {main_metrics.avg_win:.4f}
- **Average Loss:** {main_metrics.avg_loss:.4f}
- **Largest Win:** {main_metrics.largest_win:.4f}
- **Largest Loss:** {main_metrics.largest_loss:.4f}
- **Total Trades:** {main_metrics.total_trades}
- **Average Trade Duration:** {main_metrics.avg_trade_duration:.1f}

### Advanced Metrics
- **Kelly Criterion:** {main_metrics.kelly_criterion:.3f}
- **Expectancy:** {main_metrics.expectancy:.4f}
- **Recovery Factor:** {main_metrics.recovery_factor:.2f}
- **Payoff Ratio:** {main_metrics.payoff_ratio:.2f}

---

## Episode-Level Analysis

- **Mean Episode Return:** {additional_metrics['episode_metrics']['mean_episode_return']:.2%}
- **Episode Return Std:** {additional_metrics['episode_metrics']['std_episode_return']:.2%}
- **Min Episode Return:** {additional_metrics['episode_metrics']['min_episode_return']:.2%}
- **Max Episode Return:** {additional_metrics['episode_metrics']['max_episode_return']:.2%}
- **Consistent Episodes:** {additional_metrics['episode_metrics']['consistent_episodes']}/{backtest_results['n_episodes']}
- **Consistency Ratio:** {additional_metrics['episode_metrics']['consistency_ratio']:.2%}

---

## Benchmark Comparison

"""
        
        # Add benchmark comparisons
        if benchmarks:
            report_content += "| Strategy | Total Return | Sharpe Ratio | Max Drawdown | Win Rate |\n"
            report_content += "|----------|--------------|--------------|--------------|----------|\n"
            report_content += f"| **DRL Agent** | **{main_metrics.total_return:.2%}** | **{main_metrics.sharpe_ratio:.3f}** | **{main_metrics.max_drawdown:.2%}** | **{main_metrics.win_rate:.2%}** |\n"
            
            for name, benchmark in benchmarks.items():
                bm = benchmark['metrics']
                strategy_name = name.replace('_', ' ').title()
                report_content += f"| {strategy_name} | {bm.total_return:.2%} | {bm.sharpe_ratio:.3f} | {bm.max_drawdown:.2%} | {bm.win_rate:.2%} |\n"
        
        report_content += f"""

---

## Risk Assessment

### Risk Level: {"üî¥ HIGH" if main_metrics.max_drawdown < -0.2 else "üü° MEDIUM" if main_metrics.max_drawdown < -0.1 else "üü¢ LOW"}

**Key Risk Factors:**
- Maximum drawdown of {main_metrics.max_drawdown:.2%} {"exceeds" if main_metrics.max_drawdown < -0.2 else "is within"} acceptable thresholds
- Sharpe ratio of {main_metrics.sharpe_ratio:.3f} indicates {"poor" if main_metrics.sharpe_ratio < 0.5 else "moderate" if main_metrics.sharpe_ratio < 1.0 else "good"} risk-adjusted returns
- Win rate of {main_metrics.win_rate:.2%} shows {"concerning" if main_metrics.win_rate < 0.4 else "acceptable" if main_metrics.win_rate < 0.6 else "good"} trade success rate

---

## Recommendations

### Strategy Performance
{"‚úÖ RECOMMEND FOR PRODUCTION" if main_metrics.sharpe_ratio > 1.0 and main_metrics.max_drawdown > -0.15 else "‚ö†Ô∏è REQUIRES OPTIMIZATION" if main_metrics.sharpe_ratio > 0.5 else "‚ùå NOT RECOMMENDED"}

### Specific Recommendations:
"""

        # Add specific recommendations based on performance
        if main_metrics.sharpe_ratio < 0.5:
            report_content += "- **Low Sharpe Ratio:** Consider improving risk-adjusted returns through better risk management\n"
        
        if main_metrics.max_drawdown < -0.2:
            report_content += "- **High Drawdown:** Implement stronger position sizing and stop-loss mechanisms\n"
        
        if main_metrics.win_rate < 0.4:
            report_content += "- **Low Win Rate:** Improve entry/exit timing or signal quality\n"
        
        if main_metrics.total_trades < 10:
            report_content += "- **Low Trading Frequency:** Consider more active trading or longer evaluation period\n"
        
        report_content += f"""

---

## Technical Details

- **Model Type:** PPO (Proximal Policy Optimization)
- **Training Timesteps:** {self.config['training']['total_timesteps']:,}
- **Evaluation Seed:** {self.seed}
- **Environment Features:** Advanced liquidity modeling, realistic slippage, commission modeling

---

## Visualizations

Generated plots are available in the `plots/` directory:
"""
        
        # Add plot references
        for plot_path in plots:
            plot_name = plot_path.name.replace('_', ' ').replace('.png', '').title()
            report_content += f"- {plot_name}: `{plot_path.name}`\n"
        
        report_content += f"""

---

## Reproducibility

This evaluation can be reproduced using:
```bash
python evaluate.py --model <model_path> --seed {self.seed}
```

**Configuration Hash:** {hash(str(self.config)) % 10000:04d}  
**Evaluation Date:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

---

*Report generated by Professional DRL Trading Evaluation System*
"""
        
        # Write report
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"‚úÖ Comprehensive report generated: {report_path}")
        return str(report_path)
    
    def run_full_evaluation(self, model_path: str, n_episodes: int = 10) -> dict:
        """Run complete evaluation pipeline"""
        print("\n" + "="*60)
        print("üî¨ PROFESSIONAL DRL AGENT EVALUATION")
        print("="*60)
        print(f"ü§ñ Model: {model_path}")
        print(f"üìÖ Test period: {self.config['data']['test_start']} to {self.config['data']['test_end']}")
        print(f"üéØ Episodes: {n_episodes}")
        print("="*60)
        
        try:
            # Setup
            self.create_test_environment()
            self.load_model(model_path)
            
            # Run backtest
            backtest_results = self.run_backtest(n_episodes=n_episodes)
            
            # Calculate metrics
            metrics = self.calculate_comprehensive_metrics(backtest_results)
            
            # Run benchmarks
            benchmarks = self.run_benchmark_comparisons(backtest_results)
            
            # Create visualizations
            plots = self.create_comprehensive_visualizations(backtest_results, metrics, benchmarks)
            
            # Generate report
            report_path = self.generate_comprehensive_report(backtest_results, metrics, benchmarks, plots)
            
            print("\n" + "="*60)
            print("üéâ EVALUATION COMPLETED SUCCESSFULLY!")
            print("="*60)
            print(f"üìÑ Report: {report_path}")
            print(f"üé® Plots: {len(plots)} visualizations generated")
            print(f"üìä Total Return: {metrics['main_metrics'].total_return:.2%}")
            print(f"‚ö° Sharpe Ratio: {metrics['main_metrics'].sharpe_ratio:.3f}")
            print(f"üìâ Max Drawdown: {metrics['main_metrics'].max_drawdown:.2%}")
            
            return {
                'backtest_results': backtest_results,
                'metrics': metrics,
                'benchmarks': benchmarks,
                'plots': plots,
                'report_path': report_path
            }
            
        except Exception as e:
            print(f"\nüí• Evaluation failed: {e}")
            import traceback
            traceback.print_exc()
            return None
        finally:
            if hasattr(self, 'test_env'):
                self.test_env.close()


def main():
    """Main evaluation function"""
    parser = argparse.ArgumentParser(description='Professional DRL Trading Agent Evaluation')
    parser.add_argument('--model', type=str, required=True,
                        help='Path to trained model (.zip file)')
    parser.add_argument('--config', type=str, default='configs/ppo_config.yaml',
                        help='Path to configuration file')
    parser.add_argument('--episodes', type=int, default=10,
                        help='Number of episodes to evaluate')
    parser.add_argument('--seed', type=int, default=None,
                        help='Random seed (overrides config)')
    parser.add_argument('--deterministic', action='store_true', default=True,
                        help='Use deterministic policy')
    
    args = parser.parse_args()
    
    print("üî¨ PROFESSIONAL DRL TRADING EVALUATION")
    print("=" * 50)
    print("üìà Comprehensive Backtesting & Analysis")
    print("üèÜ Benchmark Comparisons Included")
    print("=" * 50)
    
    # Validate model path
    if not Path(args.model).exists():
        print(f"‚ùå Model file not found: {args.model}")
        return 1
    
    try:
        # Initialize evaluator
        evaluator = ProfessionalEvaluator(config_path=args.config)
        
        # Apply command line overrides
        if args.seed:
            evaluator.seed = args.seed
            set_random_seed(args.seed)
            np.random.seed(args.seed)
        
        # Run evaluation
        results = evaluator.run_full_evaluation(
            model_path=args.model,
            n_episodes=args.episodes
        )
        
        if results:
            print(f"\n‚úÖ SUCCESS! Evaluation completed")
            print(f"üìÑ Report: {results['report_path']}")
            print(f"üìÅ Results directory: {evaluator.config['paths']['results_dir']}")
            return 0
        else:
            print(f"\n‚ùå Evaluation failed!")
            return 1
            
    except KeyboardInterrupt:
        print(f"\n‚èπÔ∏è Evaluation interrupted by user")
        return 130
    except Exception as e:
        print(f"\nüí• Unexpected error: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())