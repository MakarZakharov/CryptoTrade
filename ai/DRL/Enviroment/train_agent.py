"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è DRL –∞–≥–µ–Ω—Ç–∞ –Ω–∞ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å –∏ –ª–æ–≥–∏ –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.
"""

import os
from datetime import datetime
from stable_baselines3 import PPO, A2C
from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback
from stable_baselines3.common.monitor import Monitor
import numpy as np

from env import CryptoTradingEnv, ActionSpace, RewardType


def create_directories():
    """–°–æ–∑–¥–∞—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è."""
    dirs = ['models', 'logs', 'tensorboard']
    for d in dirs:
        os.makedirs(d, exist_ok=True)
    return dirs


def train_agent(
    symbol="BTCUSDT",
    timeframe="1d",
    algorithm="PPO",
    total_timesteps=100000,
    initial_balance=10000.0,
    reward_type=RewardType.RISK_ADJUSTED,
    learning_rate=3e-4,
    save_freq=10000
):
    """
    –û–±—É—á–∏—Ç—å DRL –∞–≥–µ–Ω—Ç–∞.

    Args:
        symbol: –¢–æ—Ä–≥–æ–≤–∞—è –ø–∞—Ä–∞
        timeframe: –¢–∞–π–º—Ñ—Ä–µ–π–º
        algorithm: –ê–ª–≥–æ—Ä–∏—Ç–º (PPO –∏–ª–∏ A2C)
        total_timesteps: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è
        initial_balance: –ù–∞—á–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å
        reward_type: –¢–∏–ø —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–∞–≥—Ä–∞–¥—ã
        learning_rate: Learning rate
        save_freq: –ß–∞—Å—Ç–æ—Ç–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤
    """
    print("\n" + "=" * 70)
    print("üöÄ –û–ë–£–ß–ï–ù–ò–ï DRL –ê–ì–ï–ù–¢–ê")
    print("=" * 70)

    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    create_directories()

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏ –¥–µ–ª–∏–º –Ω–∞ train/val
    print(f"\nüìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö: {symbol} {timeframe}")

    from data_loader import DataLoader
    loader = DataLoader(symbol=symbol, timeframe=timeframe)
    loader.load()

    total_length = len(loader)
    train_size = int(total_length * 0.8)

    print(f"  –í—Å–µ–≥–æ –¥–∞–Ω–Ω—ã—Ö: {total_length} —Å–≤–µ—á–µ–π")
    print(f"  Train: {train_size} —Å–≤–µ—á–µ–π (80%)")
    print(f"  Val: {total_length - train_size} —Å–≤–µ—á–µ–π (20%)")

    # Train –æ–∫—Ä—É–∂–µ–Ω–∏–µ
    print(f"\nüèãÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ train –æ–∫—Ä—É–∂–µ–Ω–∏—è...")
    train_env = CryptoTradingEnv(
        symbol=symbol,
        timeframe=timeframe,
        start_index=0,
        end_index=train_size,
        initial_balance=initial_balance,
        action_type=ActionSpace.DISCRETE,
        reward_type=reward_type,
        observation_window=50,
        add_indicators=True
    )

    train_env = Monitor(train_env, filename=f"logs/train_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv")

    print(f"  Observation space: {train_env.observation_space.shape}")
    print(f"  Action space: {train_env.action_space}")
    print(f"  Reward type: {reward_type.value}")

    # Validation –æ–∫—Ä—É–∂–µ–Ω–∏–µ
    print(f"\nüìä –°–æ–∑–¥–∞–Ω–∏–µ validation –æ–∫—Ä—É–∂–µ–Ω–∏—è...")
    val_env = CryptoTradingEnv(
        symbol=symbol,
        timeframe=timeframe,
        start_index=train_size,
        initial_balance=initial_balance,
        action_type=ActionSpace.DISCRETE,
        reward_type=reward_type
    )

    val_env = Monitor(val_env, filename=f"logs/val_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv")

    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    print(f"\nü§ñ –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏: {algorithm}")

    if algorithm == "PPO":
        model = PPO(
            "MlpPolicy",
            train_env,
            learning_rate=learning_rate,
            n_steps=2048,
            batch_size=64,
            n_epochs=10,
            gamma=0.99,
            gae_lambda=0.95,
            clip_range=0.2,
            verbose=1,
            tensorboard_log="./tensorboard/"
        )
    elif algorithm == "A2C":
        model = A2C(
            "MlpPolicy",
            train_env,
            learning_rate=learning_rate,
            n_steps=5,
            gamma=0.99,
            gae_lambda=1.0,
            verbose=1,
            tensorboard_log="./tensorboard/"
        )
    else:
        raise ValueError(f"Unknown algorithm: {algorithm}")

    # Callbacks
    eval_callback = EvalCallback(
        val_env,
        best_model_save_path='./models/',
        log_path='./logs/',
        eval_freq=5000,
        deterministic=True,
        render=False,
        n_eval_episodes=5,
        verbose=1
    )

    checkpoint_callback = CheckpointCallback(
        save_freq=save_freq,
        save_path='./models/checkpoints/',
        name_prefix=f'{algorithm.lower()}_crypto'
    )

    # –û–±—É—á–µ–Ω–∏–µ
    print(f"\nüéì –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ {total_timesteps} —à–∞–≥–æ–≤...")
    print("=" * 70)

    try:
        model.learn(
            total_timesteps=total_timesteps,
            callback=[eval_callback, checkpoint_callback],
            progress_bar=True
        )

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
        model_name = f"models/{algorithm.lower()}_{symbol}_{timeframe}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip"
        model.save(model_name)

        print("\n" + "=" * 70)
        print("‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
        print("=" * 70)
        print(f"üì¶ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_name}")
        print(f"üìä –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: models/best_model.zip")
        print(f"üìà TensorBoard –ª–æ–≥–∏: tensorboard/")

        # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ validation
        print("\n" + "=" * 70)
        print("üìä –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ù–ê VALIDATION")
        print("=" * 70)

        obs, _ = val_env.reset()
        total_reward = 0
        steps = 0

        while True:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = val_env.step(action)
            total_reward += reward
            steps += 1

            if terminated or truncated:
                break

        metrics = val_env.get_metrics()

        print(f"\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ Validation:")
        print(f"  –®–∞–≥–æ–≤: {steps}")
        print(f"  –û–±—â–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {total_reward:.2f}")
        print(f"  Total Return: {metrics.total_return_pct:.2f}%")
        print(f"  Sharpe Ratio: {metrics.sharpe_ratio:.2f}")
        print(f"  Max Drawdown: {metrics.max_drawdown_pct:.2f}%")
        print(f"  Total Trades: {metrics.total_trades}")
        print(f"  Win Rate: {metrics.win_rate:.2f}%")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        import json
        metrics_dict = {
            'symbol': symbol,
            'timeframe': timeframe,
            'algorithm': algorithm,
            'total_timesteps': total_timesteps,
            'val_return': metrics.total_return_pct,
            'val_sharpe': metrics.sharpe_ratio,
            'val_max_dd': metrics.max_drawdown_pct,
            'val_trades': metrics.total_trades,
            'val_win_rate': metrics.win_rate,
            'model_path': model_name
        }

        with open(f"models/metrics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json", 'w') as f:
            json.dump(metrics_dict, f, indent=2)

        print("\nüéâ –ì–æ—Ç–æ–≤–æ! –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ test_agent.py –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
        print("   –∏–ª–∏ manual_trading.py –¥–ª—è —Ä—É—á–Ω–æ–π —Ç–æ—Ä–≥–æ–≤–ª–∏\n")

        return model, metrics

    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è –û–±—É—á–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        print("–°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—É—é –º–æ–¥–µ–ª—å...")
        model.save(f"models/{algorithm.lower()}_interrupted.zip")
        print("‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: models/{algorithm.lower()}_interrupted.zip")

    finally:
        train_env.close()
        val_env.close()


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='–û–±—É—á–µ–Ω–∏–µ DRL –∞–≥–µ–Ω—Ç–∞ –¥–ª—è –∫—Ä–∏–ø—Ç–æ-—Ç—Ä–µ–π–¥–∏–Ω–≥–∞')
    parser.add_argument('--symbol', type=str, default='BTCUSDT', help='–¢–æ—Ä–≥–æ–≤–∞—è –ø–∞—Ä–∞')
    parser.add_argument('--timeframe', type=str, default='1d', help='–¢–∞–π–º—Ñ—Ä–µ–π–º')
    parser.add_argument('--algorithm', type=str, default='PPO', choices=['PPO', 'A2C'], help='–ê–ª–≥–æ—Ä–∏—Ç–º')
    parser.add_argument('--timesteps', type=int, default=100000, help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è')
    parser.add_argument('--balance', type=float, default=10000.0, help='–ù–∞—á–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å')
    parser.add_argument('--lr', type=float, default=3e-4, help='Learning rate')

    args = parser.parse_args()

    print("\nüéØ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è:")
    print(f"  Symbol: {args.symbol}")
    print(f"  Timeframe: {args.timeframe}")
    print(f"  Algorithm: {args.algorithm}")
    print(f"  Timesteps: {args.timesteps}")
    print(f"  Initial Balance: ${args.balance}")
    print(f"  Learning Rate: {args.lr}")

    train_agent(
        symbol=args.symbol,
        timeframe=args.timeframe,
        algorithm=args.algorithm,
        total_timesteps=args.timesteps,
        initial_balance=args.balance,
        learning_rate=args.lr
    )
